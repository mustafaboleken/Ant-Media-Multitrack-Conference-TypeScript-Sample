(function (global, factory) {
  typeof exports === 'object' && typeof module !== 'undefined' ? factory(exports) :
  typeof define === 'function' && define.amd ? define(['exports'], factory) :
  (global = typeof globalThis !== 'undefined' ? globalThis : global || self, factory(global.webrtc_adaptor = {}));
})(this, (function (exports) { 'use strict';

  function asyncGeneratorStep(gen, resolve, reject, _next, _throw, key, arg) {
    try {
      var info = gen[key](arg);
      var value = info.value;
    } catch (error) {
      reject(error);
      return;
    }
    if (info.done) {
      resolve(value);
    } else {
      Promise.resolve(value).then(_next, _throw);
    }
  }
  function _asyncToGenerator(fn) {
    return function () {
      var self = this,
        args = arguments;
      return new Promise(function (resolve, reject) {
        var gen = fn.apply(self, args);
        function _next(value) {
          asyncGeneratorStep(gen, resolve, reject, _next, _throw, "next", value);
        }
        function _throw(err) {
          asyncGeneratorStep(gen, resolve, reject, _next, _throw, "throw", err);
        }
        _next(undefined);
      });
    };
  }
  function _defineProperty(obj, key, value) {
    key = _toPropertyKey(key);
    if (key in obj) {
      Object.defineProperty(obj, key, {
        value: value,
        enumerable: true,
        configurable: true,
        writable: true
      });
    } else {
      obj[key] = value;
    }
    return obj;
  }
  function _toPrimitive(input, hint) {
    if (typeof input !== "object" || input === null) return input;
    var prim = input[Symbol.toPrimitive];
    if (prim !== undefined) {
      var res = prim.call(input, hint || "default");
      if (typeof res !== "object") return res;
      throw new TypeError("@@toPrimitive must return a primitive value.");
    }
    return (hint === "string" ? String : Number)(input);
  }
  function _toPropertyKey(arg) {
    var key = _toPrimitive(arg, "string");
    return typeof key === "symbol" ? key : String(key);
  }
  function _classPrivateFieldGet(receiver, privateMap) {
    var descriptor = _classExtractFieldDescriptor(receiver, privateMap, "get");
    return _classApplyDescriptorGet(receiver, descriptor);
  }
  function _classPrivateFieldSet(receiver, privateMap, value) {
    var descriptor = _classExtractFieldDescriptor(receiver, privateMap, "set");
    _classApplyDescriptorSet(receiver, descriptor, value);
    return value;
  }
  function _classExtractFieldDescriptor(receiver, privateMap, action) {
    if (!privateMap.has(receiver)) {
      throw new TypeError("attempted to " + action + " private field on non-instance");
    }
    return privateMap.get(receiver);
  }
  function _classApplyDescriptorGet(receiver, descriptor) {
    if (descriptor.get) {
      return descriptor.get.call(receiver);
    }
    return descriptor.value;
  }
  function _classApplyDescriptorSet(receiver, descriptor, value) {
    if (descriptor.set) {
      descriptor.set.call(receiver, value);
    } else {
      if (!descriptor.writable) {
        throw new TypeError("attempted to set read only private field");
      }
      descriptor.value = value;
    }
  }
  function _classPrivateMethodGet(receiver, privateSet, fn) {
    if (!privateSet.has(receiver)) {
      throw new TypeError("attempted to get private field on non-instance");
    }
    return fn;
  }
  function _checkPrivateRedeclaration(obj, privateCollection) {
    if (privateCollection.has(obj)) {
      throw new TypeError("Cannot initialize the same private elements twice on an object");
    }
  }
  function _classPrivateFieldInitSpec(obj, privateMap, value) {
    _checkPrivateRedeclaration(obj, privateMap);
    privateMap.set(obj, value);
  }
  function _classPrivateMethodInitSpec(obj, privateSet) {
    _checkPrivateRedeclaration(obj, privateSet);
    privateSet.add(obj);
  }

  class PeerStats {
    /**
     * Creates an instance of the class.
     * @param {string} streamId - The stream ID.
     * @constructor
     */
    constructor(streamId) {
      /**
       * The stream ID.
       * @type {string}
       */
      this.streamId = streamId;

      /**
       * The total number of bytes received.
       * @type {number}
       */
      this.totalBytesReceivedCount = 0;

      /**
       * The total number of bytes sent.
       * @type {number}
       */
      this.totalBytesSent = 0;

      /**
       * The number of video packets lost.
       * @type {number}
       */
      this.videoPacketsLost = 0;

      /**
       * The fraction of lost video packets.
       * @type {number}
       */
      this.fractionLost = 0;

      /**
       * The start time.
       * @type {number}
       */
      this.startTime = 0;

      /**
       * The last number of frames encoded.
       * @type {number}
       */
      this.lastFramesEncoded = 0;

      /**
       * The total number of frames encoded.
       * @type {number}
       */
      this.totalFramesEncodedCount = 0;

      /**
       * The last number of bytes received.
       * @type {number}
       */
      this.lastBytesReceived = 0;

      /**
       * The last number of bytes sent.
       * @type {number}
       */
      this.lastBytesSent = 0;

      /**
       * The total number of video packets sent.
       * @type {number}
       */
      this.totalVideoPacketsSent = 0;

      /**
       * The total number of audio packets sent.
       * @type {number}
       */
      this.totalAudioPacketsSent = 0;

      /**
       * The current timestamp.
       * @type {number}
       */
      this.currentTimestamp = 0;

      /**
       * The last recorded timestamp.
       * @type {number}
       */
      this.lastTime = 0;

      /**
       * The timer ID.
       * @type {number}
       */
      this.timerId = 0;

      /**
       * The first byte sent count.
       * @type {number}
       */
      this.firstByteSentCount = 0;

      /**
       * The first bytes received count.
       * @type {number}
       */
      this.firstBytesReceivedCount = 0;

      /**
       * The audio level.
       * @type {number}
       */
      this.audioLevel = -1;

      /**
       * The quality limitation reason.
       * @type {string}
       */
      this.qualityLimitationReason = "";

      /**
       * The source resolution width.
       * @type {number}
       */
      this.resWidth = 0;

      /**
       * The source resolution height.
       * @type {number}
       */
      this.resHeight = 0;

      /**
       * The source frames per second.
       * @type {number}
       */
      this.srcFps = 0;

      /**
       * The frame width of the sent video.
       * @type {number}
       */
      this.frameWidth = 0;

      /**
       * The frame height of the sent video.
       * @type {number}
       */
      this.frameHeight = 0;

      /**
       * The video round-trip time.
       * @type {number}
       */
      this.videoRoundTripTime = 0;

      /**
       * The video jitter.
       * @type {number}
       */
      this.videoJitter = 0;

      /**
       * The audio round-trip time.
       * @type {number}
       */
      this.audioRoundTripTime = 0;

      /**
       * The audio jitter.
       * @type {number}
       */
      this.audioJitter = 0;

      /**
       * The number of audio packets lost.
       * @type {number}
       */
      this.audioPacketsLost = 0;

      /**
       * The number of frames received.
       * @type {number}
       */
      this.framesReceived = 0;

      /**
       * The number of frames dropped.
       * @type {number}
       */
      this.framesDropped = 0;

      /**
       * The number of frames decoded.
       * @type {number}
       */
      this.framesDecoded = 0;

      /**
       * The average audio jitter delay.
       * @type {number}
       */
      this.audioJitterAverageDelay = 0;

      /**
       * The average video jitter delay.
       * @type {number}
       */
      this.videoJitterAverageDelay = 0;
      this.availableOutgoingBitrate = Infinity;
    }
    //kbits/sec
    get averageOutgoingBitrate() {
      return Math.floor(8 * (this.totalBytesSentCount - this.firstByteSentCount) / (this.currentTimestamp - this.startTime));
    }

    //frames per second
    get currentFPS() {
      return ((this.totalFramesEncodedCount - this.lastFramesEncoded) / (this.currentTimestamp - this.lastTime) * 1000).toFixed(1);
    }

    //kbits/sec
    get averageIncomingBitrate() {
      return Math.floor(8 * (this.totalBytesReceivedCount - this.firstBytesReceivedCount) / (this.currentTimestamp - this.startTime));
    }

    //kbits/sec
    get currentOutgoingBitrate() {
      return Math.floor(8 * (this.totalBytesSentCount - this.lastBytesSent) / (this.currentTimestamp - this.lastTime));
    }

    //kbits/sec
    get currentIncomingBitrate() {
      return Math.floor(8 * (this.totalBytesReceivedCount - this.lastBytesReceived) / (this.currentTimestamp - this.lastTime));
    }
    /**
     * @param {number} timestamp
     * @returns {void}
     */
    set currentTime(timestamp) {
      this.lastTime = this.currentTimestamp;
      this.currentTimestamp = timestamp;
      if (this.startTime == 0) {
        this.startTime = timestamp - 1; // do not have zero division error
      }
    }
    /**
     * @param {number} bytesReceived
     * @returns {void}
     */
    set totalBytesReceived(bytesReceived) {
      this.lastBytesReceived = this.totalBytesReceivedCount;
      this.totalBytesReceivedCount = bytesReceived;
      if (this.firstBytesReceivedCount == 0) {
        this.firstBytesReceivedCount = bytesReceived;
      }
    }
    /**
     * @param {number} bytesSent
     * @returns {void}
     */
    set totalBytesSent(bytesSent) {
      this.lastBytesSent = this.totalBytesSentCount;
      this.totalBytesSentCount = bytesSent;
      if (this.firstByteSentCount == 0) {
        this.firstByteSentCount = bytesSent;
      }
    }
    /**
     * @param {number} framesEncoded
     * @returns {void}
     */
    set totalFramesEncoded(framesEncoded) {
      this.lastFramesEncoded = this.totalFramesEncodedCount;
      this.totalFramesEncodedCount = framesEncoded;
      if (this.lastFramesEncoded == 0) {
        this.lastFramesEncoded = framesEncoded;
      }
    }
  }

  /*
  * loglevel - https://github.com/pimterry/loglevel
  *
  * Copyright (c) 2013 Tim Perry
  * Licensed under the MIT license.
  */
  (function (root, definition) {

    window.log = definition();
  })(undefined, function () {

    // Slightly dubious tricks to cut down minimized file size
    var noop = function noop() {};
    var undefinedType = "undefined";
    var isIE = typeof window !== undefinedType && typeof window.navigator !== undefinedType && /Trident\/|MSIE /.test(window.navigator.userAgent);
    var logMethods = ["trace", "debug", "info", "warn", "error"];

    // Cross-browser bind equivalent that works at least back to IE6
    function bindMethod(obj, methodName) {
      var method = obj[methodName];
      if (typeof method.bind === 'function') {
        return method.bind(obj);
      } else {
        try {
          return Function.prototype.bind.call(method, obj);
        } catch (e) {
          // Missing bind shim or IE8 + Modernizr, fallback to wrapping
          return function () {
            return Function.prototype.apply.apply(method, [obj, arguments]);
          };
        }
      }
    }

    // Trace() doesn't print the message in IE, so for that case we need to wrap it
    function traceForIE() {
      if (console.log) {
        if (console.log.apply) {
          console.log.apply(console, arguments);
        } else {
          // In old IE, native console methods themselves don't have apply().
          Function.prototype.apply.apply(console.log, [console, arguments]);
        }
      }
      if (console.trace) console.trace();
    }

    // Build the best logging method possible for this env
    // Wherever possible we want to bind, not wrap, to preserve stack traces
    function realMethod(methodName) {
      if (methodName === 'debug') {
        methodName = 'log';
      }
      if (typeof console === undefinedType) {
        return false; // No method possible, for now - fixed later by enableLoggingWhenConsoleArrives
      } else if (methodName === 'trace' && isIE) {
        return traceForIE;
      } else if (console[methodName] !== undefined) {
        return bindMethod(console, methodName);
      } else if (console.log !== undefined) {
        return bindMethod(console, 'log');
      } else {
        return noop;
      }
    }

    // These private functions always need `this` to be set properly

    function replaceLoggingMethods(level, loggerName) {
      /*jshint validthis:true */
      for (var i = 0; i < logMethods.length; i++) {
        var methodName = logMethods[i];
        this[methodName] = i < level ? noop : this.methodFactory(methodName, level, loggerName);
      }

      // Define log.log as an alias for log.debug
      this.log = this.debug;
    }

    // In old IE versions, the console isn't present until you first open it.
    // We build realMethod() replacements here that regenerate logging methods
    function enableLoggingWhenConsoleArrives(methodName, level, loggerName) {
      return function () {
        if (typeof console !== undefinedType) {
          replaceLoggingMethods.call(this, level, loggerName);
          this[methodName].apply(this, arguments);
        }
      };
    }

    // By default, we use closely bound real methods wherever possible, and
    // otherwise we wait for a console to appear, and then try again.
    function defaultMethodFactory(methodName, level, loggerName) {
      /*jshint validthis:true */
      return realMethod(methodName) || enableLoggingWhenConsoleArrives.apply(this, arguments);
    }
    function Logger(name, defaultLevel, factory) {
      var self = this;
      var currentLevel;
      defaultLevel = defaultLevel == null ? "WARN" : defaultLevel;
      var storageKey = "loglevel";
      if (typeof name === "string") {
        storageKey += ":" + name;
      } else if (typeof name === "symbol") {
        storageKey = undefined;
      }
      function persistLevelIfPossible(levelNum) {
        var levelName = (logMethods[levelNum] || 'silent').toUpperCase();
        if (typeof window === undefinedType || !storageKey) return;

        // Use localStorage if available
        try {
          window.localStorage[storageKey] = levelName;
          return;
        } catch (ignore) {}

        // Use session cookie as fallback
        try {
          window.document.cookie = encodeURIComponent(storageKey) + "=" + levelName + ";";
        } catch (ignore) {}
      }
      function getPersistedLevel() {
        var storedLevel;
        if (typeof window === undefinedType || !storageKey) return;
        try {
          storedLevel = window.localStorage[storageKey];
        } catch (ignore) {}

        // Fallback to cookies if local storage gives us nothing
        if (typeof storedLevel === undefinedType) {
          try {
            var cookie = window.document.cookie;
            var location = cookie.indexOf(encodeURIComponent(storageKey) + "=");
            if (location !== -1) {
              storedLevel = /^([^;]+)/.exec(cookie.slice(location))[1];
            }
          } catch (ignore) {}
        }

        // If the stored level is not valid, treat it as if nothing was stored.
        if (self.levels[storedLevel] === undefined) {
          storedLevel = undefined;
        }
        return storedLevel;
      }
      function clearPersistedLevel() {
        if (typeof window === undefinedType || !storageKey) return;

        // Use localStorage if available
        try {
          window.localStorage.removeItem(storageKey);
          return;
        } catch (ignore) {}

        // Use session cookie as fallback
        try {
          window.document.cookie = encodeURIComponent(storageKey) + "=; expires=Thu, 01 Jan 1970 00:00:00 UTC";
        } catch (ignore) {}
      }

      /*
       *
       * Public logger API - see https://github.com/pimterry/loglevel for details
       *
       */

      self.name = name;
      self.levels = {
        "TRACE": 0,
        "DEBUG": 1,
        "INFO": 2,
        "WARN": 3,
        "ERROR": 4,
        "SILENT": 5
      };
      self.methodFactory = factory || defaultMethodFactory;
      self.getLevel = function () {
        return currentLevel;
      };
      self.setLevel = function (level, persist) {
        if (typeof level === "string" && self.levels[level.toUpperCase()] !== undefined) {
          level = self.levels[level.toUpperCase()];
        }
        if (typeof level === "number" && level >= 0 && level <= self.levels.SILENT) {
          currentLevel = level;
          if (persist !== false) {
            // defaults to true
            persistLevelIfPossible(level);
          }
          replaceLoggingMethods.call(self, level, name);
          if (typeof console === undefinedType && level < self.levels.SILENT) {
            return "No console available for logging";
          }
        } else {
          throw "log.setLevel() called with invalid level: " + level;
        }
      };
      self.setDefaultLevel = function (level) {
        defaultLevel = level;
        if (!getPersistedLevel()) {
          self.setLevel(level, false);
        }
      };
      self.resetLevel = function () {
        self.setLevel(defaultLevel, false);
        clearPersistedLevel();
      };
      self.enableAll = function (persist) {
        self.setLevel(self.levels.TRACE, persist);
      };
      self.disableAll = function (persist) {
        self.setLevel(self.levels.SILENT, persist);
      };

      // Initialize with the right level
      var initialLevel = getPersistedLevel();
      if (initialLevel == null) {
        initialLevel = defaultLevel;
      }
      self.setLevel(initialLevel, false);
    }

    /*
     *
     * Top-level API
     *
     */

    var defaultLogger = new Logger();
    var _loggersByName = {};
    defaultLogger.getLogger = function getLogger(name) {
      if (typeof name !== "symbol" && typeof name !== "string" || name === "") {
        throw new TypeError("You must supply a name when creating a logger.");
      }
      var logger = _loggersByName[name];
      if (!logger) {
        logger = _loggersByName[name] = new Logger(name, defaultLogger.getLevel(), defaultLogger.methodFactory);
      }
      return logger;
    };

    // Grab the current global log variable in case of overwrite
    var _log = typeof window !== undefinedType ? window.log : undefined;
    defaultLogger.noConflict = function () {
      if (typeof window !== undefinedType && window.log === defaultLogger) {
        window.log = _log;
      }
      return defaultLogger;
    };
    defaultLogger.getLoggers = function getLoggers() {
      return _loggersByName;
    };

    // ES6 default export, for compatibility
    defaultLogger['default'] = defaultLogger;
    return defaultLogger;
  });

  var Logger$5 = window.log;
  class WebSocketAdaptor {
    /**
     * 
     * @param {object} initialValues 
     */
    constructor(initialValues) {
      /**
       * @type {boolean}
       */
      this.debug = false;
      for (var key in initialValues) {
        if (initialValues.hasOwnProperty(key)) {
          this[key] = initialValues[key];
        }
      }
      this.initWebSocketConnection();
    }
    /**
     * Initializes the WebSocket connection.
     * @param {Function} callbackConnected - Optional callback function to be called when the connection is established.
     * @returns {void}
     */
    initWebSocketConnection(callbackConnected) {
      this.connecting = true;
      this.connected = false;
      this.pingTimerId = -1;

      /*
      * It's not mandatory if you don't use the new Load Balancer mechanism
      * It uses one of the nodes on Cluster mode
      * Example parameters: "origin" or "edge"
      */
      var url = new URL(this.websocket_url);
      if (!['origin', 'edge'].includes(url.searchParams.get('target'))) {
        url.searchParams.set('target', this.webrtcadaptor.isPlayMode ? 'edge' : 'origin');
        this.websocket_url = url.toString();
      }
      this.wsConn = new WebSocket(this.websocket_url);
      this.wsConn.onopen = () => {
        if (this.debug) {
          Logger$5.debug("websocket connected");
        }
        this.pingTimerId = setInterval(() => {
          this.sendPing();
        }, 3000);
        this.connected = true;
        this.connecting = false;
        this.callback("initialized");
        if (typeof callbackConnected != "undefined") {
          callbackConnected();
        }
      };
      this.wsConn.onmessage = event => {
        var obj = JSON.parse(event.data);
        if (obj.command == "start") {
          //this command is received first, when publishing so playmode is false

          if (this.debug) {
            Logger$5.debug("received start command");
          }
          this.webrtcadaptor.startPublishing(obj.streamId);
        } else if (obj.command == "takeCandidate") {
          if (this.debug) {
            Logger$5.debug("received ice candidate for stream id " + obj.streamId);
            Logger$5.debug(obj.candidate);
          }
          this.webrtcadaptor.takeCandidate(obj.streamId, obj.label, obj.candidate);
        } else if (obj.command == "takeConfiguration") {
          if (this.debug) {
            Logger$5.debug("received remote description type for stream id: " + obj.streamId + " type: " + obj.type);
          }
          this.webrtcadaptor.takeConfiguration(obj.streamId, obj.sdp, obj.type, obj.idMapping);
        } else if (obj.command == "stop") {
          if (this.debug) {
            Logger$5.debug("Stop command received");
          }
          //server sends stop command when the peers are connected to each other in peer-to-peer.
          //It is not being sent in publish,play modes
          this.webrtcadaptor.closePeerConnection(obj.streamId);
        } else if (obj.command == "error") {
          this.callbackError(obj.definition, obj);
        } else if (obj.command == "notification") {
          this.callback(obj.definition, obj);
        } else if (obj.command == "streamInformation") {
          this.callback(obj.command, obj);
        } else if (obj.command == "roomInformation") {
          this.callback(obj.command, obj);
        } else if (obj.command == "pong") {
          this.callback(obj.command);
        } else if (obj.command == "trackList") {
          this.callback(obj.command, obj);
        } else if (obj.command == "connectWithNewId") {
          this.multiPeerStreamId = obj.streamId;
          this.join(obj.streamId);
        } else if (obj.command == "peerMessageCommand") {
          this.callback(obj.command, obj);
        }
      };
      this.wsConn.onerror = error => {
        this.connecting = false;
        this.connected = false;
        Logger$5.info(" error occured: " + JSON.stringify(error));
        this.clearPingTimer();
        this.callbackError("WebSocketNotConnected", error);
      };
      this.wsConn.onclose = event => {
        this.connecting = false;
        this.connected = false;
        if (this.debug) {
          Logger$5.debug("connection closed.");
        }
        this.clearPingTimer();
        this.callback("closed", event);
      };
    }
    clearPingTimer() {
      if (this.pingTimerId != -1) {
        if (this.debug) {
          Logger$5.debug("Clearing ping message timer");
        }
        clearInterval(this.pingTimerId);
        this.pingTimerId = -1;
      }
    }
    sendPing() {
      var jsCmd = {
        command: "ping"
      };
      this.wsConn.send(JSON.stringify(jsCmd));
    }
    close() {
      this.wsConn.close();
    }
    /**
     * 
     * @param {*} text 
     * @returns 
     */
    send(text) {
      if (this.connecting == false && this.connected == false) {
        //try to reconnect
        this.initWebSocketConnection(() => {
          this.send(text);
        });
        return;
      }
      try {
        this.wsConn.send(text);
        if (this.debug) {
          Logger$5.debug("sent message:" + text);
        }
      } catch (error) {
        Logger$5.warn("Cannot send message:" + text);
      }
    }
    isConnected() {
      return this.connected;
    }
    isConnecting() {
      return this.connecting;
    }
  }

  var Logger$4 = window.log;
  class SoundMeter {
    /**
     * 
     * @param {AudioContext} context 
     */
    constructor(context) {
      this.context = context;
      this.instant = 0.0;
      this.mic = null;
      this.volumeMeterNode = null;
    }
    /**
     * 
     * @param {MediaStream} stream 
     * @param {Function} levelCallback 
     * @param {Function} errorCallback 
     * @returns 
     */
    connectToSource(stream, levelCallback, errorCallback) {
      return this.context.audioWorklet.addModule(new URL('./volume-meter-processor.js', (typeof document === 'undefined' && typeof location === 'undefined' ? require('u' + 'rl').pathToFileURL(__filename).href : typeof document === 'undefined' ? location.href : (document.currentScript && document.currentScript.src || new URL('webrtc_adaptor.js', document.baseURI).href)))).then(() => {
        this.mic = this.context.createMediaStreamSource(stream);
        this.volumeMeterNode = new AudioWorkletNode(this.context, 'volume-meter');
        this.volumeMeterNode.port.onmessage = event => {
          if (event.data.type == 'debug') {
            Logger$4.debug(event.data.message);
          } else {
            this.instant = event.data;
            levelCallback(this.instant.toFixed(2));
            Logger$4.debug("Audio level: " + this.instant.toFixed(2));
          }
        };
        this.mic.connect(this.volumeMeterNode);
      }).catch(err => {
        if (errorCallback !== undefined) {
          errorCallback(err);
        }
        Logger$4.error("Error in soundmeter: " + err);
        throw err;
      });
    }
    stop() {
      if (this.volumeMeterNode != null) {
        this.volumeMeterNode.port.postMessage('stop');
        this.volumeMeterNode.disconnect();
        this.volumeMeterNode.port.close();
        this.volumeMeterNode = null;
      }
      if (this.mic != null) {
        this.mic.disconnect();
        this.mic = null;
      }
    }
  }

  var Logger$3 = window.log;
  /**
   * Media management class is responsible to manage audio and video
   * sources and tracks management for the local stream.
   * Also audio and video properties (like bitrate) are managed by this class .
   */
  class MediaManager {
    /**
     * 
     * @param {object} initialValues 
     */
    constructor(initialValues) {
      /**
       * the maximum bandwith value that browser can send a stream
       * keep in mind that browser may send video less than this value
       */
      this.bandwidth = 1200; //kbps

      /**
       * This flags enables/disables debug logging
       */
      this.debug = false;

      /**
       * The cam_location below is effective when camera and screen is send at the same time.
       * possible values are top and bottom. It's on right all the time
       */
      this.camera_location = "top";

      /**
       * The cam_margin below is effective when camera and screen is send at the same time.
       * This is the margin value in px from the edges
       */
      this.camera_margin = 15;

      /**
       * this camera_percent is how large the camera view appear on the screen. It's %15 by default.
       */
      this.camera_percent = 15;

      /**
       * initial media constraints provided by the user
       */
      this.mediaConstraints = {
        video: true,
        audio: true
      };

      /**
       * this is the callback function to get video/audio sender from WebRTCAdaptor
       */
      this.getSender = initialValues.getSender;

      /**
       * This is the Stream Id for the publisher.
       */
      this.publishStreamId = null;

      /**
       * this is the object of the local stream to publish
       * it is initiated in initLocalStream method
       */
      this.localStream = null;

      /**
       * publish mode is determined by the user and set by @mediaConstraints.video
       * It may be camera, screen, screen+camera
       */
      this.publishMode = "camera"; //screen, screen+camera

      /**
       * Default callback. It's overriden below if it exists
       */
      this.callback = (info, obj) => {
        Logger$3.debug("Callback info: " + info + " object: " + typeof obj !== undefined ? JSON.stringify(obj) : "");
      };

      /**
       * Default callback error implementation. It's overriden below if it exists
       */
      this.callbackError = err => {
        Logger$3.error(err);
      };

      /**
       * The values of the above fields are provided as user parameters by the constructor.
       * TODO: Also some other hidden parameters may be passed here
       */
      for (var key in initialValues.userParameters) {
        if (initialValues.userParameters.hasOwnProperty(key)) {
          this[key] = initialValues.userParameters[key];
        }
      }

      /**
       * current volume value which is set by the user
       */
      this.currentVolume = null;

      /**
       * Keeps the audio track to be closed in case of audio track change
       */
      this.previousAudioTrack = null;

      /**
      * silent audio track for switching between dummy track to real tracks on the fly
      */
      this.silentAudioTrack = null;

      /**
       * The screen video track in screen+camera mode
       */
      this.desktopStream = null;

      /**
       * The camera (overlay) video track in screen+camera mode
       */
      this.smallVideoTrack = null;

      /**
      * black video track for switching between dummy video track to real tracks on the fly
      */
      this.blackVideoTrack = null;

      /**
       * Audio context to use for meter, mix, gain
       */
      this.audioContext = new AudioContext();

      /**
      * osciallator to generate silent audio
      */
      this.oscillator = null;

      /**
       * the main audio in single audio case
       * the primary audio in mixed audio case
       *
       * its volume can be controled
       */
      this.primaryAudioTrackGainNode = null;

      /**
       * the secondary audio in mixed audio case
       *
       * its volume can be controled
       */
      this.secondaryAudioTrackGainNode = null;

      /**
       * this is the sound meter object for the local stream
       */
      this.localStreamSoundMeter = null;

      /**
       * this is the level callback for sound meter object
       */
      this.levelCallback = null;

      /**
       * Timer to create black frame to publish when video is muted
       */
      this.blackFrameTimer = null;

      /**
       * Timer to draw camera and desktop to canvas
       */
      this.desktopCameraCanvasDrawerTimer = null;

      /**
       * For audio check when the user is muted itself.
       * Check enableAudioLevelWhenMuted
       */
      this.mutedAudioStream = null;

      /**
       * This flag is the status of audio stream
       * Checking when the audio stream is updated
       */
      this.isMuted = false;

      /**
       * meter refresh period for "are you talking?" check
       */
      this.meterRefresh = null;

      /**
       * For keeping track of whether user turned off the camera
       */
      this.cameraEnabled = true;

      /**
       * Replacement stream for video track when the camera is turn off
      */
      this.replacementStream = null;

      /**
       * html video element that presents local stream
       */
      this.localVideo = this.localVideoElement || document.getElementById(this.localVideoId);

      //A dummy stream created to replace the tracks when camera is turned off.
      this.dummyCanvas = document.createElement("canvas");

      // It should be compatible with previous version
      if (this.mediaConstraints) {
        if (this.mediaConstraints.video == "camera") {
          this.publishMode = "camera";
        } else if (this.mediaConstraints.video == "screen") {
          this.publishMode = "screen";
        } else if (this.mediaConstraints.video == "screen+camera") {
          this.publishMode = "screen+camera";
        }
      } else {
        //just define default values
        this.mediaConstraints = {
          video: true,
          audio: true
        };
      }

      //Check browser support for screen share function
      this.checkBrowserScreenShareSupported();
    }

    /**
     * Called by the WebRTCAdaptor at the start if it isn't play mode
     */
    initLocalStream() {
      this.checkWebRTCPermissions();
      if (typeof this.mediaConstraints.video != "undefined" && this.mediaConstraints.video != false) {
        return this.openStream(this.mediaConstraints);
      } else if (typeof this.mediaConstraints.audio != "undefined" && this.mediaConstraints.audio != false) {
        // get only audio
        var media_audio_constraint = {
          audio: this.mediaConstraints.audio
        };
        return this.navigatorUserMedia(media_audio_constraint, stream => {
          return this.gotStream(stream);
        }, true);
      } else {
        //neither video nor audio is requested
        //just return null stream
        Logger$3.debug("no media requested, just return an empty stream");
        return new Promise((resolve, reject) => {
          resolve(null);
        });
      }
    }

    /*
    * Called to checks if Websocket and media usage are allowed
    */
    checkWebRTCPermissions() {
      if (!("WebSocket" in window)) {
        Logger$3.debug("WebSocket not supported.");
        this.callbackError("WebSocketNotSupported");
        return;
      }
      if (typeof navigator.mediaDevices == "undefined") {
        Logger$3.debug("Cannot open camera and mic because of unsecure context. Please Install SSL(https)");
        this.callbackError("UnsecureContext");
        return;
      }
      if (typeof navigator.mediaDevices == "undefined" || navigator.mediaDevices == undefined || navigator.mediaDevices == null) {
        this.callbackError("getUserMediaIsNotAllowed");
      }
    }

    /*
     * Called to get the available video and audio devices on the system
     */
    getDevices() {
      return navigator.mediaDevices.enumerateDevices().then(devices => {
        var deviceArray = new Array();
        var checkAudio = false;
        var checkVideo = false;
        devices.forEach(device => {
          if (device.kind == "audioinput" || device.kind == "videoinput") {
            deviceArray.push(device);
            if (device.kind == "audioinput") {
              checkAudio = true;
            }
            if (device.kind == "videoinput") {
              checkVideo = true;
            }
          }
        });
        this.callback("available_devices", deviceArray);

        //TODO: is the following part necessary. why?
        if (checkAudio == false && this.localStream == null) {
          Logger$3.debug("Audio input not found");
          Logger$3.debug("Retrying to get user media without audio");
          if (this.inputDeviceNotFoundLimit < 2) {
            if (checkVideo != false) {
              this.openStream({
                video: true,
                audio: false
              });
              this.inputDeviceNotFoundLimit++;
            } else {
              Logger$3.debug("Video input not found");
              alert("There is no video or audio input");
            }
          } else {
            alert("No input device found, publish is not possible");
          }
        }
        return deviceArray;
      }).catch(err => {
        Logger$3.error("Cannot get devices -> error name: " + err.name + ": " + err.message);
        throw err;
      });
    }

    /*
     * Called to add a device change listener
     */
    trackDeviceChange() {
      navigator.mediaDevices.ondevicechange = () => {
        this.getDevices();
      };
    }

    /**
     * This function create a canvas which combines screen video and camera video as an overlay
     *
     * @param {*} stream : screen share stream
     * @param {*} streamId
     * @param {*} onEndedCallback : callback when called on screen share stop
     */
    setDesktopwithCameraSource(stream, streamId, onEndedCallback) {
      this.desktopStream = stream;
      return this.navigatorUserMedia({
        video: true,
        audio: false
      }, cameraStream => {
        this.smallVideoTrack = cameraStream.getVideoTracks()[0];

        //create a canvas element
        var canvas = document.createElement("canvas");
        var canvasContext = canvas.getContext("2d");

        //create video element for screen
        //var screenVideo = document.getElementById('sourceVideo');
        var screenVideo = document.createElement('video');
        screenVideo.srcObject = stream;
        screenVideo.play();
        //create video element for camera
        var cameraVideo = document.createElement('video');
        cameraVideo.srcObject = cameraStream;
        cameraVideo.play();
        var canvasStream = canvas.captureStream(15);
        if (onEndedCallback != null) {
          stream.getVideoTracks()[0].onended = function (event) {
            onEndedCallback(event);
          };
        }
        var promise;
        if (this.localStream == null) {
          promise = this.gotStream(canvasStream);
        } else {
          promise = this.updateVideoTrack(canvasStream, streamId, onended, null);
        }
        promise.then(() => {
          //update the canvas
          this.desktopCameraCanvasDrawerTimer = setInterval(() => {
            //draw screen to canvas
            canvas.width = screenVideo.videoWidth;
            canvas.height = screenVideo.videoHeight;
            canvasContext.drawImage(screenVideo, 0, 0, canvas.width, canvas.height);
            var cameraWidth = screenVideo.videoWidth * (this.camera_percent / 100);
            var cameraHeight = cameraVideo.videoHeight / cameraVideo.videoWidth * cameraWidth;
            var positionX = canvas.width - cameraWidth - this.camera_margin;
            var positionY;
            if (this.camera_location == "top") {
              positionY = this.camera_margin;
            } else {
              //if not top, make it bottom
              //draw camera on right bottom corner
              positionY = canvas.height - cameraHeight - this.camera_margin;
            }
            canvasContext.drawImage(cameraVideo, positionX, positionY, cameraWidth, cameraHeight);
          }, 66);
        });
      }, true);
    }

    /**
     * This function does these:
     *    1. Remove the audio track from the stream provided if it is camera. Other case
     *       is screen video + system audio track. In this case audio is kept in stream.
     *    2. Open audio track again if audio constaint isn't false
     *    3. Make audio track Gain Node to be able to volume adjustable
     *  4. If screen is shared and system audio is available then the system audio and
     *     opened audio track are mixed
     *
     * @param {*} mediaConstraints
     * @param {*} audioConstraint
     * @param {*} stream
     * @param {*} streamId
     */
    prepareStreamTracks(mediaConstraints, audioConstraint, stream, streamId) {
      //this trick, getting audio and video separately, make us add or remove tracks on the fly
      var audioTracks = stream.getAudioTracks();
      if (audioTracks.length > 0 && this.publishMode == "camera") {
        audioTracks[0].stop();
        stream.removeTrack(audioTracks[0]);
      }
      //now get only audio to add this stream
      if (audioConstraint != "undefined" && audioConstraint != false) {
        var media_audio_constraint = {
          audio: audioConstraint
        };
        return this.navigatorUserMedia(media_audio_constraint).then(audioStream => {
          //here audioStream has onr audio track only
          audioStream = this.setGainNodeStream(audioStream);
          // now audio stream has two audio strams.
          // 1. Gain Node : this will be added to local stream to publish
          // 2. Original audio track : keep its reference to stop later

          //add callback if desktop is sharing
          var onended = event => {
            this.callback("screen_share_stopped");
            this.setVideoCameraSource(streamId, mediaConstraints, null, true);
          };
          if (this.publishMode == "screen") {
            return this.updateVideoTrack(stream, streamId, onended, true).then(() => {
              if (audioTracks.length > 0) {
                //system audio share case, then mix it with device audio
                audioStream = this.mixAudioStreams(stream, audioStream);
              }
              return this.updateAudioTrack(audioStream, streamId, null);
            });
          } else if (this.publishMode == "screen+camera") {
            if (audioTracks.length > 0) {
              //system audio share case, then mix it with device audio
              audioStream = this.mixAudioStreams(stream, audioStream);
            }
            return this.updateAudioTrack(audioStream, streamId, null).then(() => {
              return this.setDesktopwithCameraSource(stream, streamId, onended);
            });
          } else {
            if (audioConstraint != false && audioConstraint != undefined) {
              stream.addTrack(audioStream.getAudioTracks()[0]);
            }
            if (stream.getVideoTracks().length > 0) {
              return this.updateVideoTrack(stream, streamId, null, null).then(() => {
                return this.updateAudioTrack(stream, streamId, null).then(() => {
                  return this.gotStream(stream);
                });
              });
            } else if (stream.getAudioTracks().length > 0) {
              return this.updateAudioTrack(stream, streamId, null).then(() => {
                return this.gotStream(stream);
              });
            } else {
              return this.gotStream(stream);
            }
          }
        }).catch(error => {
          if (error.name == "NotFoundError") {
            this.getDevices();
          } else {
            this.callbackError(error.name, error.message);
          }
          //throw error for promise
          throw error;
        });
      } else {
        return this.gotStream(stream);
      }
    }

    /**
     * Called to get user media (camera and/or mic)
     *
     * @param {*} mediaConstraints : media constaint
     * @param {*} func : callback on success. The stream which is got, is passed as parameter to this function
     * @param {*} catch_error : error is checked if catch_error is true
     */
    navigatorUserMedia(mediaConstraints, func, catch_error) {
      if (mediaConstraints.video == "dummy" || mediaConstraints.audio == "dummy") {
        var stream = new MediaStream();
        if (mediaConstraints.audio == "dummy") {
          stream.addTrack(this.getSilentAudioTrack());
        }
        if (mediaConstraints.video == "dummy") {
          stream.addTrack(this.getBlackVideoTrack());
        }
        return new Promise((resolve, reject) => {
          resolve(stream);
        });
      } else {
        return navigator.mediaDevices.getUserMedia(mediaConstraints).then(stream => {
          if (typeof func != "undefined" || func != null) {
            func(stream);
          }
          return stream;
        }).catch(error => {
          if (catch_error == true) {
            if (error.name == "NotFoundError") {
              this.getDevices();
            } else {
              this.callbackError(error.name, error.message);
            }
          } else {
            Logger$3.warn(error);
          }
          //throw error if there is a promise
          throw error;
        });
      }
    }

    /**
     * Called to get display media (screen share)
     *
     * @param {*} mediaConstraints : media constaint
     * @param {*} func : callback on success. The stream which is got, is passed as parameter to this function
     */
    navigatorDisplayMedia(mediaConstraints, func) {
      return navigator.mediaDevices.getDisplayMedia(mediaConstraints).then(stream => {
        if (typeof func != "undefined") {
          func(stream);
        }
        return stream;
      }).catch(error => {
        if (error.name === "NotAllowedError") {
          Logger$3.debug("Permission denied error");
          this.callbackError("ScreenSharePermissionDenied");

          // If error catched then redirect Default Stream Camera
          if (this.localStream == null) {
            var mediaConstraints = {
              video: true,
              audio: true
            };
            this.openStream(mediaConstraints);
          } else {
            this.switchVideoCameraCapture(streamId);
          }
        }
      });
    }

    /**
     * Called to get the media (User Media or Display Media)
     * @param {*} mediaConstraints, media constraints 
     * @param {*} streamId, streamId to be used to replace track if there is an active peer connection
     */
    getMedia(mediaConstraints, streamId) {
      var audioConstraint = false;
      if (typeof mediaConstraints.audio != "undefined" && mediaConstraints.audio != false) {
        audioConstraint = mediaConstraints.audio;
      }
      if (this.desktopCameraCanvasDrawerTimer != null) {
        clearInterval(this.desktopCameraCanvasDrawerTimer);
        this.desktopCameraCanvasDrawerTimer = null;
      }

      // Check Media Constraint video value screen or screen + camera
      if (this.publishMode == "screen+camera" || this.publishMode == "screen") {
        return this.navigatorDisplayMedia(mediaConstraints).then(stream => {
          if (this.smallVideoTrack) this.smallVideoTrack.stop();
          return this.prepareStreamTracks(mediaConstraints, audioConstraint, stream, streamId);
        });
      } else {
        return this.navigatorUserMedia(mediaConstraints).then(stream => {
          if (this.smallVideoTrack) this.smallVideoTrack.stop();
          return this.prepareStreamTracks(mediaConstraints, audioConstraint, stream, streamId);
        }).catch(error => {
          if (error.name == "NotFoundError") {
            this.getDevices();
          } else {
            this.callbackError(error.name, error.message);
          }
        });
      }
    }

    /**
     * Open media stream, it may be screen, camera or audio
     */
    openStream(mediaConstraints, streamId) {
      this.mediaConstraints = mediaConstraints;
      return this.getMedia(mediaConstraints, streamId).then(() => {
        if (this.mediaConstraints.video != "dummy" && this.mediaConstraints.video != undefined) {
          this.stopBlackVideoTrack();
          this.clearBlackVideoTrackTimer();
        }
        if (this.mediaConstraints.audio != "dummy" && this.mediaConstraints.audio != undefined) {
          this.stopSilentAudioTrack();
        }
      });
    }

    /**
     * Closes stream, if you want to stop peer connection, call stop(streamId)
     */
    closeStream() {
      if (this.localStream) {
        this.localStream.getVideoTracks().forEach(function (track) {
          track.onended = null;
          track.stop();
        });
        this.localStream.getAudioTracks().forEach(function (track) {
          track.onended = null;
          track.stop();
        });
      }
      if (this.videoTrack) {
        this.videoTrack.stop();
      }
      if (this.audioTrack) {
        this.audioTrack.stop();
      }
      if (this.smallVideoTrack) {
        this.smallVideoTrack.stop();
      }
      if (this.previousAudioTrack) {
        this.previousAudioTrack.stop();
      }
    }

    /**
     * Checks browser supports screen share feature
     * if exist it calls callback with "browser_screen_share_supported"
     */
    checkBrowserScreenShareSupported() {
      if (typeof navigator.mediaDevices != "undefined" && navigator.mediaDevices.getDisplayMedia || navigator.getDisplayMedia) {
        this.callback("browser_screen_share_supported");
      }
    }
    /**
     * Changes the secondary stream gain in mixed audio mode
     *
     * @param {*} enable
     */
    enableSecondStreamInMixedAudio(enable) {
      if (this.secondaryAudioTrackGainNode != null) {
        if (enable) {
          this.secondaryAudioTrackGainNode.gain.value = 1;
        } else {
          this.secondaryAudioTrackGainNode.gain.value = 0;
        }
      }
    }

    /**
     * Changes local stream when new stream is prepared
     *
     * @param {*} stream
     */
    gotStream(stream) {
      this.localStream = stream;
      if (this.localVideo) {
        this.localVideo.srcObject = stream;
      }
      this.getDevices();
      this.trackDeviceChange();
      return new Promise((resolve, reject) => {
        resolve();
      });
    }

    /**
     * Changes local video and sets localStream as source
     *
     * @param {*} videoEl
     */
    changeLocalVideo(videoEl) {
      this.localVideo = videoEl;
      if (this.localStream) {
        this.localVideo.srcObject = this.localStream;
      }
    }

    /**
     * These methods are initialized when the user is muted himself in a publish scenario
     * It will keep track if the user is trying to speak without sending any data to server
     * Please don't forget to disable this function with disableAudioLevelWhenMuted if you use it.
     */
    enableAudioLevelWhenMuted() {
      navigator.mediaDevices.getUserMedia({
        video: false,
        audio: true
      }).then(stream => {
        this.mutedAudioStream = stream;
        this.mutedSoundMeter = new SoundMeter(this.audioContext);
        soundMeter.connectToSource(this.mutedAudioStream, value => {
          if (value > 0.1) {
            this.callback("speaking_but_muted");
          }
        }, e => {
          if (e) {
            alert(e);
            return;
          }
          this.meterRefresh = setInterval(() => {
            if (soundMeter.instant.toFixed(2) > 0.1) {
              this.callback("speaking_but_muted");
            }
          }, 200);
        });
      }).catch(function (err) {
        Logger$3.debug("Can't get the soundlevel on mute");
      });
    }
    disableAudioLevelWhenMuted() {
      if (this.meterRefresh != null) {
        clearInterval(this.meterRefresh);
        this.meterRefresh = null;
      }
      if (this.mutedSoundMeter != null) {
        this.mutedSoundMeter.stop();
        this.mutedSoundMeter = null;
      }
      if (this.mutedAudioStream != null) {
        this.mutedAudioStream.getTracks().forEach(function (track) {
          track.stop();
        });
      }
    }

    /**
    * @Deprecated. It's not the job of SDK to make these things. It increases the complexity of the code. 
     * We provide samples for having these function
     * 
     * This method mixed the first stream audio to the second stream audio and
     * @param {*} stream  : Primary stream that contain video and audio (system audio)
     * @param {*} secondStream :stream has device audio
     * @returns mixed stream.
     */
    mixAudioStreams(stream, secondStream) {
      //Logger.debug("audio stream track count: " + audioStream.getAudioTracks().length);
      var composedStream = new MediaStream();
      //added the video stream from the screen
      stream.getVideoTracks().forEach(function (videoTrack) {
        composedStream.addTrack(videoTrack);
      });
      var audioDestionation = this.audioContext.createMediaStreamDestination();
      if (stream.getAudioTracks().length > 0) {
        this.primaryAudioTrackGainNode = this.audioContext.createGain();

        //Adjust the gain for screen sound
        this.primaryAudioTrackGainNode.gain.value = 1;
        var audioSource = this.audioContext.createMediaStreamSource(stream);
        audioSource.connect(this.primaryAudioTrackGainNode).connect(audioDestionation);
      } else {
        Logger$3.debug("Origin stream does not have audio track");
      }
      if (secondStream.getAudioTracks().length > 0) {
        this.secondaryAudioTrackGainNode = this.audioContext.createGain();

        //Adjust the gain for second sound
        this.secondaryAudioTrackGainNode.gain.value = 1;
        var audioSource2 = this.audioContext.createMediaStreamSource(secondStream);
        audioSource2.connect(this.secondaryAudioTrackGainNode).connect(audioDestionation);
      } else {
        Logger$3.debug("Second stream does not have audio track");
      }
      audioDestionation.stream.getAudioTracks().forEach(function (track) {
        composedStream.addTrack(track);
        Logger$3.debug("audio destination add track");
      });
      return composedStream;
    }

    /**
     * This method creates a Gain Node stream to make the audio track adjustable
     *
     * @param {*} stream
     * @returns
     */
    setGainNodeStream(stream) {
      if (this.mediaConstraints.audio != false && typeof this.mediaConstraints.audio != "undefined") {
        // Get the videoTracks from the stream.
        var videoTracks = stream.getVideoTracks();

        // Get the audioTracks from the stream.
        var audioTracks = stream.getAudioTracks();

        /**
         * Create a new audio context and build a stream source,
         * stream destination and a gain node. Pass the stream into
         * the mediaStreamSource so we can use it in the Web Audio API.
         */
        this.audioContext = new AudioContext();
        var mediaStreamSource = this.audioContext.createMediaStreamSource(stream);
        var mediaStreamDestination = this.audioContext.createMediaStreamDestination();
        this.primaryAudioTrackGainNode = this.audioContext.createGain();

        /**
         * Connect the stream to the gainNode so that all audio
         * passes through the gain and can be controlled by it.
         * Then pass the stream from the gain to the mediaStreamDestination
         * which can pass it back to the RTC client.
         */
        mediaStreamSource.connect(this.primaryAudioTrackGainNode);
        this.primaryAudioTrackGainNode.connect(mediaStreamDestination);
        if (this.currentVolume == null) {
          this.primaryAudioTrackGainNode.gain.value = 1;
        } else {
          this.primaryAudioTrackGainNode.gain.value = this.currentVolume;
        }

        /**
         * The mediaStreamDestination.stream outputs a MediaStream object
         * containing a single AudioMediaStreamTrack. Add the video track
         * to the new stream to rejoin the video with the controlled audio.
         */
        var controlledStream = mediaStreamDestination.stream;
        for (var videoTrack of videoTracks) {
          controlledStream.addTrack(videoTrack);
        }
        for (var audioTrack of audioTracks) {
          controlledStream.addTrack(audioTrack);
        }
        if (this.previousAudioTrack !== null) {
          this.previousAudioTrack.stop();
        }
        this.previousAudioTrack = controlledStream.getAudioTracks()[1];

        /**
         * Use the stream that went through the gainNode. This
         * is the same stream but with altered input volume levels.
         */
        return controlledStream;
      }
      return stream;
    }

    /**
     * Called by User
     * to switch the Screen Share mode
     *
     * @param {*} streamId
     */
    switchDesktopCapture(streamId) {
      this.publishMode = "screen";
      if (typeof this.mediaConstraints.video != "undefined" && this.mediaConstraints.video != false) {
        this.mediaConstraints.video = true;
      }
      //TODO: I don't think we need to get audio again. We just need to switch the video stream
      return this.getMedia(this.mediaConstraints, streamId);
    }

    /**
     * Called by User
     * to switch the Screen Share with Camera mode
     *
     * @param {*} streamId
     */
    switchDesktopCaptureWithCamera(streamId) {
      if (typeof this.mediaConstraints.video != "undefined" && this.mediaConstraints.video != false) {
        this.mediaConstraints.video = true;
      }
      this.publishMode = "screen+camera";

      //TODO: I don't think we need to get audio again. We just need to switch the video stream
      return this.getMedia(this.mediaConstraints, streamId);
    }

    /**
     * This method updates the local stream. It removes existant audio track from the local stream
     * and add the audio track in `stream` parameter to the local stream
     */
    updateLocalAudioStream(stream, onEndedCallback) {
      var newAudioTrack = stream.getAudioTracks()[0];
      if (this.localStream != null && this.localStream.getAudioTracks()[0] != null) {
        var audioTrack = this.localStream.getAudioTracks()[0];
        if (audioTrack != newAudioTrack) {
          this.localStream.removeTrack(audioTrack);
          audioTrack.stop();
          this.localStream.addTrack(newAudioTrack);
        }
      } else if (this.localStream != null) {
        this.localStream.addTrack(newAudioTrack);
      } else {
        this.localStream = stream;
      }
      if (this.localVideo != null) {
        //it can be null
        this.localVideo.srcObject = this.localStream;
      }
      if (onEndedCallback != null) {
        stream.getAudioTracks()[0].onended = function (event) {
          onEndedCallback(event);
        };
      }
      if (this.isMuted) {
        this.muteLocalMic();
      } else {
        this.unmuteLocalMic();
      }
      if (this.localStreamSoundMeter != null) {
        this.enableAudioLevelForLocalStream(this.levelCallback);
      }
    }

    /**
     * This method updates the local stream. It removes existant video track from the local stream
     * and add the video track in `stream` parameter to the local stream
     */
    updateLocalVideoStream(stream, onEndedCallback, stopDesktop) {
      if (stopDesktop && this.desktopStream != null) {
        this.desktopStream.getVideoTracks()[0].stop();
      }
      var newVideoTrack = stream.getVideoTracks()[0];
      if (this.localStream != null && this.localStream.getVideoTracks()[0] != null) {
        var videoTrack = this.localStream.getVideoTracks()[0];
        if (videoTrack != newVideoTrack) {
          this.localStream.removeTrack(videoTrack);
          videoTrack.stop();
          this.localStream.addTrack(newVideoTrack);
        }
      } else if (this.localStream != null) {
        this.localStream.addTrack(newVideoTrack);
      } else {
        this.localStream = stream;
      }
      if (this.localVideo) {
        this.localVideo.srcObject = this.localStream;
      }
      if (onEndedCallback != null) {
        stream.getVideoTracks()[0].onended = function (event) {
          onEndedCallback(event);
        };
      }
    }

    /**
     * Called by User
     * to change video source
     *
     * @param {*} streamId
     * @param {*} deviceId
     */
    switchAudioInputSource(streamId, deviceId) {
      //stop the track because in some android devices need to close the current camera stream
      var audioTrack = this.localStream.getAudioTracks()[0];
      if (audioTrack) {
        audioTrack.stop();
      } else {
        Logger$3.warn("There is no audio track in local stream");
      }
      if (typeof deviceId != "undefined") {
        //Update the media constraints
        if (this.mediaConstraints.audio !== true) this.mediaConstraints.audio.deviceId = deviceId;else this.mediaConstraints.audio = {
          "deviceId": deviceId
        };

        //to change only audio track set video false otherwise issue #3826 occurs on Android
        var tempMediaConstraints = {
          "video": false,
          "audio": {
            "deviceId": deviceId
          }
        };
        return this.setAudioInputSource(streamId, tempMediaConstraints, null, deviceId);
      } else {
        return new Promise((resolve, reject) => {
          reject("There is no device id for audio input source");
        });
      }
    }

    /**
     * This method sets Audio Input Source and called when you change audio device
     * It calls updateAudioTrack function to update local audio stream.
     */
    setAudioInputSource(streamId, mediaConstraints, onEndedCallback) {
      return this.navigatorUserMedia(mediaConstraints, stream => {
        stream = this.setGainNodeStream(stream);
        return this.updateAudioTrack(stream, streamId, mediaConstraints, onEndedCallback);
      }, true);
    }

    /**
     * Called by User
     * to change video camera capture
     *
     * @param {*} streamId Id of the stream to be changed.
     * @param {*} deviceId Id of the device which will use as a media device
     * @param {*} onEndedCallback callback for when the switching video state is completed, can be used to understand if it is loading or not
     *
     * This method is used to switch to video capture.
     */
    switchVideoCameraCapture(streamId, deviceId, onEndedCallback) {
      //stop the track because in some android devices need to close the current camera stream
      if (this.localStream && this.localStream.getVideoTracks().length > 0) {
        var videoTrack = this.localStream.getVideoTracks()[0];
        videoTrack.stop();
      } else {
        Logger$3.warn("There is no video track in local stream");
      }
      this.publishMode = "camera";
      return navigator.mediaDevices.enumerateDevices().then(devices => {
        for (var i = 0; i < devices.length; i++) {
          if (devices[i].kind == "videoinput") {
            //Adjust video source only if there is a matching device id with the given one.
            //It creates problems if we don't check that since video can be just true to select default cam and it is like that in many cases.
            if (devices[i].deviceId == deviceId) {
              if (this.mediaConstraints.video !== true) this.mediaConstraints.video.deviceId = {
                exact: deviceId
              };else this.mediaConstraints.video = {
                deviceId: {
                  exact: deviceId
                }
              };
              break;
            }
          }
        }
        //If no matching device found don't adjust the media constraints let it be true instead of a device ID
        Logger$3.debug("Given deviceId = " + deviceId + " - Media constraints video property = " + this.mediaConstraints.video);
        return this.setVideoCameraSource(streamId, this.mediaConstraints, null, true, deviceId);
      });
    }

    /**
     * This method sets Video Input Source and called when you change video device
     * It calls updateVideoTrack function to update local video stream.
     */
    setVideoCameraSource(streamId, mediaConstraints, onEndedCallback, stopDesktop) {
      return this.navigatorUserMedia(mediaConstraints, stream => {
        if (stopDesktop && this.secondaryAudioTrackGainNode && stream.getAudioTracks().length > 0) {
          //This audio track update is necessary for such a case:
          //If you enable screen share with browser audio and then
          //return back to the camera, the audio should be only from mic.
          //If, we don't update audio with the following lines,
          //the mixed (mic+browser) audio would be streamed in the camera mode.
          this.secondaryAudioTrackGainNode = null;
          stream = this.setGainNodeStream(stream);
          this.updateAudioTrack(stream, streamId, mediaConstraints, onEndedCallback);
        }
        if (this.cameraEnabled) {
          return this.updateVideoTrack(stream, streamId, onEndedCallback, stopDesktop);
        } else {
          return this.turnOffLocalCamera();
        }
      }, true);
    }

    /**
     * Called by User
     * to switch between front and back camera on mobile devices
     *
     * @param {*} streamId Id of the stream to be changed.
     * @param {*} facingMode it can be "user" or "environment"
     *
     * This method is used to switch front and back camera.
     */
    switchVideoCameraFacingMode(streamId, facingMode) {
      //stop the track because in some android devices need to close the current camera stream
      if (this.localStream && this.localStream.getVideoTracks().length > 0) {
        var videoTrack = this.localStream.getVideoTracks()[0];
        videoTrack.stop();
      } else {
        Logger$3.warn("There is no video track in local stream");
      }

      // When device id set, facing mode is not working
      // so, remove device id
      if (this.mediaConstraints.video !== undefined && this.mediaConstraints.video.deviceId !== undefined) {
        delete this.mediaConstraints.video.deviceId;
      }
      var videoConstraint = {
        'facingMode': facingMode
      };
      this.mediaConstraints.video = Object.assign({}, this.mediaConstraints.video, videoConstraint);
      this.publishMode = "camera";
      Logger$3.debug("Media constraints video property = " + this.mediaConstraints.video);
      return this.setVideoCameraSource(streamId, {
        video: this.mediaConstraints.video
      }, null, true);
    }

    /**
     * Updates the audio track in the audio sender
     * getSender method is set on MediaManagercreation by WebRTCAdaptor
     *
     * @param {*} stream
     * @param {*} streamId
     * @param {*} onEndedCallback
     */
    updateAudioTrack(stream, streamId, onEndedCallback) {
      var audioTrackSender = this.getSender(streamId, "audio");
      if (audioTrackSender) {
        return audioTrackSender.replaceTrack(stream.getAudioTracks()[0]).then(result => {
          this.updateLocalAudioStream(stream, onEndedCallback);
        }).catch(function (error) {
          Logger$3.debug(error.name);
          throw error;
        });
      } else {
        this.updateLocalAudioStream(stream, onEndedCallback);
        return new Promise((resolve, reject) => {
          resolve();
        });
      }
    }

    /**
     * Updates the video track in the video sender
     * getSender method is set on MediaManagercreation by WebRTCAdaptor
     *
     * @param {*} stream
     * @param {*} streamId
     * @param {*} onEndedCallback
     */
    updateVideoTrack(stream, streamId, onEndedCallback, stopDesktop) {
      var videoTrackSender = this.getSender(streamId, "video");
      if (videoTrackSender) {
        return videoTrackSender.replaceTrack(stream.getVideoTracks()[0]).then(result => {
          this.updateLocalVideoStream(stream, onEndedCallback, stopDesktop);
        }).catch(error => {
          Logger$3.debug(error.name);
        });
      } else {
        this.updateLocalVideoStream(stream, onEndedCallback, stopDesktop);
        return new Promise((resolve, reject) => {
          resolve();
        });
      }
    }

    /**
     * If you mute turn off the camera still some data should be sent
     * Tihs method create a black frame to reduce data transfer
     */
    getBlackVideoTrack() {
      this.dummyCanvas.getContext('2d').fillRect(0, 0, 320, 240);

      //REFACTOR: it's not good to set to a replacement stream  
      this.replacementStream = this.dummyCanvas.captureStream();
      //We need to send black frames within a time interval, because when the user turn off the camera,
      //player can't connect to the sender since there is no data flowing. Sending a black frame in each 3 seconds resolves it.
      if (this.blackFrameTimer == null) {
        this.blackFrameTimer = setInterval(() => {
          this.getBlackVideoTrack();
        }, 3000);
      }
      this.blackVideoTrack = this.replacementStream.getVideoTracks()[0];
      return this.blackVideoTrack;
    }

    /**
     * Silent audio track
    */
    getSilentAudioTrack() {
      this.stopSilentAudioTrack();
      this.oscillator = this.audioContext.createOscillator();
      var dst = this.oscillator.connect(this.audioContext.createMediaStreamDestination());
      this.oscillator.start();
      this.silentAudioTrack = dst.stream.getAudioTracks()[0];
      return this.silentAudioTrack;
    }
    stopSilentAudioTrack() {
      if (this.oscillator != null) {
        this.oscillator.stop();
        this.oscillator.disconnect();
        this.oscillator = null;
      }
      if (this.silentAudioTrack != null) {
        this.silentAudioTrack.stop();
        this.silentAudioTrack = null;
      }
    }

    /**
     * Called by User
     * turns of the camera stream and starts streaming black dummy frame
     */
    turnOffLocalCamera(streamId) {
      //Initialize the first dummy frame for switching.
      this.getBlackVideoTrack();
      if (this.localStream != null) {
        var choosenId;
        if (streamId != null || typeof streamId != "undefined") {
          choosenId = streamId;
        } else {
          choosenId = this.publishStreamId;
        }
        this.cameraEnabled = false;
        return this.updateVideoTrack(this.replacementStream, choosenId, null, true);
      } else {
        return new Promise((resolve, reject) => {
          this.callbackError("NoActiveConnection");
          reject("NoActiveStream");
        });
      }
    }
    clearBlackVideoTrackTimer() {
      if (this.blackFrameTimer != null) {
        clearInterval(this.blackFrameTimer);
        this.blackFrameTimer = null;
      }
    }
    stopBlackVideoTrack() {
      if (this.blackVideoTrack != null) {
        this.blackVideoTrack.stop();
        this.blackVideoTrack = null;
      }
    }

    /**
     * Called by User
     * turns of the camera stream and starts streaming camera again instead of black dummy frame
     */
    turnOnLocalCamera(streamId) {
      this.clearBlackVideoTrackTimer();
      this.stopBlackVideoTrack();
      if (this.localStream == null) {
        return this.navigatorUserMedia(this.mediaConstraints, stream => {
          this.gotStream(stream);
        }, false);
      }
      //This method will get the camera track and replace it with dummy track
      else {
        return this.navigatorUserMedia(this.mediaConstraints, stream => {
          var choosenId;
          if (streamId != null || typeof streamId != "undefined") {
            choosenId = streamId;
          } else {
            choosenId = this.publishStreamId;
          }
          this.cameraEnabled = true;
          this.updateVideoTrack(stream, choosenId, null, true);
        }, false);
      }
    }

    /**
     * Called by User
     * to mute local audio streaming
     */
    muteLocalMic() {
      this.isMuted = true;
      if (this.localStream != null) {
        this.localStream.getAudioTracks().forEach(track => track.enabled = false);
      } else {
        this.callbackError("NoActiveConnection");
      }
    }

    /**
     * Called by User
     * to unmute local audio streaming
     *
     * if there is audio it calls callbackError with "AudioAlreadyActive" parameter
     */
    unmuteLocalMic() {
      this.isMuted = false;
      if (this.localStream != null) {
        this.localStream.getAudioTracks().forEach(track => track.enabled = true);
      } else {
        this.callbackError("NoActiveConnection");
      }
    }

    /**
     * If we have multiple video tracks in coming versions, this method may cause some issues
     */
    getVideoSender(streamId) {
      var videoSender = null;
      if (typeof adapter !== "undefined" && adapter !== null && (adapter.browserDetails.browser === 'chrome' || adapter.browserDetails.browser === 'firefox' || adapter.browserDetails.browser === 'safari' && adapter.browserDetails.version >= 64) && 'RTCRtpSender' in window && 'setParameters' in window.RTCRtpSender.prototype) {
        videoSender = this.getSender(streamId, "video");
      }
      return videoSender;
    }

    /**
     * Called by User
     * to set maximum video bandwidth is in kbps
     */
    changeBandwidth(bandwidth, streamId) {
      var errorDefinition = "";
      var videoSender = this.getVideoSender(streamId);
      if (videoSender != null) {
        var parameters = videoSender.getParameters();
        if (!parameters.encodings) {
          parameters.encodings = [{}];
        }
        if (bandwidth === 'unlimited') {
          delete parameters.encodings[0].maxBitrate;
        } else {
          parameters.encodings[0].maxBitrate = bandwidth * 1000;
        }
        return videoSender.setParameters(parameters);
      } else {
        errorDefinition = "Video sender not found to change bandwidth. Streaming may not be active";
      }
      return Promise.reject(errorDefinition);
    }
    /**
     * Called by user
     * sets the volume level
     *
     * @param {*} volumeLevel : Any number between 0 and 1.
     */
    setVolumeLevel(volumeLevel) {
      this.currentVolume = volumeLevel;
      if (this.primaryAudioTrackGainNode != null) {
        this.primaryAudioTrackGainNode.gain.value = volumeLevel;
      }
      if (this.secondaryAudioTrackGainNode != null) {
        this.secondaryAudioTrackGainNode.gain.value = volumeLevel;
      }
    }

    /**
     * Called by user
     * To create a sound meter for the local stream
     *
     * @param {Function} levelCallback : callback to provide the audio level to user
     * @param {*} period : measurement period
     */
    enableAudioLevelForLocalStream(levelCallback) {
      this.levelCallback = levelCallback;
      this.disableAudioLevelForLocalStream();
      this.localStreamSoundMeter = new SoundMeter(this.audioContext);
      if (this.audioContext.state !== 'running') {
        return this.audioContext.resume().then(() => {
          return this.localStreamSoundMeter.connectToSource(this.localStream, levelCallback);
        });
      } else {
        return this.localStreamSoundMeter.connectToSource(this.localStream, levelCallback);
      }
    }
    disableAudioLevelForLocalStream() {
      if (this.localStreamSoundMeter != null) {
        this.localStreamSoundMeter.stop();
        this.localStreamSoundMeter = null;
      }
    }

    /**
     * Called by user
     * To change audio/video constraints on the fly
     *
     */
    applyConstraints(newConstraints) {
      var constraints = {};
      if (newConstraints.audio === undefined && newConstraints.video === undefined) {
        //if audio or video field is not defined, assume that it's a video constraint
        constraints.video = newConstraints;
        this.mediaConstraints.video = Object.assign({}, this.mediaConstraints.video, constraints.video);
      } else if (newConstraints.video !== undefined) {
        constraints.video = newConstraints.video;
        this.mediaConstraints.video = Object.assign({}, this.mediaConstraints.video, constraints.video);
      }
      if (newConstraints.audio !== undefined) {
        constraints.audio = newConstraints.audio;
        this.mediaConstraints.audio = Object.assign({}, this.mediaConstraints.audio, constraints.audio);
      }
      var promise = null;
      if (constraints.video !== undefined) {
        if (this.localStream && this.localStream.getVideoTracks().length > 0) {
          var videoTrack = this.localStream.getVideoTracks()[0];
          promise = videoTrack.applyConstraints(this.mediaConstraints.video);
        } else {
          promise = new Promise((resolve, reject) => {
            reject("There is no video track to apply constraints");
          });
        }
      }
      if (constraints.audio !== undefined) {
        //just give the audio constraints not to get video stream
        //we dont call applyContrains for audio because it does not work. I think this is due to gainStream things. This is why we call getUserMedia again

        //use the publishStreamId because we don't have streamId in the parameter anymore
        promise = this.setAudioInputSource(this.publishStreamId, {
          audio: this.mediaConstraints.audio
        }, null);
      }
      if (this.localStreamSoundMeter != null) {
        this.enableAudioLevelForLocalStream(this.levelCallback);
      }
      return promise;
    }
  }

  var Logger$2 = window.log;

  /**
   * This structure is used to handle large size data channel messages (like image)
   * which should be splitted into chunks while sending and receiving.
   *
   */
  class ReceivingMessage {
    /**
     *
     * @param {number} size
     */
    constructor(size) {
      this.size = size;
      this.received = 0;
      this.data = new ArrayBuffer(size);
    }
  }

  /**
   * WebRTCAdaptor Class is interface to the JS SDK of Ant Media Server (AMS). This class manages the signalling,
   * keeps the states of peers.
   *
   * This class is used for peer-to-peer signalling,
   * publisher and player signalling and conference.
   *
   * Also it is responsible for some room management in conference case.
   *
   * There are different use cases in AMS. This class is used for all of them.
   *
   * WebRTC Publish
   * WebRTC Play
   * WebRTC Data Channel Connection
   * WebRTC Conference
   * WebRTC Multitrack Play
   * WebRTC Multitrack Conference
   * WebRTC peer-to-peer session
   *
   */
  class WebRTCAdaptor {
    /**
     * Register plugins to the WebRTCAdaptor
     * @param {Function} plugin
     */
    static register(pluginInitMethod) {
      WebRTCAdaptor.pluginInitMethods.push(pluginInitMethod);
    }
    /**
     *
     * @param {object} initialValues
     */
    constructor(initialValues) {
      /**
       * PeerConnection configuration while initializing the PeerConnection.
       * https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection/RTCPeerConnection#parameters
       *
       * More than one STURN and/or TURN servers can be added.  Here is a typical turn server configuration
       *
       *    {
       * 	  urls: "",
       *	  username: "",
       *    credential: "",
       *	}
       *
       *  Default value is the google stun server
       */
      this.peerconnection_config = {
        'iceServers': [{
          'urls': 'stun:stun1.l.google.com:19302'
        }],
        sdpSemantics: 'unified-plan'
      };

      /**
       * Used while creating SDP (answer or offer)
       * https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection/createOffer#parameters
       */
      this.sdp_constraints = {
        OfferToReceiveAudio: false,
        OfferToReceiveVideo: false
      };

      /**
       * This keeps the PeerConnections for each stream id.
       * It is an array because one @WebRTCAdaptor instance can manage multiple WebRTC connections as in the conference.
       * Its indices are the Stream Ids of each stream
       */
      this.remotePeerConnection = new Array();

      /**
       * This keeps statistics for the each PeerConnection.
       * It is an array because one @WebRTCAdaptor instance can manage multiple WebRTC connections as in the conference.
       * Its indices are the Stream Ids of each stream
       */
      this.remotePeerConnectionStats = new Array();

      /**
       * This keeps the Remote Description (SDP) set status for each PeerConnection.
       * We need to keep this status because sometimes ice candidates from the remote peer
       * may come before the Remote Description (SDP). So we need to store those ice candidates
       * in @iceCandidateList field until we get and set the Remote Description.
       * Otherwise setting ice candidates before Remote description may cause problem.
       */
      this.remoteDescriptionSet = new Array();

      /**
       * This keeps the Ice Candidates which are received before the Remote Description (SDP) received.
       * For details please check @remoteDescriptionSet field.
       */
      this.iceCandidateList = new Array();

      /**
       * This is the name for the room that is desired to join in conference mode.
       */
      this.roomName = null;

      /**
       * This keeps StreamIds for the each playing session.
       * It is an array because one @WebRTCAdaptor instance can manage multiple playing sessions.
       */
      this.playStreamId = new Array();

      /**
       * This is the flag indicates if multiple peers will join a peer in the peer to peer mode.
       * This is used only with Embedded SDk
       */
      this.isMultiPeer = false;

      /**
       * This is the stream id that multiple peers can join a peer in the peer to peer mode.
       * This is used only with Embedded SDk
       */
      this.multiPeerStreamId = null;

      /**
       * This is instance of @WebSocketAdaptor and manages to websocket connection.
       * All signalling messages are sent to/recived from
       * the Ant Media Server over this web socket connection
       */
      this.webSocketAdaptor = null;

      /**
       * This flags indicates if this @WebRTCAdaptor instance is used only for playing session(s)
       * You don't need camera/mic access in play mode
       */
      this.isPlayMode = false;

      /**
       * This flags enables/disables debug logging
       */
      this.debug = false;

      /**
       * This is the Stream Id for the publisher. One @WebRCTCAdaptor supports only one publishing
       * session for now (23.02.2022).
       * In conference mode you can join a room with null stream id. In that case
       * Ant Media Server generates a stream id and provides it JoinedTheRoom callback and it is set to this field.
       */
      this.publishStreamId = null;

      /**
       * This is used to keep stream id and track id (which is provided in SDP) mapping
       * in MultiTrack Playback and conference.
       */
      this.idMapping = new Array();

      /**
       * This is used when only data is brodcasted with the same way video and/or audio.
       * The difference is that no video or audio is sent when this field is true
       */
      this.onlyDataChannel = false;

      /**
       * While publishing and playing streams data channel is enabled by default
       */
      this.dataChannelEnabled = true;

      /**
       * This is array of @ReceivingMessage
       * When you receive multiple large size messages @ReceivingMessage simultaneously
       * this map is used to indicate them with its index tokens.
       */
      this.receivingMessages = new Map();

      /**
       * Supported candidate types. Below types are for both sending and receiving candidates.
       * It means if when client receives candidate from STUN server, it sends to the server if candidate's protocol
       * is in the list. Likely, when client receives remote candidate from server, it adds as ice candidate
       * if candidate protocol is in the list below.
       */
      this.candidateTypes = ["udp", "tcp"];

      /**
       * Method to call when there is an event happened
       */
      this.callback = null;

      /**
       * Method to call when there is an error happened
       */
      this.callbackError = null;

      /**
       * Flag to indicate if the stream is published or not after the connection fails
       */
      this.reconnectIfRequiredFlag = true;

      /**
       * websocket url to connect
       * @deprecated use websocketURL
       */
      this.websocket_url = null;

      /**
       * Websocket URL
       */
      this.websocketURL = null;

      /**
       * flag to initialize components in constructor
       */
      this.initializeComponents = true;

      /**
       * PAY ATTENTION: The values of the above fields are provided as this constructor parameter.
       * TODO: Also some other hidden parameters may be passed here
       */
      for (var key in initialValues) {
        if (initialValues.hasOwnProperty(key)) {
          this[key] = initialValues[key];
        }
      }
      if (this.websocketURL == null) {
        this.websocketURL = this.websocket_url;
      }
      if (this.websocketURL == null) {
        throw new Error("WebSocket URL is not defined. It's mandatory");
      }
      /**
       * The html video tag for receiver is got here
       */
      this.remoteVideo = this.remoteVideoElement || document.getElementById(this.remoteVideoId);

      /**
       * Keeps the sound meters for each connection. Its index is stream id
       */
      this.soundMeters = new Array();

      /**
       * Keeps the current audio level for each playing streams in conference mode
       */
      this.soundLevelList = new Array();

      /**
       * This is the event listeners that WebRTC Adaptor calls when there is a new event happened
       */
      this.eventListeners = new Array();

      /**
       * This is the error event listeners that WebRTC Adaptor calls when there is an error happened
       */
      this.errorEventListeners = new Array();

      /**
       * This is token that is being used to publish the stream. It's added here to use in reconnect scenario
       */
      this.publishToken = null;

      /**
       * subscriber id that is being used to publish the stream. It's added here to use in reconnect scenario
       */
      this.publishSubscriberId = null;

      /**
       * subscriber code that is being used to publish the stream. It's added here to use in reconnect scenario
       */
      this.publishSubscriberCode = null;

      /**
       * This is the stream name that is being published. It's added here to use in reconnect scenario
       */
      this.publishStreamName = null;

      /**
       * This is the stream id of the main track that the current publishStreamId is going to be subtrack of it. It's added here to use in reconnect scenario
       */
      this.publishMainTrack = null;

      /**
       * This is the metadata that is being used to publish the stream. It's added here to use in reconnect scenario
       */
      this.publishMetaData = null;

      /**
       * This is the token to play the stream. It's added here to use in reconnect scenario
       */
      this.playToken = null;

      /**
       * This is the room id to play the stream. It's added here to use in reconnect scenario
       * This approach is old conferencing. It's better to use multi track conferencing
       */
      this.playRoomId = null;

      /**
       * These are enabled tracks to play the stream. It's added here to use in reconnect scenario
       */
      this.playEnableTracks = null;

      /**
       * This is the subscriber Id to play the stream. It's added here to use in reconnect scenario
       */
      this.playSubscriberId = null;

      /**
       * This is the subscriber code to play the stream. It's added here to use in reconnect scenario
       */
      this.playSubscriberCode = null;

      /**
       * This is the meta data to play the stream. It's added here to use in reconnect scenario
       */
      this.playMetaData = null;

      /**
       * This is the time info for the last reconnection attempt
       */
      this.lastReconnectiontionTrialTime = 0;

      /**
       * All media management works for teh local stream are made by @MediaManager class.
       * for details please check @MediaManager
       */
      this.mediaManager = new MediaManager({
        userParameters: initialValues,
        webRTCAdaptor: this,
        callback: (info, obj) => {
          this.notifyEventListeners(info, obj);
        },
        callbackError: (error, message) => {
          this.notifyErrorEventListeners(error, message);
        },
        getSender: (streamId, type) => {
          return this.getSender(streamId, type);
        }
      });

      //Initialize the local stream (if needed) and web socket connection
      if (this.initializeComponents) {
        this.initialize();
      }
    }

    /**
     * Init plugins
     */
    initPlugins() {
      WebRTCAdaptor.pluginInitMethods.forEach(initMethod => {
        initMethod(this);
      });
    }

    /**
     * Add event listener to be notified. This is generally for plugins
     * @param {*} listener
     */
    addEventListener(listener) {
      this.eventListeners.push(listener);
    }

    /**
     * Add error event listener to be notified. Thisis generally for plugins
     * @param {*} errorListener
     */
    addErrorEventListener(errorListener) {
      this.errorEventListeners.push(errorListener);
    }

    /**
     * Notify event listeners and callback method
     * @param {*} info
     * @param {*} obj
     */
    notifyEventListeners(info, obj) {
      this.eventListeners.forEach(listener => {
        listener(info, obj);
      });
      if (this.callback != null) {
        this.callback(info, obj);
      }
    }

    /**
     * Notify error event listeners and callbackError method
     * @param {*} error
     * @param {*} message
     */
    notifyErrorEventListeners(error, message) {
      this.errorEventListeners.forEach(listener => {
        listener(error, message);
      });
      if (this.callbackError != null) {
        this.callbackError(error, message);
      }
    }

    /**
     * Called by constuctor to
     *    -check local stream unless it is in play mode
     *    -start websocket connection
     */
    initialize() {
      if (!this.isPlayMode && !this.onlyDataChannel && this.mediaManager.localStream == null) {
        //we need local stream because it not a play mode
        return this.mediaManager.initLocalStream().then(() => {
          this.initPlugins();
          this.checkWebSocketConnection();
          return new Promise((resolve, reject) => {
            resolve("Wait 'initialized' callback from websocket");
          });
        }).catch(error => {
          Logger$2.warn(error);
          throw error;
        });
      }
      return new Promise((resolve, reject) => {
        this.initPlugins();
        this.checkWebSocketConnection();
        resolve("Wait 'initialized' callback from websocket");
      });
    }

    /**
     * Called to start a new WebRTC stream. AMS responds with start message.
     * Parameters:
     *  @param {string} streamId : unique id for the stream
     *  @param {string=} [token] : required if any stream security (token control) enabled. Check https://github.com/ant-media/Ant-Media-Server/wiki/Stream-Security-Documentation
     *  @param {string=} [subscriberId] : required if TOTP enabled. Check https://github.com/ant-media/Ant-Media-Server/wiki/Time-based-One-Time-Password-(TOTP)
     *  @param {string=} [subscriberCode] : required if TOTP enabled. Check https://github.com/ant-media/Ant-Media-Server/wiki/Time-based-One-Time-Password-(TOTP)
     *  @param {string=} [streamName] : required if you want to set a name for the stream
     *  @param {string=} [mainTrack] :  required if you want to start the stream as a subtrack for a main stream which has id of this parameter.
     *                Check:https://antmedia.io/antmediaserver-webrtc-multitrack-playing-feature/
     *                !!! for multitrack conference set this value with roomName
     *  @param {string=} [metaData] : a free text information for the stream to AMS. It is provided to Rest methods by the AMS
     */
    publish(streamId, token, subscriberId, subscriberCode, streamName, mainTrack, metaData) {
      //TODO: should refactor the repeated code
      this.publishStreamId = streamId;
      this.mediaManager.publishStreamId = streamId;
      this.publishToken = token;
      this.publishSubscriberId = subscriberId;
      this.publishSubscriberCode = subscriberCode;
      this.publishStreamName = streamName;
      this.publishMainTrack = mainTrack;
      this.publishMetaData = metaData;
      if (this.onlyDataChannel) {
        this.sendPublishCommand(streamId, token, subscriberId, subscriberCode, streamName, mainTrack, metaData, false, false);
      }
      //If it started with playOnly mode and wants to publish now
      else if (this.mediaManager.localStream == null) {
        this.mediaManager.initLocalStream().then(() => {
          var videoEnabled = false;
          var audioEnabled = false;
          if (this.mediaManager.localStream != null) {
            videoEnabled = this.mediaManager.localStream.getVideoTracks().length > 0;
            audioEnabled = this.mediaManager.localStream.getAudioTracks().length > 0;
          }
          this.sendPublishCommand(streamId, token, subscriberId, subscriberCode, streamName, mainTrack, metaData, videoEnabled, audioEnabled);
        }).catch(error => {
          Logger$2.warn(error);
          throw error;
        });
      } else {
        var videoEnabled = this.mediaManager.localStream.getVideoTracks().length > 0;
        var audioEnabled = this.mediaManager.localStream.getAudioTracks().length > 0;
        this.sendPublishCommand(streamId, token, subscriberId, subscriberCode, streamName, mainTrack, metaData, videoEnabled, audioEnabled);
      }
      //init peer connection for reconnectIfRequired
      this.initPeerConnection(streamId, "publish");
      setTimeout(() => {
        //check if it is connected or not
        //this resolves if the server responds with some error message
        if (this.iceConnectionState(this.publishStreamId) != "checking" && this.iceConnectionState(this.publishStreamId) != "connected" && this.iceConnectionState(this.publishStreamId) != "completed") {
          //if it is not connected, try to reconnect
          this.reconnectIfRequired(0);
        }
      }, 5000);
    }
    sendPublishCommand(streamId, token, subscriberId, subscriberCode, streamName, mainTrack, metaData, videoEnabled, audioEnabled) {
      var jsCmd = {
        command: "publish",
        streamId: streamId,
        token: token,
        subscriberId: typeof subscriberId !== undefined && subscriberId != null ? subscriberId : "",
        subscriberCode: typeof subscriberCode !== undefined && subscriberCode != null ? subscriberCode : "",
        streamName: typeof streamName !== undefined && streamName != null ? streamName : "",
        mainTrack: typeof mainTrack !== undefined && mainTrack != null ? mainTrack : "",
        video: videoEnabled,
        audio: audioEnabled,
        metaData: typeof metaData !== undefined && metaData != null ? metaData : ""
      };
      this.webSocketAdaptor.send(JSON.stringify(jsCmd));
    }

    /**
     * Called to join a room. AMS responds with joinedTheRoom message.
     * Parameters:
     * @param {string} roomName : unique id of the room
     * @param {string=} streamId : unique id of the stream belongs to this participant
     * @param {string=} mode :    legacy for older implementation (default value)
     *            mcu for merging streams
     *            amcu: audio only conferences with mixed audio
     */
    joinRoom(roomName, streamId, mode) {
      this.roomName = roomName;
      var jsCmd = {
        command: "joinRoom",
        room: roomName,
        streamId: streamId,
        mode: mode
      };
      this.webSocketAdaptor.send(JSON.stringify(jsCmd));
    }

    /**
     * Called to start a playing session for a stream. AMS responds with start message.
     * Parameters:
     *  @param {string} streamId :(string) unique id for the stream that you want to play
     *  @param {string=} token :(string) required if any stream security (token control) enabled. Check https://github.com/ant-media/Ant-Media-Server/wiki/Stream-Security-Documentation
     *  @param {string=} roomId :(string) required if this stream is belonging to a room participant
     *  @param {Array.<MediaStreamTrack>=} enableTracks :(array) required if the stream is a main stream of multitrack playing. You can pass the the subtrack id list that you want to play.
     *                    you can also provide a track id that you don't want to play by adding ! before the id.
     *  @param {string=} subscriberId :(string) required if TOTP enabled. Check https://github.com/ant-media/Ant-Media-Server/wiki/Time-based-One-Time-Password-(TOTP)
     *  @param {string=} subscriberCode :(string) required if TOTP enabled. Check https://github.com/ant-media/Ant-Media-Server/wiki/Time-based-One-Time-Password-(TOTP)
     *  @param {string=} metaData :(string, json) a free text information for the stream to AMS. It is provided to Rest methods by the AMS
     */
    play(streamId, token, roomId, enableTracks, subscriberId, subscriberCode, metaData) {
      this.playStreamId.push(streamId);
      this.playToken = token;
      this.playRoomId = roomId;
      this.playEnableTracks = enableTracks;
      this.playSubscriberId = subscriberId;
      this.playSubscriberCode = subscriberCode;
      this.playMetaData = metaData;
      var jsCmd = {
        command: "play",
        streamId: streamId,
        token: typeof token !== undefined && token != null ? token : "",
        room: typeof roomId !== undefined && roomId != null ? roomId : "",
        trackList: typeof enableTracks !== undefined && enableTracks != null ? enableTracks : [],
        subscriberId: typeof subscriberId !== undefined && subscriberId != null ? subscriberId : "",
        subscriberCode: typeof subscriberCode !== undefined && subscriberId != null ? subscriberCode : "",
        viewerInfo: typeof metaData !== undefined && metaData != null ? metaData : ""
      };
      this.webSocketAdaptor.send(JSON.stringify(jsCmd));

      //init peer connection for reconnectIfRequired
      this.initPeerConnection(streamId, "play");
      setTimeout(() => {
        //check if it is connected or not
        //this resolves if the server responds with some error message
        if (this.iceConnectionState(streamId) != "checking" && this.iceConnectionState(streamId) != "connected" && this.iceConnectionState(streamId) != "completed") {
          //if it is not connected, try to reconnect
          this.reconnectIfRequired(0);
        }
      }, 5000);
    }

    /**
     * Reconnects to the stream if it is not stopped on purpose
     * @param {number} [delayMs]
     * @returns
     */
    reconnectIfRequired() {
      var delayMs = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : 3000;
      if (this.reconnectIfRequiredFlag) {
        //It's important to run the following methods after 3000 ms because the stream may be stopped by the user in the meantime
        if (delayMs > 0) {
          setTimeout(() => {
            this.tryAgain();
          }, delayMs);
        } else {
          this.tryAgain();
        }
      }
    }
    tryAgain() {
      var now = Date.now();
      //to prevent too many trial from different paths
      if (now - this.lastReconnectiontionTrialTime < 3000) {
        return;
      }
      this.lastReconnectiontionTrialTime = now;

      //reconnect publish
      //if remotePeerConnection has a peer connection for the stream id, it means that it is not stopped on purpose

      if (this.remotePeerConnection[this.publishStreamId] != null &&
      //check connection status to not stop streaming an active stream
      this.iceConnectionState(this.publishStreamId) != "checking" && this.iceConnectionState(this.publishStreamId) != "connected" && this.iceConnectionState(this.publishStreamId) != "completed") {
        // notify that reconnection process started for publish
        this.notifyEventListeners("reconnection_attempt_for_publisher", this.publishStreamId);
        this.closePeerConnection(this.publishStreamId);
        Logger$2.log("It will try to publish again because it is not stopped on purpose");
        this.publish(this.publishStreamId, this.publishToken, this.publishSubscriberId, this.publishSubscriberCode, this.publishStreamName, this.publishMainTrack, this.publishMetaData);
      }

      //reconnect play
      for (var index in this.playStreamId) {
        var streamId = this.playStreamId[index];
        if (this.remotePeerConnection[streamId] != "null" &&
        //check connection status to not stop streaming an active stream
        this.iceConnectionState(streamId) != "checking" && this.iceConnectionState(streamId) != "connected" && this.iceConnectionState(streamId) != "completed") {
          // notify that reconnection process started for play
          this.notifyEventListeners("reconnection_attempt_for_player", streamId);
          Logger$2.log("It will try to play again because it is not stopped on purpose");
          this.closePeerConnection(streamId);
          this.play(streamId, this.playToken, this.playRoomId, this.playEnableTracks, this.playSubscriberId, this.playSubscriberCode, this.playMetaData);
        }
      }
    }

    /**
     * Called to stop a publishing/playing session for a stream. AMS responds with publishFinished or playFinished message.
     * Parameters:
     *  @param {string} streamId : unique id for the stream that you want to stop publishing or playing
     */
    stop(streamId) {
      //stop is called on purpose and it deletes the peer connection from remotePeerConnections
      this.closePeerConnection(streamId);
      if (this.webSocketAdaptor != null && this.webSocketAdaptor.isConnected()) {
        var jsCmd = {
          command: "stop",
          streamId: streamId
        };
        this.webSocketAdaptor.send(JSON.stringify(jsCmd));
      }
    }

    /**
     * Called to join a peer-to-peer mode session as peer. AMS responds with joined message.
     * Parameters:
     * @param {string} streamId : unique id for the peer-to-peer session
     */
    join(streamId) {
      var jsCmd = {
        command: "join",
        streamId: streamId,
        multiPeer: this.isMultiPeer && this.multiPeerStreamId == null,
        mode: this.isPlayMode ? "play" : "both"
      };
      this.webSocketAdaptor.send(JSON.stringify(jsCmd));
    }

    /**
     * Called by browser when a new track is added to WebRTC connetion. This is used to infor html pages with newStreamAvailable callback.
     * Parameters:
     * 	 event: TODO
     * 	 streamId: unique id for the stream
     */
    onTrack(event, streamId) {
      Logger$2.debug("onTrack for stream");
      if (this.remoteVideo != null) {
        if (this.remoteVideo.srcObject !== event.streams[0]) {
          this.remoteVideo.srcObject = event.streams[0];
          Logger$2.debug('Received remote stream');
        }
      } else {
        var dataObj = {
          stream: event.streams[0],
          track: event.track,
          streamId: streamId,
          trackId: this.idMapping[streamId][event.transceiver.mid]
        };
        this.notifyEventListeners("newTrackAvailable", dataObj);

        //deprecated. Listen newTrackAvailable in callback. It's kept for backward compatibility
        this.notifyEventListeners("newStreamAvailable", dataObj);
      }
    }

    /**
     * Called to leave from a conference room. AMS responds with leavedTheRoom message.
     * Parameters:
     * @param {string} roomName : unique id for the conference room
     */
    leaveFromRoom(roomName) {
      for (var key in this.remotePeerConnection) {
        this.closePeerConnection(key);
      }
      this.roomName = roomName;
      var jsCmd = {
        command: "leaveFromRoom",
        room: roomName
      };
      Logger$2.debug("leave request is sent for " + roomName);
      this.webSocketAdaptor.send(JSON.stringify(jsCmd));
    }

    /**
     * Called to leave from a peer-to-peer mode session. AMS responds with leaved message.
     * Parameters:
     * @param {string} streamId : unique id for the peer-to-peer session
     */
    leave(streamId) {
      var jsCmd = {
        command: "leave",
        streamId: this.isMultiPeer && this.multiPeerStreamId != null ? this.multiPeerStreamId : streamId
      };
      this.webSocketAdaptor.send(JSON.stringify(jsCmd));
      this.closePeerConnection(streamId);
      this.multiPeerStreamId = null;
    }

    /**
     * Called to get a stream information for a specific stream. AMS responds with streamInformation message.
     * Parameters:
     * @param {string} streamId : unique id for the stream that you want to get info about
     */
    getStreamInfo(streamId) {
      var jsCmd = {
        command: "getStreamInfo",
        streamId: streamId
      };
      this.webSocketAdaptor.send(JSON.stringify(jsCmd));
    }

    /**
     * Called to get the list of video track assignments. AMS responds with the videoTrackAssignmentList message.
     * Parameters:
     * @param {string} streamId : unique id for the stream that you want to get info about
     */
    requestVideoTrackAssignments(streamId) {
      var jsCmd = {
        command: "getVideoTrackAssignments",
        streamId: streamId
      };
      this.webSocketAdaptor.send(JSON.stringify(jsCmd));
    }

    /**
     * Called to get the broadcast object for a specific stream. AMS responds with the broadcastObject callback.
     * Parameters:
     * @param {string} streamId : unique id for the stream that you want to get info about
     */
    getBroadcastObject(streamId) {
      var jsCmd = {
        command: "getBroadcastObject",
        streamId: streamId
      };
      this.webSocketAdaptor.send(JSON.stringify(jsCmd));
    }

    /**
     * Called to update the meta information for a specific stream.
     * Parameters:
     * @param {string} streamId : unique id for the stream that you want to update MetaData
     * @param {string}  metaData : new free text information for the stream
     */
    updateStreamMetaData(streamId, metaData) {
      var jsCmd = {
        command: "updateStreamMetaData",
        streamId: streamId,
        metaData: metaData
      };
      this.webSocketAdaptor.send(JSON.stringify(jsCmd));
    }

    /**
     * Called to get the room information for a specific room. AMS responds with roomInformation message
     * which includes the ids and names of the streams in that room.
     * If there is no active streams in the room, AMS returns error `no_active_streams_in_room` in error callback
     * Parameters:
     * @param {string} roomName : unique id for the room that you want to get info about
     * @param {string} streamId : unique id for the stream that is streamed by this @WebRTCAdaptor
     */
    getRoomInfo(roomName, streamId) {
      var jsCmd = {
        command: "getRoomInfo",
        streamId: streamId,
        room: roomName
      };
      this.webSocketAdaptor.send(JSON.stringify(jsCmd));
    }

    /**
     * Called to enable/disable data flow from the AMS for a specific track under a main track.
     * Parameters:
     * @param {string}  mainTrackId : unique id for the main stream
     * @param {string}  trackId : unique id for the track that you want to enable/disable data flow for
     * @param {boolean} enabled : true or false
     */
    enableTrack(mainTrackId, trackId, enabled) {
      var jsCmd = {
        command: "enableTrack",
        streamId: mainTrackId,
        trackId: trackId,
        enabled: enabled
      };
      this.webSocketAdaptor.send(JSON.stringify(jsCmd));
    }

    /**
     * Called to get the track ids under a main stream. AMS responds with trackList message.
     * Parameters:
     * @param {string} streamId : unique id for the main stream
     * @param {string=} [token] : not used
     * TODO: check this function
     */
    getTracks(streamId, token) {
      this.playStreamId.push(streamId);
      var jsCmd = {
        command: "getTrackList",
        streamId: streamId,
        token: token
      };
      this.webSocketAdaptor.send(JSON.stringify(jsCmd));
    }

    /**
     * Called by WebSocketAdaptor when a new ice candidate is received from AMS.
     * Parameters:
     *     event: TODO
     *     streamId: unique id for the stream
     */
    iceCandidateReceived(event, streamId) {
      if (event.candidate) {
        var protocolSupported = false;
        if (event.candidate.candidate == "") {
          //event candidate can be received and its value can be "".
          //don't compare the protocols
          protocolSupported = true;
        } else if (typeof event.candidate.protocol == "undefined") {
          this.candidateTypes.forEach(element => {
            if (event.candidate.candidate.toLowerCase().includes(element)) {
              protocolSupported = true;
            }
          });
        } else {
          protocolSupported = this.candidateTypes.includes(event.candidate.protocol.toLowerCase());
        }
        if (protocolSupported) {
          var jsCmd = {
            command: "takeCandidate",
            streamId: streamId,
            label: event.candidate.sdpMLineIndex,
            id: event.candidate.sdpMid,
            candidate: event.candidate.candidate
          };
          if (this.debug) {
            Logger$2.debug("sending ice candiate for stream Id " + streamId);
            Logger$2.debug(JSON.stringify(event.candidate));
          }
          this.webSocketAdaptor.send(JSON.stringify(jsCmd));
        } else {
          Logger$2.debug("Candidate's protocol(full sdp: " + event.candidate.candidate + ") is not supported. Supported protocols: " + this.candidateTypes);
          if (event.candidate.candidate != "") {
            //
            this.notifyErrorEventListeners("protocol_not_supported", "Support protocols: " + this.candidateTypes.toString() + " candidate: " + event.candidate.candidate);
          }
        }
      } else {
        Logger$2.debug("No event.candidate in the iceCandidate event");
      }
    }

    /**
     * Called internally to sanitize the text if it contains script to prevent xss
     * @param text
     * @returns {*}
     */
    sanitizeHTML(text) {
      if (text.includes("script")) return text.replace(/</g, "&lt;").replace(/>/g, "&gt;");
      return text;
    }

    /**
     * Called internally to initiate Data Channel.
     * Note that Data Channel should be enabled fromAMS settings.
     *  @param {string}  streamId : unique id for the stream
     *  @param {*} dataChannel : provided by PeerConnection
     */
    initDataChannel(streamId, dataChannel) {
      dataChannel.onerror = error => {
        Logger$2.debug("Data Channel Error:", error);
        var obj = {
          streamId: streamId,
          error: error
        };
        Logger$2.debug("channel status: ", dataChannel.readyState);
        if (dataChannel.readyState != "closed") {
          this.notifyErrorEventListeners("data_channel_error", obj);
        }
      };
      dataChannel.onmessage = event => {
        var obj = {
          streamId: streamId,
          data: event.data
        };
        var data = obj.data;
        if (typeof data === 'string' || data instanceof String) {
          obj.data = this.sanitizeHTML(obj.data);
          this.notifyEventListeners("data_received", obj);
        } else {
          var length = data.length || data.size || data.byteLength;
          var view = new Int32Array(data, 0, 1);
          var token = view[0];
          var msg = this.receivingMessages[token];
          if (msg == undefined) {
            var view = new Int32Array(data, 0, 2);
            var size = view[1];
            msg = new ReceivingMessage(size);
            this.receivingMessages[token] = msg;
            if (length > 8) {
              Logger$2.debug("something went wrong in msg receiving");
            }
            return;
          }
          var rawData = data.slice(4, length);
          var dataView = new Uint8Array(msg.data);
          dataView.set(new Uint8Array(rawData), msg.received, length - 4);
          msg.received += length - 4;
          if (msg.size == msg.received) {
            obj.data = msg.data;
            this.notifyEventListeners("data_received", obj);
          }
        }
      };
      dataChannel.onopen = () => {
        this.remotePeerConnection[streamId].dataChannel = dataChannel;
        Logger$2.debug("Data channel is opened");
        this.notifyEventListeners("data_channel_opened", streamId);
      };
      dataChannel.onclose = () => {
        Logger$2.debug("Data channel is closed");
        this.notifyEventListeners("data_channel_closed", streamId);
      };
    }

    /**
     * Called internally to initiate PeerConnection.
     * @param {string} streamId : unique id for the stream
     * @param {string}  dataChannelMode : can be "publish" , "play" or "peer" based on this it is decided which way data channel is created
     */
    initPeerConnection(streamId, dataChannelMode) {
      //null == undefined -> it's true
      //null === undefined -> it's false

      if (this.remotePeerConnection[streamId] == null) {
        var closedStreamId = streamId;
        Logger$2.debug("stream id in init peer connection: " + streamId + " close stream id: " + closedStreamId);
        this.remotePeerConnection[streamId] = new RTCPeerConnection(this.peerconnection_config);
        this.remoteDescriptionSet[streamId] = false;
        this.iceCandidateList[streamId] = new Array();
        if (!this.playStreamId.includes(streamId)) {
          if (this.mediaManager.localStream != null) {
            //AddStream is deprecated thus updated to the addTrack after version 2.4.2.1
            this.mediaManager.localStream.getTracks().forEach(track => this.remotePeerConnection[streamId].addTrack(track, this.mediaManager.localStream));
          }
        }
        this.remotePeerConnection[streamId].onicecandidate = event => {
          this.iceCandidateReceived(event, closedStreamId);
        };
        this.remotePeerConnection[streamId].ontrack = event => {
          this.onTrack(event, closedStreamId);
        };
        this.remotePeerConnection[streamId].onnegotiationneeded = event => {
          Logger$2.debug("onnegotiationneeded");
        };
        if (this.dataChannelEnabled) {
          // skip initializing data channel if it is disabled
          if (dataChannelMode == "publish") {
            //open data channel if it's publish mode peer connection
            var dataChannelOptions = {
              ordered: true
            };
            if (this.remotePeerConnection[streamId].createDataChannel) {
              var dataChannel = this.remotePeerConnection[streamId].createDataChannel(streamId, dataChannelOptions);
              this.initDataChannel(streamId, dataChannel);
            } else {
              Logger$2.warn("CreateDataChannel is not supported");
            }
          } else if (dataChannelMode == "play") {
            //in play mode, server opens the data channel
            this.remotePeerConnection[streamId].ondatachannel = ev => {
              this.initDataChannel(streamId, ev.channel);
            };
          } else {
            //for peer mode do both for now
            var _dataChannelOptions = {
              ordered: true
            };
            if (this.remotePeerConnection[streamId].createDataChannel) {
              var dataChannelPeer = this.remotePeerConnection[streamId].createDataChannel(streamId, _dataChannelOptions);
              this.initDataChannel(streamId, dataChannelPeer);
              this.remotePeerConnection[streamId].ondatachannel = ev => {
                this.initDataChannel(streamId, ev.channel);
              };
            } else {
              Logger$2.warn("CreateDataChannel is not supported");
            }
          }
        }
        this.remotePeerConnection[streamId].oniceconnectionstatechange = event => {
          var obj = {
            state: this.remotePeerConnection[streamId].iceConnectionState,
            streamId: streamId
          };
          if (obj.state == "failed" || obj.state == "disconnected" || obj.state == "closed") {
            this.reconnectIfRequired(3000);
          }
          this.notifyEventListeners("ice_connection_state_changed", obj);

          //
          if (!this.isPlayMode && !this.playStreamId.includes(streamId)) {
            if (this.remotePeerConnection[streamId].iceConnectionState == "connected") {
              this.mediaManager.changeBandwidth(this.mediaManager.bandwidth, streamId).then(() => {
                Logger$2.debug("Bandwidth is changed to " + this.mediaManager.bandwidth);
              }).catch(e => Logger$2.warn(e));
            }
          }
        };
      }
    }

    /**
     * Called internally to close PeerConnection.
     * @param {string} streamId : unique id for the stream
     */
    closePeerConnection(streamId) {
      var peerConnection = this.remotePeerConnection[streamId];
      if (peerConnection != null) {
        this.remotePeerConnection[streamId] = null;
        delete this.remotePeerConnection[streamId];
        if (peerConnection.dataChannel != null) {
          peerConnection.dataChannel.close();
        }
        if (peerConnection.signalingState != "closed") {
          peerConnection.close();
        }
        var playStreamIndex = this.playStreamId.indexOf(streamId);
        if (playStreamIndex != -1) {
          this.playStreamId.splice(playStreamIndex, 1);
        }
      }
      //this is for the stats
      if (this.remotePeerConnectionStats[streamId] != null) {
        clearInterval(this.remotePeerConnectionStats[streamId].timerId);
        delete this.remotePeerConnectionStats[streamId];
      }
      if (this.soundMeters[streamId] != null) {
        delete this.soundMeters[streamId];
      }
    }

    /**
     * Called to get the signalling state for a stream.
     * This information can be used for error handling.
     * Check: https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection/connectionState
     * @param {string} streamId : unique id for the stream
     */
    signallingState(streamId) {
      if (this.remotePeerConnection[streamId] != null) {
        return this.remotePeerConnection[streamId].signalingState;
      }
      return null;
    }

    /**
     * Called to get the ice connection state for a stream.
     * This information can be used for error handling.
     * Check: https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection/iceConnectionState
     * @param {string} streamId : unique id for the stream
     */
    iceConnectionState(streamId) {
      if (this.remotePeerConnection[streamId] != null) {
        return this.remotePeerConnection[streamId].iceConnectionState;
      }
      return null;
    }

    /**
     * Called by browser when Local Configuration (SDP) is created successfully.
     * It is set as LocalDescription first then sent to AMS.
     * @param {object} configuration : created Local Configuration (SDP)
     * @param {string} streamId : unique id for the stream
     */
    gotDescription(configuration, streamId) {
      this.remotePeerConnection[streamId].setLocalDescription(configuration).then(responose => {
        Logger$2.debug("Set local description successfully for stream Id " + streamId);
        var jsCmd = {
          command: "takeConfiguration",
          streamId: streamId,
          type: configuration.type,
          sdp: configuration.sdp
        };
        Logger$2.debug("setLocalDescription:" + configuration.sdp);
        this.webSocketAdaptor.send(JSON.stringify(jsCmd));
      }).catch(error => {
        Logger$2.error("Cannot set local description. Error is: " + error);
      });
    }

    /**
     * Called by WebSocketAdaptor when Remote Configuration (SDP) is received from AMS.
     * It is set as RemoteDescription first then if @iceCandidateList has candidate that
     * is received bfore this message, it is added as ice candidate.
     * @param {object} configuration : received Remote Configuration (SDP)
     * @param {string} idOfStream : unique id for the stream
     * @param {string} typeOfConfiguration
     * @param {string} idMapping : stream id and track id (which is provided in SDP) mapping in MultiTrack Playback and conference.
     *                It is recorded to match stream id as new tracks are added with @onTrack
     */
    takeConfiguration(idOfStream, configuration, typeOfConfiguration, idMapping) {
      var streamId = idOfStream;
      var type = typeOfConfiguration;
      var conf = configuration;
      var isTypeOffer = type == "offer";
      var dataChannelMode = "publish";
      if (isTypeOffer) {
        dataChannelMode = "play";
      }
      this.idMapping[streamId] = idMapping;
      this.initPeerConnection(streamId, dataChannelMode);
      Logger$2.debug("setRemoteDescription:" + conf);
      this.remotePeerConnection[streamId].setRemoteDescription(new RTCSessionDescription({
        sdp: conf,
        type: type
      })).then(response => {
        if (this.debug) {
          Logger$2.debug("set remote description is succesfull with response: " + response + " for stream : " + streamId + " and type: " + type);
          Logger$2.debug(conf);
        }
        this.remoteDescriptionSet[streamId] = true;
        var length = this.iceCandidateList[streamId].length;
        Logger$2.debug("Ice candidate list size to be added: " + length);
        for (var i = 0; i < length; i++) {
          this.addIceCandidate(streamId, this.iceCandidateList[streamId][i]);
        }
        this.iceCandidateList[streamId] = [];
        if (isTypeOffer) {
          //SDP constraints may be different in play mode
          Logger$2.debug("try to create answer for stream id: " + streamId);
          this.remotePeerConnection[streamId].createAnswer(this.sdp_constraints).then(configuration => {
            Logger$2.debug("created answer for stream id: " + streamId);
            //support for stereo
            configuration.sdp = configuration.sdp.replace("useinbandfec=1", "useinbandfec=1; stereo=1");
            this.gotDescription(configuration, streamId);
          }).catch(error => {
            Logger$2.error("create answer error :" + error);
          });
        }
      }).catch(error => {
        if (this.debug) {
          Logger$2.error("set remote description is failed with error: " + error);
        }
        if (error.toString().indexOf("InvalidAccessError") > -1 || error.toString().indexOf("setRemoteDescription") > -1) {
          /**
           * This error generally occurs in codec incompatibility.
           * AMS for a now supports H.264 codec. This error happens when some browsers try to open it from VP8.
           */
          this.notifyErrorEventListeners("notSetRemoteDescription");
        }
      });
    }

    /**
     * Called by WebSocketAdaptor when new ice candidate is received from AMS.
     * If Remote Description (SDP) is already set, the candidate is added immediately,
     * otherwise stored in @iceCandidateList to add after Remote Description (SDP) set.
     * @param {string} idOfTheStream : unique id for the stream
     * @param {number|null} tmpLabel : sdpMLineIndex
     * @param {string} tmpCandidate : ice candidate
     */
    takeCandidate(idOfTheStream, tmpLabel, tmpCandidate) {
      var streamId = idOfTheStream;
      var label = tmpLabel;
      var candidateSdp = tmpCandidate;
      var candidate = new RTCIceCandidate({
        sdpMLineIndex: label,
        candidate: candidateSdp
      });
      var dataChannelMode = "peer";
      this.initPeerConnection(streamId, dataChannelMode);
      Logger$2.debug("takeCandidate:" + candidateSdp);
      if (this.remoteDescriptionSet[streamId] == true) {
        this.addIceCandidate(streamId, candidate);
      } else {
        Logger$2.debug("Ice candidate is added to list because remote description is not set yet");
        this.iceCandidateList[streamId].push(candidate);
      }
    }
    /**
     * Called internally to add the Ice Candidate to PeerConnection
     *  @param {string} streamId : unique id for the stream
     *  @param {object} candidate : ice candidate
     */
    addIceCandidate(streamId, candidate) {
      var protocolSupported = false;
      if (candidate.candidate == "") {
        //candidate can be received and its value can be "".
        //don't compare the protocols
        protocolSupported = true;
      } else if (typeof candidate.protocol == "undefined") {
        this.candidateTypes.forEach(element => {
          if (candidate.candidate.toLowerCase().includes(element)) {
            protocolSupported = true;
          }
        });
      } else {
        protocolSupported = this.candidateTypes.includes(candidate.protocol.toLowerCase());
      }
      if (protocolSupported) {
        this.remotePeerConnection[streamId].addIceCandidate(candidate).then(response => {
          if (this.debug) {
            Logger$2.debug("Candidate is added for stream " + streamId);
          }
        }).catch(error => {
          Logger$2.error("ice candiate cannot be added for stream id: " + streamId + " error is: " + error);
          Logger$2.error(candidate);
        });
      } else {
        if (this.debug) {
          Logger$2.debug("Candidate's protocol(" + candidate.protocol + ") is not supported." + "Candidate: " + candidate.candidate + " Supported protocols:" + this.candidateTypes);
        }
      }
    }
    /**
     * Called by WebSocketAdaptor when start message is received //TODO: may be changed. this logic shouldn't be in WebSocketAdaptor
     * @param {string} idOfStream : unique id for the stream
     */
    startPublishing(idOfStream) {
      var streamId = idOfStream;
      this.initPeerConnection(streamId, "publish");
      this.remotePeerConnection[streamId].createOffer(this.sdp_constraints).then(configuration => {
        this.gotDescription(configuration, streamId);
      }).catch(error => {
        Logger$2.error("create offer error for stream id: " + streamId + " error: " + error);
      });
    }
    /**
     * Toggle video track on the server side.
     *
     * @param {string}  streamId : is the id of the stream
     * @param {string}  trackId : is the id of the track. streamId is also one of the trackId of the stream. If you are having just a single track on your
     *         stream, you need to give streamId as trackId parameter as well.
     * @param {boolean}  enabled : is the enable/disable video track. If it's true, server sends video track. If it's false, server does not send video
     */
    toggleVideo(streamId, trackId, enabled) {
      var jsCmd = {
        command: "toggleVideo",
        streamId: streamId,
        trackId: trackId,
        enabled: enabled
      };
      this.webSocketAdaptor.send(JSON.stringify(jsCmd));
    }

    /**
     * Toggle audio track on the server side.
     *
     * @param {string} streamId : is the id of the stream
     * @param {string}  trackId : is the id of the track. streamId is also one of the trackId of the stream. If you are having just a single track on your
     *            stream, you need to give streamId as trackId parameter as well.
     * @param {boolean}  enabled : is the enable/disable video track. If it's true, server sends audio track. If it's false, server does not send audio
     *
     */
    toggleAudio(streamId, trackId, enabled) {
      var jsCmd = {
        command: "toggleAudio",
        streamId: streamId,
        trackId: trackId,
        enabled: enabled
      };
      this.webSocketAdaptor.send(JSON.stringify(jsCmd));
    }

    /**
     * Called to get statistics for a PeerConnection. It can be publisher or player.
     *
     * @param {string} streamId : unique id for the stream
     */
    getStats(streamId) {
      Logger$2.debug("peerstatsgetstats = " + this.remotePeerConnectionStats[streamId]);
      return new Promise((resolve, reject) => {
        this.remotePeerConnection[streamId].getStats(null).then(stats => {
          var bytesReceived = -1;
          var videoPacketsLost = -1;
          var audioPacketsLost = -1;
          var fractionLost = -1;
          var currentTime = -1;
          var bytesSent = -1;
          var videoPacketsSent = -1;
          var audioPacketsSent = -1;
          var audioLevel = -1;
          var qlr = "";
          var framesEncoded = -1;
          var width = -1;
          var height = -1;
          var fps = -1;
          var frameWidth = -1;
          var frameHeight = -1;
          var videoRoundTripTime = -1;
          var videoJitter = -1;
          var audioRoundTripTime = -1;
          var audioJitter = -1;
          var framesDecoded = -1;
          var framesDropped = -1;
          var framesReceived = -1;
          var audioJitterAverageDelay = -1;
          var videoJitterAverageDelay = -1;
          var availableOutgoingBitrate = Infinity;
          stats.forEach(value => {
            //Logger.debug(value);
            if (value.type == "inbound-rtp" && typeof value.kind != "undefined") {
              bytesReceived += value.bytesReceived;
              if (value.kind == "audio") {
                audioPacketsLost = value.packetsLost;
              } else if (value.kind == "video") {
                videoPacketsLost = value.packetsLost;
              }
              fractionLost += value.fractionLost;
              currentTime = value.timestamp;
            } else if (value.type == "outbound-rtp") {
              //TODO: SPLIT AUDIO AND VIDEO BITRATES
              if (value.kind == "audio") {
                audioPacketsSent = value.packetsSent;
              } else if (value.kind == "video") {
                videoPacketsSent = value.packetsSent;
              }
              bytesSent += value.bytesSent;
              currentTime = value.timestamp;
              qlr = value.qualityLimitationReason;
              if (value.framesEncoded != null) {
                //audio tracks are undefined here
                framesEncoded += value.framesEncoded;
              }
            } else if (value.type == "track" && typeof value.kind != "undefined" && value.kind == "audio") {
              if (typeof value.audioLevel != "undefined") {
                audioLevel = value.audioLevel;
              }
              if (typeof value.jitterBufferDelay != "undefined" && typeof value.jitterBufferEmittedCount != "undefined") {
                audioJitterAverageDelay = value.jitterBufferDelay / value.jitterBufferEmittedCount;
              }
            } else if (value.type == "track" && typeof value.kind != "undefined" && value.kind == "video") {
              if (typeof value.frameWidth != "undefined") {
                frameWidth = value.frameWidth;
              }
              if (typeof value.frameHeight != "undefined") {
                frameHeight = value.frameHeight;
              }
              if (typeof value.framesDecoded != "undefined") {
                framesDecoded = value.framesDecoded;
              }
              if (typeof value.framesDropped != "undefined") {
                framesDropped = value.framesDropped;
              }
              if (typeof value.framesReceived != "undefined") {
                framesReceived = value.framesReceived;
              }
              if (typeof value.jitterBufferDelay != "undefined" && typeof value.jitterBufferEmittedCount != "undefined") {
                videoJitterAverageDelay = value.jitterBufferDelay / value.jitterBufferEmittedCount;
              }
            } else if (value.type == "remote-inbound-rtp" && typeof value.kind != "undefined") {
              if (typeof value.packetsLost != "undefined") {
                if (value.kind == "video") {
                  //this is the packetsLost for publishing
                  videoPacketsLost = value.packetsLost;
                } else if (value.kind == "audio") {
                  //this is the packetsLost for publishing
                  audioPacketsLost = value.packetsLost;
                }
              }
              if (typeof value.roundTripTime != "undefined") {
                if (value.kind == "video") {
                  videoRoundTripTime = value.roundTripTime;
                } else if (value.kind == "audio") {
                  audioRoundTripTime = value.roundTripTime;
                }
              }
              if (typeof value.jitter != "undefined") {
                if (value.kind == "video") {
                  videoJitter = value.jitter;
                } else if (value.kind == "audio") {
                  audioJitter = value.jitter;
                }
              }
            } else if (value.type == "media-source") {
              if (value.kind == "video") {
                //returns video source dimensions, not necessarily dimensions being encoded by browser
                width = value.width;
                height = value.height;
                fps = value.framesPerSecond;
              }
            } else if (value.type == "candidate-pair" && value.state == "succeeded" && value.availableOutgoingBitrate != undefined) {
              availableOutgoingBitrate = value.availableOutgoingBitrate / 1000;
            }
          });
          this.remotePeerConnectionStats[streamId].totalBytesReceived = bytesReceived;
          this.remotePeerConnectionStats[streamId].videoPacketsLost = videoPacketsLost;
          this.remotePeerConnectionStats[streamId].audioPacketsLost = audioPacketsLost;
          this.remotePeerConnectionStats[streamId].fractionLost = fractionLost;
          this.remotePeerConnectionStats[streamId].currentTime = currentTime;
          this.remotePeerConnectionStats[streamId].totalBytesSent = bytesSent;
          this.remotePeerConnectionStats[streamId].totalVideoPacketsSent = videoPacketsSent;
          this.remotePeerConnectionStats[streamId].totalAudioPacketsSent = audioPacketsSent;
          this.remotePeerConnectionStats[streamId].audioLevel = audioLevel;
          this.remotePeerConnectionStats[streamId].qualityLimitationReason = qlr;
          this.remotePeerConnectionStats[streamId].totalFramesEncoded = framesEncoded;
          this.remotePeerConnectionStats[streamId].resWidth = width;
          this.remotePeerConnectionStats[streamId].resHeight = height;
          this.remotePeerConnectionStats[streamId].srcFps = fps;
          this.remotePeerConnectionStats[streamId].frameWidth = frameWidth;
          this.remotePeerConnectionStats[streamId].frameHeight = frameHeight;
          this.remotePeerConnectionStats[streamId].videoRoundTripTime = videoRoundTripTime;
          this.remotePeerConnectionStats[streamId].videoJitter = videoJitter;
          this.remotePeerConnectionStats[streamId].audioRoundTripTime = audioRoundTripTime;
          this.remotePeerConnectionStats[streamId].audioJitter = audioJitter;
          this.remotePeerConnectionStats[streamId].framesDecoded = framesDecoded;
          this.remotePeerConnectionStats[streamId].framesDropped = framesDropped;
          this.remotePeerConnectionStats[streamId].framesReceived = framesReceived;
          this.remotePeerConnectionStats[streamId].videoJitterAverageDelay = videoJitterAverageDelay;
          this.remotePeerConnectionStats[streamId].audioJitterAverageDelay = audioJitterAverageDelay;
          this.remotePeerConnectionStats[streamId].availableOutgoingBitrate = availableOutgoingBitrate;
          this.notifyEventListeners("updated_stats", this.remotePeerConnectionStats[streamId]);
          resolve(true);
        }).catch(err => {
          resolve(false);
        });
      });
    }

    /**
     * Called to start a periodic timer to get statistics periodically (5 seconds) for a specific stream.
     *
     * @param {string} streamId : unique id for the stream
     */
    enableStats(streamId) {
      if (this.remotePeerConnectionStats[streamId] == null) {
        this.remotePeerConnectionStats[streamId] = new PeerStats(streamId);
        this.remotePeerConnectionStats[streamId].timerId = setInterval(() => {
          this.getStats(streamId);
        }, 5000);
      }
    }

    /**
     * Called to stop the periodic timer which is set by @enableStats
     *
     * @param {string} streamId : unique id for the stream
     */
    disableStats(streamId) {
      if (this.remotePeerConnectionStats[streamId] != null || typeof this.remotePeerConnectionStats[streamId] != 'undefined') {
        clearInterval(this.remotePeerConnectionStats[streamId].timerId);
      }
    }

    /**
     * Called to check and start Web Socket connection if it is not started
     */
    checkWebSocketConnection() {
      if (this.webSocketAdaptor == null || this.webSocketAdaptor.isConnected() == false && this.webSocketAdaptor.isConnecting() == false) {
        Logger$2.debug("websocket url : " + this.websocketURL);
        this.webSocketAdaptor = new WebSocketAdaptor({
          websocket_url: this.websocketURL,
          webrtcadaptor: this,
          callback: (info, obj) => {
            if (info == "closed") {
              this.reconnectIfRequired();
            }
            this.notifyEventListeners(info, obj);
          },
          callbackError: (error, message) => {
            this.notifyErrorEventListeners(error, message);
          },
          debug: this.debug
        });
      }
    }

    /**
     * Called to stop Web Socket connection
     * After calling this function, create new WebRTCAdaptor instance, don't use the the same object
     * Because all streams are closed on server side as well when websocket connection is closed.
     */
    closeWebSocket() {
      for (var key in this.remotePeerConnection) {
        this.closePeerConnection(key);
      }
      //free the remote peer connection by initializing again
      this.remotePeerConnection = new Array();
      this.webSocketAdaptor.close();
    }

    /**
     * @param {string} streamId Called to send a text message to other peer in the peer-to-peer sessionnnection is closed.
     * @param {*} definition
     * @param {*} data
     */
    peerMessage(streamId, definition, data) {
      var jsCmd = {
        command: "peerMessageCommand",
        streamId: streamId,
        definition: definition,
        data: data
      };
      this.webSocketAdaptor.send(JSON.stringify(jsCmd));
    }

    /**
     * Called to force AMS to send the video with the specified resolution in case of Adaptive Streaming (ABR) enabled.
     * Normally the resolution is automatically determined by AMS according to the network condition.
     * @param {string}  streamId : unique id for the stream
     * @param {*}  resolution : default is auto. You can specify any height value from the ABR list.
     */
    forceStreamQuality(streamId, resolution) {
      var jsCmd = {
        command: "forceStreamQuality",
        streamId: streamId,
        streamHeight: resolution
      };
      this.webSocketAdaptor.send(JSON.stringify(jsCmd));
    }

    /**
     * Called to send data via DataChannel. DataChannel should be enabled on AMS settings.
     * @param {string} streamId : unique id for the stream
     * @param {*}  data : data that you want to send. It may be a text (may in Json format or not) or binary
     */
    sendData(streamId, data) {
      var CHUNK_SIZE = 16000;
      if (this.remotePeerConnection[streamId] !== undefined) {
        var dataChannel = this.remotePeerConnection[streamId].dataChannel;
        if (dataChannel === undefined || dataChannel === null || typeof dataChannel === 'undefined') {
          Logger$2.warn('dataChannel is null or undefined');
          return;
        } else if (dataChannel.readyState !== 'open') {
          Logger$2.warn('dataChannel.readyState is not open: ' + dataChannel.readyState);
          return;
        }
        var length = data.length || data.size || data.byteLength;
        var sent = 0;
        if (typeof data === 'string' || data instanceof String) {
          dataChannel.send(data);
        } else {
          var token = Math.floor(Math.random() * 999999);
          var header = new Int32Array(2);
          header[0] = token;
          header[1] = length;
          dataChannel.send(header);
          var sent = 0;
          while (sent < length) {
            var size = Math.min(length - sent, CHUNK_SIZE);
            var buffer = new Uint8Array(size + 4);
            var tokenArray = new Int32Array(1);
            tokenArray[0] = token;
            buffer.set(new Uint8Array(tokenArray.buffer, 0, 4), 0);
            var chunk = data.slice(sent, sent + size);
            buffer.set(new Uint8Array(chunk), 4);
            sent += size;
            dataChannel.send(buffer);
          }
        }
      } else {
        Logger$2.warn("Send data is called for undefined peer connection with stream id: " + streamId);
      }
    }

    /**
     * Called by user
     * to add SoundMeter to a stream (remote stream)
     * to measure audio level. This sound Meters are added to a map with the key of StreamId.
     * When user called @getSoundLevelList, the instant levels are provided.
     *
     * This list can be used to add a sign to talking participant
     * in conference room. And also to determine the dominant audio to focus that player.
     * @param {MediaStream} stream
     * @param {string} streamId
     */
    enableAudioLevel(stream, streamId) {
      var soundMeter = new SoundMeter(this.mediaManager.audioContext);

      // Put variables in global scope to make them available to the
      // browser console.
      // this function fetches getSoundLevelList and this list get instant levels from soundmeter directly
      // so we don't need to fill inside of levelCallback here, just pass an empty function
      soundMeter.connectToSource(stream, () => {}, function (e) {
        if (e) {
          alert(e);
          return;
        }
        Logger$2.debug("Added sound meter for stream: " + streamId + " = " + soundMeter.instant.toFixed(2));
      });
      this.soundMeters[streamId] = soundMeter;
    }

    /**
     * Called by the user
     * to get the audio levels for the streams for the provided StreamIds
     *
     * @param {*} streamsList
     */
    getSoundLevelList(streamsList) {
      for (var i = 0; i < streamsList.length; i++) {
        this.soundLevelList[streamsList[i]] = this.soundMeters[streamsList[i]].instant.toFixed(2);
      }
      this.notifyEventListeners("gotSoundList", this.soundLevelList);
    }

    /**
     * Called media manaher to get video/audio sender for the local peer connection
     *
     * @param {string} streamId :
     * @param {string} type : "video" or "audio"
     * @returns
     */
    getSender(streamId, type) {
      var sender = null;
      if (this.remotePeerConnection[streamId] != null) {
        sender = this.remotePeerConnection[streamId].getSenders().find(function (s) {
          return s.track.kind == type;
        });
      }
      return sender;
    }

    /**
     * Called by user
     *
     * @param {string} videoTrackId : track id associated with pinned video
     * @param {string} streamId : streamId of the pinned video
     * @param {boolean} enabled : true | false
     * @returns
     */
    assignVideoTrack(videoTrackId, streamId, enabled) {
      var jsCmd = {
        command: "assignVideoTrackCommand",
        streamId: streamId,
        videoTrackId: videoTrackId,
        enabled: enabled
      };
      this.webSocketAdaptor.send(JSON.stringify(jsCmd));
    }

    /**
     * Called by user
     * video tracks may be less than the participants count
     * so these parameters are used for assigning video tracks to participants.
     * This message is used to make pagination in conference.
     * @param {string} streamId
     * @param {number} offset : start index for participant list to play
     * @param {number} size : number of the participants to play
     * @returns
     */
    updateVideoTrackAssignments(streamId, offset, size) {
      var jsCmd = {
        streamId: streamId,
        command: "updateVideoTrackAssignmentsCommand",
        offset: offset,
        size: size
      };
      this.webSocketAdaptor.send(JSON.stringify(jsCmd));
    }

    /**
     * Called by user
     * This message is used to set max video track count in a conference.
     * @param {string} streamId
     * @param {number} maxTrackCount : maximum video track count
     * @returns
     */
    setMaxVideoTrackCount(streamId, maxTrackCount) {
      var jsCmd = {
        streamId: streamId,
        command: "setMaxVideoTrackCountCommand",
        maxTrackCount: maxTrackCount
      };
      this.webSocketAdaptor.send(JSON.stringify(jsCmd));
    }

    /**
     * Called by user
     * This message is used to send audio level in a conference.
     *
    * IMPORTANT: AMS v2.7+ can get the audio level from the RTP header and sends audio level to the viewers the same way here.
     *  Just one difference, AMS sends the audio level in the range of 0 and 127. 0 is max, 127 is ms
      *  It means that likely you don't need to send UPDATE_AUDIO_LEVEL anymore
     *
     * @param {string} streamId
     * @param {*} value : audio level
     * @returns
     */
    updateAudioLevel(streamId, value) {
      var jsCmd = {
        streamId: streamId,
        eventType: "UPDATE_AUDIO_LEVEL",
        audioLevel: value
      };
      this.sendData(streamId, JSON.stringify(jsCmd));
    }

    /**
     * Called by user
     * This message is used to get debug data from server for debugging purposes in conference.
     * @param {string} streamId
     * @returns
     */
    getDebugInfo(streamId) {
      var jsCmd = {
        streamId: streamId,
        command: "getDebugInfo"
      };
      this.webSocketAdaptor.send(JSON.stringify(jsCmd));
    }

    /**
     * The following messages are forwarded to MediaManager. They are also kept here because of backward compatibility.
     * You can find the details about them in media_manager.js
     * @param {string} streamId
     */
    turnOffLocalCamera(streamId) {
      this.mediaManager.turnOffLocalCamera(streamId);
    }
    /**
     *
     * @param {string} streamId
     * @returns
     */
    turnOnLocalCamera(streamId) {
      return this.mediaManager.turnOnLocalCamera(streamId);
    }
    muteLocalMic() {
      this.mediaManager.muteLocalMic();
    }
    unmuteLocalMic() {
      this.mediaManager.unmuteLocalMic();
    }
    /**
     *
     * @param {string} streamId
     * @returns
     */
    switchDesktopCapture(streamId) {
      return this.mediaManager.switchDesktopCapture(streamId);
    }

    /**
     * Switch to Video camera capture again. Updates the video track on the fly as well.
     * @param {string} streamId
     * @param {string} deviceId
     * @returns {Promise}
     */
    switchVideoCameraCapture(streamId, deviceId) {
      return this.mediaManager.switchVideoCameraCapture(streamId, deviceId);
    }

    /**
     * Update video track of the stream. Updates the video track on the fly as well.
     * @param {string} stream
     * @param {string} streamId
     * @param {function} onEndedCallback
     * @param {boolean} stopDesktop
     * @returns {Promise}
     */
    updateVideoTrack(stream, streamId, onEndedCallback, stopDesktop) {
      return this.mediaManager.updateVideoTrack(stream, streamId, onEndedCallback, stopDesktop);
    }

    /**
     * Update audio track of the stream. Updates the audio track on the fly as well. It just replaces the audio track with the first one in the stream
     * @param {*} stream
     * @param {*} streamId
     * @param {*} onEndedCallback
     * @returns
     */
    updateAudioTrack(stream, streamId, onEndedCallback) {
      return this.mediaManager.updateAudioTrack(stream, streamId, onEndedCallback);
    }

    /**
     * Called by User
     * to switch between front and back camera on mobile devices
     *
     * @param {string} streamId Id of the stream to be changed.
     * @param {string} facingMode it can be ""user" or "environment"
     *
     * This method is used to switch front and back camera.
     */
    switchVideoCameraFacingMode(streamId, facingMode) {
      return this.mediaManager.switchVideoCameraFacingMode(streamId, facingMode);
    }
    /**
     *
     * @param {string} streamId
     * @returns
     */
    switchDesktopCaptureWithCamera(streamId) {
      return this.mediaManager.switchDesktopCaptureWithCamera(streamId);
    }
    /**
     *
     * @param {string} streamId
     * @param {string} deviceId
     * @returns
     */
    switchAudioInputSource(streamId, deviceId) {
      return this.mediaManager.switchAudioInputSource(streamId, deviceId);
    }
    /**
     *
     * @param {number} volumeLevel
     */
    setVolumeLevel(volumeLevel) {
      this.mediaManager.setVolumeLevel(volumeLevel);
    }
    /**
     *
     * Using sound meter in order to get audio level may cause audio distortion in Windows browsers
     * @param {Function} levelCallback
     * @param {number} period
     * @returns
     */
    enableAudioLevelForLocalStream(levelCallback, period) {
      return this.mediaManager.enableAudioLevelForLocalStream(levelCallback);
    }
    disableAudioLevelForLocalStream() {
      this.mediaManager.disableAudioLevelForLocalStream();
    }
    /**
     *
     * @param {object} constraints
     * @returns
     */
    applyConstraints(constraints) {
      return this.mediaManager.applyConstraints(constraints);
    }
    /**
     *
     * @param {number} bandwidth
     * @param {string} streamId
     */
    changeBandwidth(bandwidth, streamId) {
      this.mediaManager.changeBandwidth(bandwidth, streamId);
    }
    enableAudioLevelWhenMuted() {
      this.mediaManager.enableAudioLevelWhenMuted();
    }
    disableAudioLevelWhenMuted() {
      this.mediaManager.disableAudioLevelWhenMuted();
    }
    /**
     *
     * @param {string} streamId
     * @returns
     */
    getVideoSender(streamId) {
      return this.mediaManager.getVideoSender(streamId);
    }
    /**
     *
     * @param {object} mediaConstraints : media constraints to be used for opening the stream
     * @param {string} streamId : id of the stream to replace tracks with
     * @returns
     */
    openStream(mediaConstraints, streamId) {
      return this.mediaManager.openStream(mediaConstraints, streamId);
    }
    closeStream() {
      return this.mediaManager.closeStream();
    }
  }

  /* The Information Callbacks Called by This Class */
  //TODO:

  /* The Error Callbacks Called by This Class */
  //TODO:
  /**
   * @type {Array<Function>}
   */
  _defineProperty(WebRTCAdaptor, "pluginInitMethods", new Array());

  //ask if adaptive m3u8 file

  if (!String.prototype.endsWith) {
    String.prototype.endsWith = function (searchString, position) {
      var subjectString = this.toString();
      if (typeof position !== 'number' || !isFinite(position) || Math.floor(position) !== position || position > subjectString.length) {
        position = subjectString.length;
      }
      position -= searchString.length;
      var lastIndex = subjectString.lastIndexOf(searchString, position);
      return lastIndex !== -1 && lastIndex === position;
    };
  }
  /**
   * 
   * @param {string} sParam 
   * @param {string=} search
   * @returns 
   */
  function getUrlParameter(sParam, search) {
    if (typeof search === undefined || search == null) {
      search = window.location.search;
    }
    var sPageURL = decodeURIComponent(search.substring(1)),
      sURLVariables = sPageURL.split('&'),
      sParameterName,
      i;
    for (i = 0; i < sURLVariables.length; i++) {
      sParameterName = sURLVariables[i].split('=');
      if (sParameterName[0] === sParam) {
        return sParameterName[1] === undefined ? true : sParameterName[1];
      }
    }
  }

  (function () {

    function t(t) {
      var e = 0;
      return function () {
        return e < t.length ? {
          done: !1,
          value: t[e++]
        } : {
          done: !0
        };
      };
    }
    var e,
      n = "function" == typeof Object.defineProperties ? Object.defineProperty : function (t, e, n) {
        return t == Array.prototype || t == Object.prototype || (t[e] = n.value), t;
      },
      r = function t(e) {
        e = ["object" == typeof globalThis && globalThis, e, "object" == typeof window && window, "object" == typeof self && self, "object" == typeof global && global];
        for (var n = 0; n < e.length; ++n) {
          var r = e[n];
          if (r && r.Math == Math) return r;
        }
        throw Error("Cannot find global object");
      }(this);
    function i(t, e) {
      if (e) a: {
        var i = r;
        t = t.split(".");
        for (var o = 0; o < t.length - 1; o++) {
          var a = t[o];
          if (!(a in i)) break a;
          i = i[a];
        }
        (e = e(o = i[t = t[t.length - 1]])) != o && null != e && n(i, t, {
          configurable: !0,
          writable: !0,
          value: e
        });
      }
    }
    function o(t) {
      return (t = {
        next: t
      })[Symbol.iterator] = function () {
        return this;
      }, t;
    }
    function a(e) {
      var n = "undefined" != typeof Symbol && Symbol.iterator && e[Symbol.iterator];
      return n ? n.call(e) : {
        next: t(e)
      };
    }
    function u(t) {
      if (!(t instanceof Array)) {
        t = a(t);
        for (var e, n = []; !(e = t.next()).done;) n.push(e.value);
        t = n;
      }
      return t;
    }
    i("Symbol", function (t) {
      function e(t, e) {
        this.g = t, n(this, "description", {
          configurable: !0,
          writable: !0,
          value: e
        });
      }
      if (t) return t;
      e.prototype.toString = function () {
        return this.g;
      };
      var r = "jscomp_symbol_" + (1e9 * Math.random() >>> 0) + "_",
        i = 0;
      return function t(n) {
        if (this instanceof t) throw TypeError("Symbol is not a constructor");
        return new e(r + (n || "") + "_" + i++, n);
      };
    }), i("Symbol.iterator", function (e) {
      if (e) return e;
      e = Symbol("Symbol.iterator");
      for (var i = "Array Int8Array Uint8Array Uint8ClampedArray Int16Array Uint16Array Int32Array Uint32Array Float32Array Float64Array".split(" "), a = 0; a < i.length; a++) {
        var u = r[i[a]];
        "function" == typeof u && "function" != typeof u.prototype[e] && n(u.prototype, e, {
          configurable: !0,
          writable: !0,
          value: function () {
            return o(t(this));
          }
        });
      }
      return e;
    });
    var s,
      f,
      c = "function" == typeof Object.create ? Object.create : function (t) {
        function e() {}
        return e.prototype = t, new e();
      };
    if ("function" == typeof Object.setPrototypeOf) f = Object.setPrototypeOf;else {
      a: {
        var l = {};
        try {
          l.__proto__ = {
            a: !0
          }, R = l.a;
          break a;
        } catch (h) {}
        R = !1;
      }
      f = R ? function (t, e) {
        if (t.__proto__ = e, t.__proto__ !== e) throw TypeError(t + " is not extensible");
        return t;
      } : null;
    }
    var g = f;
    function $(t, e) {
      if (t.prototype = c(e.prototype), t.prototype.constructor = t, g) g(t, e);else for (var n in e) if ("prototype" != n) {
        if (Object.defineProperties) {
          var r = Object.getOwnPropertyDescriptor(e, n);
          r && Object.defineProperty(t, n, r);
        } else t[n] = e[n];
      }
      t.ea = e.prototype;
    }
    function p() {
      this.l = !1, this.i = null, this.h = void 0, this.g = 1, this.s = this.m = 0, this.j = null;
    }
    function v(t) {
      if (t.l) throw TypeError("Generator is already running");
      t.l = !0;
    }
    function d(t, e) {
      t.j = {
        U: e,
        V: !0
      }, t.g = t.m || t.s;
    }
    function _(t, e, n) {
      return t.g = n, {
        value: e
      };
    }
    function y(t) {
      this.g = new p(), this.h = t;
    }
    function m(t, e, n, r) {
      try {
        var i = e.call(t.g.i, n);
        if (!(i instanceof Object)) throw TypeError("Iterator result " + i + " is not an object");
        if (!i.done) return t.g.l = !1, i;
        var o = i.value;
      } catch (a) {
        return t.g.i = null, d(t.g, a), b(t);
      }
      return t.g.i = null, r.call(t.g, o), b(t);
    }
    function b(t) {
      for (; t.g.g;) try {
        var e = t.h(t.g);
        if (e) return t.g.l = !1, {
          value: e.value,
          done: !1
        };
      } catch (n) {
        t.g.h = void 0, d(t.g, n);
      }
      if (t.g.l = !1, t.g.j) {
        if (e = t.g.j, t.g.j = null, e.V) throw e.U;
        return {
          value: e.return,
          done: !0
        };
      }
      return {
        value: void 0,
        done: !0
      };
    }
    function w(t) {
      this.next = function (e) {
        return v(t.g), t.g.i ? e = m(t, t.g.i.next, e, t.g.o) : (t.g.o(e), e = b(t)), e;
      }, this.throw = function (e) {
        return v(t.g), t.g.i ? e = m(t, t.g.i.throw, e, t.g.o) : (d(t.g, e), e = b(t)), e;
      }, this.return = function (e) {
        var n, r, i;
        return n = t, r = e, v(n.g), (i = n.g.i) ? m(n, "return" in i ? i.return : function (t) {
          return {
            value: t,
            done: !0
          };
        }, r, n.g.return) : (n.g.return(r), b(n));
      }, this[Symbol.iterator] = function () {
        return this;
      };
    }
    function x(t, e) {
      return e = new w(new y(e)), g && t.prototype && g(e, t.prototype), e;
    }
    p.prototype.o = function (t) {
      this.h = t;
    }, p.prototype.return = function (t) {
      this.j = {
        return: t
      }, this.g = this.s;
    };
    var A = "function" == typeof Object.assign ? Object.assign : function (t, e) {
      for (var n = 1; n < arguments.length; n++) {
        var r = arguments[n];
        if (r) for (var i in r) Object.prototype.hasOwnProperty.call(r, i) && (t[i] = r[i]);
      }
      return t;
    };
    i("Object.assign", function (t) {
      return t || A;
    }), i("Promise", function (t) {
      function e(t) {
        this.h = 0, this.i = void 0, this.g = [], this.o = !1;
        var e = this.j();
        try {
          t(e.resolve, e.reject);
        } catch (n) {
          e.reject(n);
        }
      }
      function n() {
        this.g = null;
      }
      function i(t) {
        return t instanceof e ? t : new e(function (e) {
          e(t);
        });
      }
      if (t) return t;
      n.prototype.h = function (t) {
        if (null == this.g) {
          this.g = [];
          var e = this;
          this.i(function () {
            e.l();
          });
        }
        this.g.push(t);
      };
      var o = r.setTimeout;
      n.prototype.i = function (t) {
        o(t, 0);
      }, n.prototype.l = function () {
        for (; this.g && this.g.length;) {
          var t = this.g;
          this.g = [];
          for (var e = 0; e < t.length; ++e) {
            var n = t[e];
            t[e] = null;
            try {
              n();
            } catch (r) {
              this.j(r);
            }
          }
        }
        this.g = null;
      }, n.prototype.j = function (t) {
        this.i(function () {
          throw t;
        });
      }, e.prototype.j = function () {
        function t(t) {
          return function (r) {
            n || (n = !0, t.call(e, r));
          };
        }
        var e = this,
          n = !1;
        return {
          resolve: t(this.C),
          reject: t(this.l)
        };
      }, e.prototype.C = function (t) {
        if (t === this) this.l(TypeError("A Promise cannot resolve to itself"));else if (t instanceof e) this.F(t);else {
          a: switch (typeof t) {
            case "object":
              var n = null != t;
              break a;
            case "function":
              n = !0;
              break a;
            default:
              n = !1;
          }
          n ? this.u(t) : this.m(t);
        }
      }, e.prototype.u = function (t) {
        var e = void 0;
        try {
          e = t.then;
        } catch (n) {
          this.l(n);
          return;
        }
        "function" == typeof e ? this.G(e, t) : this.m(t);
      }, e.prototype.l = function (t) {
        this.s(2, t);
      }, e.prototype.m = function (t) {
        this.s(1, t);
      }, e.prototype.s = function (t, e) {
        if (0 != this.h) throw Error("Cannot settle(" + t + ", " + e + "): Promise already settled in state" + this.h);
        this.h = t, this.i = e, 2 === this.h && this.D(), this.A();
      }, e.prototype.D = function () {
        var t = this;
        o(function () {
          if (t.B()) {
            var e = r.console;
            void 0 !== e && e.error(t.i);
          }
        }, 1);
      }, e.prototype.B = function () {
        if (this.o) return !1;
        var t = r.CustomEvent,
          e = r.Event,
          n = r.dispatchEvent;
        return void 0 === n || ("function" == typeof t ? t = new t("unhandledrejection", {
          cancelable: !0
        }) : "function" == typeof e ? t = new e("unhandledrejection", {
          cancelable: !0
        }) : (t = r.document.createEvent("CustomEvent")).initCustomEvent("unhandledrejection", !1, !0, t), t.promise = this, t.reason = this.i, n(t));
      }, e.prototype.A = function () {
        if (null != this.g) {
          for (var t = 0; t < this.g.length; ++t) u.h(this.g[t]);
          this.g = null;
        }
      };
      var u = new n();
      return e.prototype.F = function (t) {
        var e = this.j();
        t.J(e.resolve, e.reject);
      }, e.prototype.G = function (t, e) {
        var n = this.j();
        try {
          t.call(e, n.resolve, n.reject);
        } catch (r) {
          n.reject(r);
        }
      }, e.prototype.then = function (t, n) {
        function r(t, e) {
          return "function" == typeof t ? function (e) {
            try {
              i(t(e));
            } catch (n) {
              o(n);
            }
          } : e;
        }
        var i,
          o,
          a = new e(function (t, e) {
            i = t, o = e;
          });
        return this.J(r(t, i), r(n, o)), a;
      }, e.prototype.catch = function (t) {
        return this.then(void 0, t);
      }, e.prototype.J = function (t, e) {
        function n() {
          switch (r.h) {
            case 1:
              t(r.i);
              break;
            case 2:
              e(r.i);
              break;
            default:
              throw Error("Unexpected state: " + r.h);
          }
        }
        var r = this;
        null == this.g ? u.h(n) : this.g.push(n), this.o = !0;
      }, e.resolve = i, e.reject = function (t) {
        return new e(function (e, n) {
          n(t);
        });
      }, e.race = function (t) {
        return new e(function (e, n) {
          for (var r = a(t), o = r.next(); !o.done; o = r.next()) i(o.value).J(e, n);
        });
      }, e.all = function (t) {
        var n = a(t),
          r = n.next();
        return r.done ? i([]) : new e(function (t, e) {
          function o(e) {
            return function (n) {
              a[e] = n, 0 == --u && t(a);
            };
          }
          var a = [],
            u = 0;
          do a.push(void 0), u++, i(r.value).J(o(a.length - 1), e), r = n.next(); while (!r.done);
        });
      }, e;
    }), i("Object.is", function (t) {
      return t || function (t, e) {
        return t === e ? 0 !== t || 1 / t == 1 / e : t != t && e != e;
      };
    }), i("Array.prototype.includes", function (t) {
      return t || function (t, e) {
        var n = this;
        n instanceof String && (n = String(n));
        var r = n.length;
        for (0 > (e = e || 0) && (e = Math.max(e + r, 0)); e < r; e++) {
          var i = n[e];
          if (i === t || Object.is(i, t)) return !0;
        }
        return !1;
      };
    }), i("String.prototype.includes", function (t) {
      return t || function (t, e) {
        if (null == this) throw TypeError("The 'this' value for String.prototype.includes must not be null or undefined");
        if (t instanceof RegExp) throw TypeError("First argument to String.prototype.includes must not be a regular expression");
        return -1 !== this.indexOf(t, e || 0);
      };
    }), i("Array.prototype.keys", function (t) {
      return t || function () {
        var t, e, n, r, i;
        return t = this, e = function (t) {
          return t;
        }, t instanceof String && (t += ""), n = 0, r = !1, (i = {
          next: function () {
            if (!r && n < t.length) {
              var i = n++;
              return {
                value: e(i, t[i]),
                done: !1
              };
            }
            return r = !0, {
              done: !0,
              value: void 0
            };
          }
        })[Symbol.iterator] = function () {
          return i;
        }, i;
      };
    });
    var k = this || self;
    function j(t, e) {
      t = t.split(".");
      var n,
        r = k;
      for ((t[0] in r) || void 0 === r.execScript || r.execScript("var " + t[0]); t.length && (n = t.shift());) t.length || void 0 === e ? r = r[n] && r[n] !== Object.prototype[n] ? r[n] : r[n] = {} : r[n] = e;
    }
    function F(t, e) {
      return e = String.fromCharCode.apply(null, e), null == t ? e : t + e;
    }
    var R,
      S,
      T,
      C = "undefined" != typeof TextDecoder,
      E = "undefined" != typeof TextEncoder;
    function O(t) {
      if (E) t = (T || (T = new TextEncoder())).encode(t);else {
        var e = void 0;
        e = void 0 !== e && e;
        for (var n = 0, r = new Uint8Array(3 * t.length), i = 0; i < t.length; i++) {
          var o = t.charCodeAt(i);
          if (128 > o) r[n++] = o;else {
            if (2048 > o) r[n++] = o >> 6 | 192;else {
              if (55296 <= o && 57343 >= o) {
                if (56319 >= o && i < t.length) {
                  var a = t.charCodeAt(++i);
                  if (56320 <= a && 57343 >= a) {
                    o = 1024 * (o - 55296) + a - 56320 + 65536, r[n++] = o >> 18 | 240, r[n++] = o >> 12 & 63 | 128, r[n++] = o >> 6 & 63 | 128, r[n++] = 63 & o | 128;
                    continue;
                  }
                  i--;
                }
                if (e) throw Error("Found an unpaired surrogate");
                o = 65533;
              }
              r[n++] = o >> 12 | 224, r[n++] = o >> 6 & 63 | 128;
            }
            r[n++] = 63 & o | 128;
          }
        }
        t = r.subarray(0, n);
      }
      return t;
    }
    var B = {},
      P = null;
    function L(t, e) {
      void 0 === e && (e = 0), U(), e = B[e];
      for (var n = Array(Math.floor(t.length / 3)), r = e[64] || "", i = 0, o = 0; i < t.length - 2; i += 3) {
        var a = t[i],
          u = t[i + 1],
          s = t[i + 2],
          f = e[a >> 2];
        a = e[(3 & a) << 4 | u >> 4], u = e[(15 & u) << 2 | s >> 6], s = e[63 & s], n[o++] = f + a + u + s;
      }
      switch (f = 0, s = r, t.length - i) {
        case 2:
          s = e[(15 & (f = t[i + 1])) << 2] || r;
        case 1:
          t = t[i], n[o] = e[t >> 2] + e[(3 & t) << 4 | f >> 4] + s + r;
      }
      return n.join("");
    }
    function U() {
      if (!P) {
        P = {};
        for (var t = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789".split(""), e = ["+/=", "+/", "-_=", "-_.", "-_"], n = 0; 5 > n; n++) {
          var r = t.concat(e[n].split(""));
          B[n] = r;
          for (var i = 0; i < r.length; i++) {
            var o = r[i];
            void 0 === P[o] && (P[o] = i);
          }
        }
      }
    }
    var I,
      N = "function" == typeof Uint8Array.prototype.slice;
    function D(t, e, n) {
      return e === n ? I || (I = new Uint8Array(0)) : N ? t.slice(e, n) : new Uint8Array(t.subarray(e, n));
    }
    var M = 0,
      G = 0;
    function V(t, e) {
      e = void 0 !== (e = void 0 === e ? {} : e).v && e.v, this.h = null, this.g = this.i = this.j = 0, this.l = !1, this.v = e, t && W(this, t);
    }
    function W(t, e) {
      var n, r, i, o, a;
      e = e.constructor === Uint8Array ? e : e.constructor === ArrayBuffer ? new Uint8Array(e) : e.constructor === Array ? new Uint8Array(e) : e.constructor === String ? ((i = 3 * (r = (n = e).length) / 4) % 3 ? i = Math.floor(i) : -1 != "=.".indexOf(n[r - 1]) && (i = -1 != "=.".indexOf(n[r - 2]) ? i - 2 : i - 1), o = new Uint8Array(i), a = 0, function t(e, n) {
        function r(t) {
          for (; i < e.length;) {
            var n = e.charAt(i++),
              r = P[n];
            if (null != r) return r;
            if (!/^[\s\xa0]*$/.test(n)) throw Error("Unknown base64 encoding at char: " + n);
          }
          return t;
        }
        U();
        for (var i = 0;;) {
          var o = r(-1),
            a = r(0),
            u = r(64),
            s = r(64);
          if (64 === s && -1 === o) break;
          n(o << 2 | a >> 4), 64 != u && (n(a << 4 & 240 | u >> 2), 64 != s && n(u << 6 & 192 | s));
        }
      }(n, function (t) {
        o[a++] = t;
      }), o.subarray(0, a)) : e instanceof Uint8Array ? new Uint8Array(e.buffer, e.byteOffset, e.byteLength) : new Uint8Array(0), t.h = e, t.j = 0, t.i = t.h.length, t.g = t.j;
    }
    function z(t) {
      var e = t.h,
        n = e[t.g],
        r = 127 & n;
      return 128 > n ? (t.g += 1, r) : (r |= (127 & (n = e[t.g + 1])) << 7, 128 > n) ? (t.g += 2, r) : (r |= (127 & (n = e[t.g + 2])) << 14, 128 > n) ? (t.g += 3, r) : (r |= (127 & (n = e[t.g + 3])) << 21, 128 > n) ? (t.g += 4, r) : (r |= (15 & (n = e[t.g + 4])) << 28, 128 > n) ? (t.g += 5, r >>> 0) : (t.g += 5, 128 <= e[t.g++] && 128 <= e[t.g++] && 128 <= e[t.g++] && 128 <= e[t.g++] && t.g++, r);
    }
    function X(t) {
      var e = t.h[t.g],
        n = t.h[t.g + 1],
        r = t.h[t.g + 2],
        i = t.h[t.g + 3];
      return t.g += 4, t = 2 * ((n = (e << 0 | n << 8 | r << 16 | i << 24) >>> 0) >> 31) + 1, e = n >>> 23 & 255, n &= 8388607, 255 == e ? n ? NaN : 1 / 0 * t : 0 == e ? 1401298464324817e-60 * t * n : t * Math.pow(2, e - 150) * (n + 8388608);
    }
    V.prototype.reset = function () {
      this.g = this.j;
    };
    var Y = [];
    function H() {
      this.g = new Uint8Array(64), this.h = 0;
    }
    function J(t, e) {
      for (; 127 < e;) t.push(127 & e | 128), e >>>= 7;
      t.push(e);
    }
    function K(t) {
      var e = {},
        n = void 0 !== e.N && e.N;
      this.o = {
        v: void 0 !== e.v && e.v
      }, this.N = n, e = this.o, Y.length ? (n = Y.pop(), e && (n.v = e.v), t && W(n, t), t = n) : t = new V(t, e), this.g = t, this.m = this.g.g, this.h = this.i = this.l = -1, this.j = !1;
    }
    function Z(t) {
      var e = t.g;
      if ((e = e.g == e.i) || (e = t.j) || (e = (e = t.g).l || 0 > e.g || e.g > e.i), e) return !1;
      t.m = t.g.g;
      var n = 7 & (e = z(t.g));
      return 0 != n && 5 != n && 1 != n && 2 != n && 3 != n && 4 != n ? (t.j = !0, !1) : (t.i = e, t.l = e >>> 3, t.h = n, !0);
    }
    function q(t, e, n) {
      var r = t.g.i,
        i = z(t.g);
      return i = t.g.g + i, t.g.i = i, n(e, t), t.g.g = i, t.g.i = r, e;
    }
    function Q(t) {
      var e = z(t.g),
        n = (t = t.g).g;
      if (t.g += e, t = t.h, C) (r = S) || (r = S = new TextDecoder("utf-8", {
        fatal: !1
      })), r = r.decode(t.subarray(n, n + e));else {
        e = n + e;
        for (var r, i, o, a, u = [], s = null; n < e;) 128 > (i = t[n++]) ? u.push(i) : 224 > i ? n >= e ? u.push(65533) : (o = t[n++], 194 > i || 128 != (192 & o) ? (n--, u.push(65533)) : u.push((31 & i) << 6 | 63 & o)) : 240 > i ? n >= e - 1 ? u.push(65533) : 128 != (192 & (o = t[n++])) || 224 === i && 160 > o || 237 === i && 160 <= o || 128 != (192 & (r = t[n++])) ? (n--, u.push(65533)) : u.push((15 & i) << 12 | (63 & o) << 6 | 63 & r) : 244 >= i ? n >= e - 2 ? u.push(65533) : 128 != (192 & (o = t[n++])) || 0 != (i << 28) + (o - 144) >> 30 || 128 != (192 & (r = t[n++])) || 128 != (192 & (a = t[n++])) ? (n--, u.push(65533)) : (i = (7 & i) << 18 | (63 & o) << 12 | (63 & r) << 6 | 63 & a, i -= 65536, u.push((i >> 10 & 1023) + 55296, (1023 & i) + 56320)) : u.push(65533), 8192 <= u.length && (s = F(s, u), u.length = 0);
        r = F(s, u);
      }
      return r;
    }
    function tt() {
      this.h = [], this.i = 0, this.g = new H();
    }
    function te(t, e) {
      0 !== e.length && (t.h.push(e), t.i += e.length);
    }
    function tn(t) {
      var e = t.i + t.g.length();
      if (0 === e) return new Uint8Array(0);
      e = new Uint8Array(e);
      for (var n = t.h, r = n.length, i = 0, o = 0; o < r; o++) {
        var a = n[o];
        0 !== a.length && (e.set(a, i), i += a.length);
      }
      return 0 !== (r = (n = t.g).h) && (e.set(n.g.subarray(0, r), i), n.h = 0), t.h = [e], e;
    }
    function tr(t, e, n) {
      if (null != n) {
        J(t.g, 8 * e + 5), t = t.g;
        var r = n;
        0 === (r = (n = 0 > r ? 1 : 0) ? -r : r) ? 0 < 1 / r ? M = G = 0 : (G = 0, M = 2147483648) : isNaN(r) ? (G = 0, M = 2147483647) : 34028234663852886e22 < r ? (G = 0, M = (n << 31 | 2139095040) >>> 0) : 11754943508222875e-54 > r ? (G = 0, M = (n << 31 | (r = Math.round(r / 1401298464324817e-60))) >>> 0) : (e = Math.floor(Math.log(r) / Math.LN2), r *= Math.pow(2, -e), G = 0, M = (n << 31 | e + 127 << 23 | (r = 8388607 & Math.round(8388608 * r))) >>> 0), n = M, t.push(n >>> 0 & 255), t.push(n >>> 8 & 255), t.push(n >>> 16 & 255), t.push(n >>> 24 & 255);
      }
    }
    H.prototype.push = function (t) {
      if (!(this.h + 1 < this.g.length)) {
        var e = this.g;
        this.g = new Uint8Array(Math.ceil(1 + 2 * this.g.length)), this.g.set(e);
      }
      this.g[this.h++] = t;
    }, H.prototype.length = function () {
      return this.h;
    }, H.prototype.end = function () {
      var t = this.g,
        e = this.h;
      return this.h = 0, D(t, 0, e);
    }, K.prototype.reset = function () {
      this.g.reset(), this.h = this.l = -1;
    };
    var ti = "function" == typeof Uint8Array;
    function to(t, e, n) {
      if (null != t) return "object" == typeof t ? ti && t instanceof Uint8Array ? n(t) : ta(t, e, n) : e(t);
    }
    function ta(t, e, n) {
      if (Array.isArray(t)) {
        for (var r = Array(t.length), i = 0; i < t.length; i++) r[i] = to(t[i], e, n);
        return Array.isArray(t) && t.W && tf(r), r;
      }
      for (i in r = {}, t) r[i] = to(t[i], e, n);
      return r;
    }
    function tu(t) {
      return "number" == typeof t ? isFinite(t) ? t : String(t) : t;
    }
    var ts = {
      W: {
        value: !0,
        configurable: !0
      }
    };
    function tf(t) {
      return Array.isArray(t) && !Object.isFrozen(t) && Object.defineProperties(t, ts), t;
    }
    function tc(t, n, r) {
      var i = e;
      e = null, t || (t = i), i = this.constructor.ca, t || (t = i ? [i] : []), this.j = i ? 0 : -1, this.i = null, this.g = t;
      a: {
        if (t = (i = this.g.length) - 1, i && null !== (i = this.g[t]) && "object" == typeof i && i.constructor === Object) {
          this.l = t - this.j, this.h = i;
          break a;
        }
        void 0 !== n && -1 < n ? (this.l = Math.max(n, t + 1 - this.j), this.h = null) : this.l = Number.MAX_VALUE;
      }
      if (r) for (n = 0; n < r.length; n++) (t = r[n]) < this.l ? (t += this.j, (i = this.g[t]) ? tf(i) : this.g[t] = tl) : (th(this), (i = this.h[t]) ? tf(i) : this.h[t] = tl);
    }
    var tl = Object.freeze(tf([]));
    function th(t) {
      var e = t.l + t.j;
      t.g[e] || (t.h = t.g[e] = {});
    }
    function tg(t, e, n) {
      return -1 === e ? null : (void 0 === n ? 0 : n) || e >= t.l ? t.h ? t.h[e] : void 0 : t.g[e + t.j];
    }
    function t$(t) {
      var e = void 0 !== e && e,
        n = tg(t, 1, e);
      return null == n && (n = tl), n === tl && tv(t, 1, n = tf([]), e), n;
    }
    function tp(t, e, n) {
      return null == (t = null == (t = tg(t, e)) ? t : +t) ? void 0 === n ? 0 : n : t;
    }
    function tv(t, e, n, r) {
      (void 0 === r ? 0 : r) || e >= t.l ? (th(t), t.h[e] = n) : t.g[e + t.j] = n;
    }
    function td(t, e) {
      t.i || (t.i = {});
      var n = t.i[1];
      if (!n) {
        var r = t$(t);
        n = [];
        for (var i = 0; i < r.length; i++) n[i] = new e(r[i]);
        t.i[1] = n;
      }
      return n;
    }
    function t_(t, e, n, r) {
      var i = td(t, n);
      e = e || new n(), t = t$(t), void 0 != r ? (i.splice(r, 0, e), t.splice(r, 0, t0(e))) : (i.push(e), t.push(t0(e)));
    }
    function t0(t, e) {
      if (t.i) for (var n in t.i) {
        var r = t.i[n];
        if (Array.isArray(r)) for (var i = 0; i < r.length; i++) r[i] && t0(r[i]);else r && t0(r);
      }
      return t.g;
    }
    function ty(t, e) {
      return null == (t = tg(t, e)) ? 0 : t;
    }
    function tm(t, e) {
      return null == (t = tg(t, e)) ? "" : t;
    }
    function tb(t, e) {
      if (t = t.m) {
        te(e, e.g.end());
        for (var n = 0; n < t.length; n++) te(e, t[n]);
      }
    }
    function tw(t, e) {
      if (4 == e.h) return !1;
      var n = e.m;
      return !function t(e) {
        switch (e.h) {
          case 0:
            if (0 != e.h) t(e);else {
              for (e = e.g; 128 & e.h[e.g];) e.g++;
              e.g++;
            }
            break;
          case 1:
            1 != e.h ? t(e) : (e = e.g, e.g += 8);
            break;
          case 2:
            if (2 != e.h) t(e);else {
              var n = z(e.g);
              e = e.g, e.g += n;
            }
            break;
          case 5:
            5 != e.h ? t(e) : (e = e.g, e.g += 4);
            break;
          case 3:
            for (n = e.l;;) {
              if (!Z(e)) {
                e.j = !0;
                break;
              }
              if (4 == e.h) {
                e.l != n && (e.j = !0);
                break;
              }
              t(e);
            }
            break;
          default:
            e.j = !0;
        }
      }(e), e.N || (e = D(e.g.h, n, e.g.g), (n = t.m) ? n.push(e) : t.m = [e]), !0;
    }
    function t1(t, e) {
      var n = void 0;
      return new (n || (n = Promise))(function (r, i) {
        function o(t) {
          try {
            u(e.next(t));
          } catch (n) {
            i(n);
          }
        }
        function a(t) {
          try {
            u(e.throw(t));
          } catch (n) {
            i(n);
          }
        }
        function u(t) {
          t.done ? r(t.value) : new n(function (e) {
            e(t.value);
          }).then(o, a);
        }
        u((e = e.apply(t, void 0)).next());
      });
    }
    function t2(t) {
      tc.call(this, t);
    }
    function tx(t, e) {
      for (; Z(e);) switch (e.i) {
        case 8:
          var n = z(e.g);
          tv(t, 1, n);
          break;
        case 21:
          tv(t, 2, n = X(e.g));
          break;
        case 26:
          tv(t, 3, n = Q(e));
          break;
        case 34:
          tv(t, 4, n = Q(e));
          break;
        default:
          if (!tw(t, e)) return t;
      }
      return t;
    }
    function tA(t) {
      tc.call(this, t, -1, t4);
    }
    tc.prototype.toJSON = function () {
      return ta(t0(this), tu, L);
    }, tc.prototype.toString = function () {
      return t0(this).toString();
    }, $(t2, tc), $(tA, tc), tA.prototype.addClassification = function (t, e) {
      t_(this, t, t2, e);
    };
    var t4 = [1];
    function t6(t) {
      tc.call(this, t);
    }
    function t3(t, e) {
      for (; Z(e);) switch (e.i) {
        case 13:
          var n = X(e.g);
          tv(t, 1, n);
          break;
        case 21:
          tv(t, 2, n = X(e.g));
          break;
        case 29:
          tv(t, 3, n = X(e.g));
          break;
        case 37:
          tv(t, 4, n = X(e.g));
          break;
        case 45:
          tv(t, 5, n = X(e.g));
          break;
        default:
          if (!tw(t, e)) return t;
      }
      return t;
    }
    function tk(t) {
      tc.call(this, t, -1, tj);
    }
    $(t6, tc), $(tk, tc);
    var tj = [1];
    function t8(t) {
      tc.call(this, t);
    }
    function tF(t, e, n) {
      if (n = t.createShader(0 === n ? t.VERTEX_SHADER : t.FRAGMENT_SHADER), t.shaderSource(n, e), t.compileShader(n), !t.getShaderParameter(n, t.COMPILE_STATUS)) throw Error("Could not compile WebGL shader.\n\n" + t.getShaderInfoLog(n));
      return n;
    }
    function tR(t) {
      return td(t, t2).map(function (t) {
        return {
          index: ty(t, 1),
          Y: tp(t, 2),
          label: null != tg(t, 3) ? tm(t, 3) : void 0,
          displayName: null != tg(t, 4) ? tm(t, 4) : void 0
        };
      });
    }
    function tS(t) {
      return {
        x: tp(t, 1),
        y: tp(t, 2),
        z: tp(t, 3),
        visibility: null != tg(t, 4) ? tp(t, 4) : void 0
      };
    }
    function t5(t, e) {
      this.h = t, this.g = e, this.l = 0;
    }
    function tT(t, e, n) {
      return (function t(e, n) {
        var r = e.g;
        if (void 0 === e.m) {
          var i = tF(r, "\n  attribute vec2 aVertex;\n  attribute vec2 aTex;\n  varying vec2 vTex;\n  void main(void) {\n    gl_Position = vec4(aVertex, 0.0, 1.0);\n    vTex = aTex;\n  }", 0),
            o = tF(r, "\n  precision mediump float;\n  varying vec2 vTex;\n  uniform sampler2D sampler0;\n  void main(){\n    gl_FragColor = texture2D(sampler0, vTex);\n  }", 1),
            a = r.createProgram();
          if (r.attachShader(a, i), r.attachShader(a, o), r.linkProgram(a), !r.getProgramParameter(a, r.LINK_STATUS)) throw Error("Could not compile WebGL program.\n\n" + r.getProgramInfoLog(a));
          i = e.m = a, r.useProgram(i), o = r.getUniformLocation(i, "sampler0"), e.j = {
            I: r.getAttribLocation(i, "aVertex"),
            H: r.getAttribLocation(i, "aTex"),
            da: o
          }, e.s = r.createBuffer(), r.bindBuffer(r.ARRAY_BUFFER, e.s), r.enableVertexAttribArray(e.j.I), r.vertexAttribPointer(e.j.I, 2, r.FLOAT, !1, 0, 0), r.bufferData(r.ARRAY_BUFFER, new Float32Array([-1, -1, -1, 1, 1, 1, 1, -1]), r.STATIC_DRAW), r.bindBuffer(r.ARRAY_BUFFER, null), e.o = r.createBuffer(), r.bindBuffer(r.ARRAY_BUFFER, e.o), r.enableVertexAttribArray(e.j.H), r.vertexAttribPointer(e.j.H, 2, r.FLOAT, !1, 0, 0), r.bufferData(r.ARRAY_BUFFER, new Float32Array([0, 1, 0, 0, 1, 0, 1, 1]), r.STATIC_DRAW), r.bindBuffer(r.ARRAY_BUFFER, null), r.uniform1i(o, 0);
        }
        i = e.j, r.useProgram(e.m), r.canvas.width = n.width, r.canvas.height = n.height, r.viewport(0, 0, n.width, n.height), r.activeTexture(r.TEXTURE0), e.h.bindTexture2d(n.glName), r.enableVertexAttribArray(i.I), r.bindBuffer(r.ARRAY_BUFFER, e.s), r.vertexAttribPointer(i.I, 2, r.FLOAT, !1, 0, 0), r.enableVertexAttribArray(i.H), r.bindBuffer(r.ARRAY_BUFFER, e.o), r.vertexAttribPointer(i.H, 2, r.FLOAT, !1, 0, 0), r.bindFramebuffer(r.DRAW_FRAMEBUFFER ? r.DRAW_FRAMEBUFFER : r.FRAMEBUFFER, null), r.clearColor(0, 0, 0, 0), r.clear(r.COLOR_BUFFER_BIT), r.colorMask(!0, !0, !0, !0), r.drawArrays(r.TRIANGLE_FAN, 0, 4), r.disableVertexAttribArray(i.I), r.disableVertexAttribArray(i.H), r.bindBuffer(r.ARRAY_BUFFER, null), e.h.bindTexture2d(0);
      }(t, e), "function" == typeof t.g.canvas.transferToImageBitmap) ? Promise.resolve(t.g.canvas.transferToImageBitmap()) : n ? Promise.resolve(t.g.canvas) : "function" == typeof createImageBitmap ? createImageBitmap(t.g.canvas) : (void 0 === t.i && (t.i = document.createElement("canvas")), new Promise(function (e) {
        t.i.height = t.g.canvas.height, t.i.width = t.g.canvas.width, t.i.getContext("2d", {}).drawImage(t.g.canvas, 0, 0, t.g.canvas.width, t.g.canvas.height), e(t.i);
      }));
    }
    function tC(t) {
      this.g = t;
    }
    $(t8, tc);
    var tE = new Uint8Array([0, 97, 115, 109, 1, 0, 0, 0, 1, 4, 1, 96, 0, 0, 3, 2, 1, 0, 10, 9, 1, 7, 0, 65, 0, 253, 15, 26, 11]);
    function tO(t, e) {
      return e + t;
    }
    function tB(t, e) {
      window[t] = e;
    }
    function tP(t) {
      if (this.g = t, this.listeners = {}, this.j = {}, this.F = {}, this.m = {}, this.s = {}, this.G = this.o = this.R = !0, this.C = Promise.resolve(), this.P = "", this.B = {}, this.locateFile = t && t.locateFile || tO, "object" == typeof window) var e = window.location.pathname.toString().substring(0, window.location.pathname.toString().lastIndexOf("/")) + "/";else if ("undefined" != typeof location) e = location.pathname.toString().substring(0, location.pathname.toString().lastIndexOf("/")) + "/";else throw Error("solutions can only be loaded on a web page or in a web worker");
      if (this.S = e, t.options) {
        e = a(Object.keys(t.options));
        for (var n = e.next(); !n.done; n = e.next()) {
          n = n.value;
          var r = t.options[n].default;
          void 0 !== r && (this.j[n] = "function" == typeof r ? r() : r);
        }
      }
    }
    function tL(t, e) {
      return t1(t, function t() {
        var r,
          i = this;
        return x(t, function (t) {
          return e in i.F ? t.return(i.F[e]) : (r = fetch(i.locateFile(e, "")).then(function (t) {
            return t.arrayBuffer();
          }), i.F[e] = r, t.return(r));
        });
      });
    }
    function t7(t, e) {
      for (var n = e.name || "$", r = [].concat(u(e.wants)), i = new t.h.StringList(), o = a(e.wants), s = o.next(); !s.done; s = o.next()) i.push_back(s.value);
      o = t.h.PacketListener.implement({
        onResults: function (i) {
          for (var o = {}, u = 0; u < e.wants.length; ++u) o[r[u]] = i.get(u);
          var s,
            f,
            c,
            l = t.listeners[n];
          l && (t.C = (s = t, f = o, c = e.outs, t1(s, function t() {
            var e,
              n,
              r,
              i,
              u,
              s,
              l,
              h,
              g,
              $,
              p,
              v,
              d,
              y = this;
            return x(t, function (t) {
              switch (t.g) {
                case 1:
                  if (!c) return t.return(f);
                  for (e = {}, n = 0, i = (r = a(Object.keys(c))).next(); !i.done; i = r.next()) "string" != typeof (u = c[i.value]) && "texture" === u.type && void 0 !== f[u.stream] && ++n;
                  1 < n && (y.G = !1), i = (s = a(Object.keys(c))).next();
                case 2:
                  if (i.done) {
                    t.g = 4;
                    break;
                  }
                  if ("string" == typeof (h = c[l = i.value])) {
                    var m, b, w;
                    return v = e, d = l, _(t, (m = y, b = l, w = f[h], t1(m, function t() {
                      var e,
                        n = this;
                      return x(t, function (t) {
                        return "number" == typeof w || w instanceof Uint8Array || w instanceof n.h.Uint8BlobList ? t.return(w) : w instanceof n.h.Texture2dDataOut ? ((e = n.s[b]) || (e = new t5(n.h, n.D), n.s[b] = e), t.return(tT(e, w, n.G))) : t.return(void 0);
                      });
                    })), 14);
                  }
                  if (g = f[h.stream], "detection_list" === h.type) {
                    if (g) {
                      for (var A = g.getRectList(), k = g.getLandmarksList(), j = g.getClassificationsList(), F = [], R = 0; R < A.size(); ++R) {
                        var S = A.get(R);
                        a: {
                          var T = new t8();
                          for (S = new K(S); Z(S);) switch (S.i) {
                            case 13:
                              var C = X(S.g);
                              tv(T, 1, C);
                              break;
                            case 21:
                              tv(T, 2, C = X(S.g));
                              break;
                            case 29:
                              tv(T, 3, C = X(S.g));
                              break;
                            case 37:
                              tv(T, 4, C = X(S.g));
                              break;
                            case 45:
                              tv(T, 5, C = X(S.g));
                              break;
                            case 48:
                              for (var E = S.g, O = 128, B = 0, P = C = 0; 4 > P && 128 <= O; P++) B |= (127 & (O = E.h[E.g++])) << 7 * P;
                              if (128 <= O && (B |= (127 & (O = E.h[E.g++])) << 28, C |= (127 & O) >> 4), 128 <= O) for (P = 0; 5 > P && 128 <= O; P++) C |= (127 & (O = E.h[E.g++])) << 7 * P + 3;
                              128 > O ? (E = B >>> 0, (C = 2147483648 & (O = C >>> 0)) && (O = ~O >>> 0, 0 == (E = ~E + 1 >>> 0) && (O = O + 1 >>> 0)), E = 4294967296 * O + (E >>> 0), C = C ? -E : E) : (E.l = !0, C = void 0), tv(T, 6, C);
                              break;
                            default:
                              if (!tw(T, S)) break a;
                          }
                        }
                        T = {
                          Z: tp(T, 1),
                          $: tp(T, 2),
                          height: tp(T, 3),
                          width: tp(T, 4),
                          rotation: tp(T, 5, 0),
                          X: ty(T, 6)
                        }, C = k.get(R);
                        a: for (S = new tk(), C = new K(C); Z(C);) if (10 === C.i) t_(S, E = q(C, new t6(), t3), t6, void 0);else if (!tw(S, C)) break a;
                        S = td(S, t6).map(tS), E = j.get(R);
                        a: for (C = new tA(), E = new K(E); Z(E);) if (10 === E.i) C.addClassification(q(E, new t2(), tx));else if (!tw(C, E)) break a;
                        T = {
                          T: T,
                          O: S,
                          M: tR(C)
                        }, F.push(T);
                      }
                      A = F;
                    } else A = [];
                    e[l] = A, t.g = 7;
                    break;
                  }
                  if ("proto_list" === h.type) {
                    if (g) {
                      for (k = 0, A = Array(g.size()); k < g.size(); k++) A[k] = g.get(k);
                      g.delete();
                    } else A = [];
                    e[l] = A, t.g = 7;
                    break;
                  }
                  if (void 0 === g) {
                    t.g = 3;
                    break;
                  }
                  if ("float_list" === h.type || "proto" === h.type) {
                    e[l] = g, t.g = 7;
                    break;
                  }
                  if ("texture" !== h.type) throw Error("Unknown output config type: '" + h.type + "'");
                  return ($ = y.s[l]) || ($ = new t5(y.h, y.D), y.s[l] = $), _(t, tT($, g, y.G), 13);
                case 13:
                  p = t.h, e[l] = p;
                case 7:
                  h.transform && e[l] && (e[l] = h.transform(e[l])), t.g = 3;
                  break;
                case 14:
                  v[d] = t.h;
                case 3:
                  i = s.next(), t.g = 2;
                  break;
                case 4:
                  return t.return(e);
              }
            });
          })).then(function (n) {
            n = l(n);
            for (var i = 0; i < e.wants.length; ++i) {
              var a = o[r[i]];
              "object" == typeof a && a.hasOwnProperty && a.hasOwnProperty("delete") && a.delete();
            }
            n && (t.C = n);
          }));
        }
      }), t.i.attachMultiListener(i, o), i.delete();
    }
    function tU(t) {
      return (void 0 === t && (t = 0), 1 === t) ? "selfie_segmentation_landscape.tflite" : "selfie_segmentation.tflite";
    }
    function tI(t) {
      var e = this;
      t = t || {}, this.g = new tP({
        locateFile: t.locateFile,
        files: function (t) {
          return [{
            simd: !0,
            url: "selfie_segmentation_solution_simd_wasm_bin.js"
          }, {
            simd: !1,
            url: "selfie_segmentation_solution_wasm_bin.js"
          }, {
            data: !0,
            url: tU(t.modelSelection)
          }];
        },
        graph: {
          url: "selfie_segmentation.binarypb"
        },
        listeners: [{
          wants: ["segmentation_mask", "image_transformed"],
          outs: {
            image: {
              type: "texture",
              stream: "image_transformed"
            },
            segmentationMask: {
              type: "texture",
              stream: "segmentation_mask"
            }
          }
        }],
        inputs: {
          image: {
            type: "video",
            stream: "input_frames_gpu"
          }
        },
        options: {
          useCpuInference: {
            type: 0,
            graphOptionXref: {
              calculatorType: "InferenceCalculator",
              fieldName: "use_cpu_inference"
            },
            default: "iPad Simulator;iPhone Simulator;iPod Simulator;iPad;iPhone;iPod".split(";").includes(navigator.platform) || navigator.userAgent.includes("Mac") && "ontouchend" in document
          },
          selfieMode: {
            type: 0,
            graphOptionXref: {
              calculatorType: "GlScalerCalculator",
              calculatorIndex: 1,
              fieldName: "flip_horizontal"
            }
          },
          modelSelection: {
            type: 1,
            graphOptionXref: {
              calculatorType: "ConstantSidePacketCalculator",
              calculatorName: "ConstantSidePacketCalculatorModelSelection",
              fieldName: "int_value"
            },
            onChange: function (t) {
              return t1(e, function e() {
                var n,
                  r,
                  i,
                  o = this;
                return x(e, function (e) {
                  return 1 == e.g ? (r = "third_party/mediapipe/modules/selfie_segmentation/" + (n = tU(t)), _(e, tL(o.g, n), 2)) : (i = e.h, o.g.overrideFile(r, i), e.return(!0));
                });
              });
            }
          }
        }
      });
    }
    (s = tP.prototype).close = function () {
      return this.i && this.i.delete(), Promise.resolve();
    }, s.reset = function () {
      return t1(this, function t() {
        var e = this;
        return x(t, function (t) {
          e.i && (e.i.reset(), e.m = {}, e.s = {}), t.g = 0;
        });
      });
    }, s.setOptions = function (t, e) {
      var n = this;
      if (e = e || this.g.options) {
        for (var r = [], i = [], o = {}, u = a(Object.keys(t)), s = u.next(); !s.done; o = {
          K: o.K,
          L: o.L
        }, s = u.next()) {
          var f = s.value;
          f in this.j && this.j[f] === t[f] || (this.j[f] = t[f], void 0 !== (s = e[f]) && (s.onChange && (o.K = s.onChange, o.L = t[f], r.push(function (t) {
            return function () {
              return t1(n, function e() {
                var r = this;
                return x(e, function (e) {
                  if (1 == e.g) return _(e, t.K(t.L), 2);
                  !0 === (e.h) && (r.o = !0), e.g = 0;
                });
              });
            };
          }(o))), s.graphOptionXref && (f = {
            valueNumber: 1 === s.type ? t[f] : 0,
            valueBoolean: 0 === s.type && t[f],
            valueString: 2 === s.type ? t[f] : ""
          }, s = Object.assign(Object.assign(Object.assign({}, {
            calculatorName: "",
            calculatorIndex: 0
          }), s.graphOptionXref), f), i.push(s))));
        }
        (0 !== r.length || 0 !== i.length) && (this.o = !0, this.A = (void 0 === this.A ? [] : this.A).concat(i), this.u = (void 0 === this.u ? [] : this.u).concat(r));
      }
    }, s.initialize = function () {
      return t1(this, function t() {
        var e = this;
        return x(t, function (t) {
          return 1 == t.g ? _(t, t1(e, function t() {
            var e,
              n,
              r,
              i,
              o,
              a,
              s,
              f,
              c,
              l,
              h,
              g = this;
            return x(t, function (t) {
              switch (t.g) {
                case 1:
                  var $, p;
                  if (e = g, !g.R) return t.return();
                  return n = ($ = g, p = g.j, void 0 === $.g.files ? [] : "function" == typeof $.g.files ? $.g.files(p) : $.g.files), _(t, function t() {
                    return t1(this, function t() {
                      return x(t, function (t) {
                        switch (t.g) {
                          case 1:
                            return t.m = 2, _(t, WebAssembly.instantiate(tE), 4);
                          case 4:
                            t.g = 3, t.m = 0;
                            break;
                          case 2:
                            return t.m = 0, t.j = null, t.return(!1);
                          case 3:
                            return t.return(!0);
                        }
                      });
                    });
                  }(), 2);
                case 2:
                  if (r = t.h, "object" == typeof window) return tB("createMediapipeSolutionsWasm", {
                    locateFile: g.locateFile
                  }), tB("createMediapipeSolutionsPackedAssets", {
                    locateFile: g.locateFile
                  }), a = n.filter(function (t) {
                    return void 0 !== t.data;
                  }), s = n.filter(function (t) {
                    return void 0 === t.data;
                  }), f = Promise.all(a.map(function (t) {
                    var n = tL(e, t.url);
                    if (void 0 !== t.path) {
                      var r = t.path;
                      n = n.then(function (t) {
                        return e.overrideFile(r, t), Promise.resolve(t);
                      });
                    }
                    return n;
                  })), c = Promise.all(s.map(function (t) {
                    var n, i;
                    return void 0 === t.simd || t.simd && r || !t.simd && !r ? (n = e.locateFile(t.url, e.S), (i = document.createElement("script")).setAttribute("src", n), i.setAttribute("lang", "text/javascript"), i.setAttribute("crossorigin", "anonymous"), new Promise(function (t) {
                      i.addEventListener("load", function () {
                        t();
                      }, !1), i.addEventListener("error", function () {
                        t();
                      }, !1), document.body.appendChild(i);
                    })) : Promise.resolve();
                  })).then(function () {
                    return t1(e, function t() {
                      var r = this;
                      return x(t, function (t) {
                        if (1 == t.g) return _(t, (window.createMediapipeSolutionsWasm)(window.createMediapipeSolutionsPackedAssets), 2);
                        r.h = t.h, t.g = 0;
                      });
                    });
                  }), l = t1(e, function t() {
                    var e = this;
                    return x(t, function (t) {
                      return e.g.graph && e.g.graph.url ? t = _(t, tL(e, e.g.graph.url), 0) : (t.g = 0, t = void 0), t;
                    });
                  }), _(t, Promise.all([c, f, l]), 7);
                  if ("function" != typeof importScripts) throw Error("solutions can only be loaded on a web page or in a web worker");
                  return i = n.filter(function (t) {
                    return void 0 === t.simd || t.simd && r || !t.simd && !r;
                  }).map(function (t) {
                    return e.locateFile(t.url, e.S);
                  }), importScripts.apply(null, u(i)), _(t, createMediapipeSolutionsWasm(Module), 6);
                case 6:
                  g.h = t.h, g.l = new OffscreenCanvas(1, 1), g.h.canvas = g.l, o = g.h.GL.createContext(g.l, {
                    antialias: !1,
                    alpha: !1,
                    ba: "undefined" != typeof WebGL2RenderingContext ? 2 : 1
                  }), g.h.GL.makeContextCurrent(o), t.g = 4;
                  break;
                case 7:
                  if (g.l = document.createElement("canvas"), !(h = g.l.getContext("webgl2", {})) && !(h = g.l.getContext("webgl", {}))) return alert("Failed to create WebGL canvas context when passing video frame."), t.return();
                  g.D = h, g.h.canvas = g.l, g.h.createContext(g.l, !0, !0, {});
                case 4:
                  g.i = new g.h.SolutionWasm(), g.R = !1, t.g = 0;
              }
            });
          }), 2) : 3 != t.g ? _(t, t1(e, function t() {
            var e,
              n,
              r,
              i,
              o,
              u,
              f,
              c = this;
            return x(t, function (t) {
              if (1 == t.g) {
                if (c.g.graph && c.g.graph.url && c.P === c.g.graph.url) return t.return();
                if (c.o = !0, !c.g.graph || !c.g.graph.url) {
                  t.g = 2;
                  return;
                }
                return c.P = c.g.graph.url, _(t, tL(c, c.g.graph.url), 3);
              }
              for (2 != t.g && (e = t.h, c.i.loadGraph(e)), r = (n = a(Object.keys(c.B))).next(); !r.done; r = n.next()) i = r.value, c.i.overrideFile(i, c.B[i]);
              if (c.B = {}, c.g.listeners) for (u = (o = a(c.g.listeners)).next(); !u.done; u = o.next()) t7(c, u.value);
              f = c.j, c.j = {}, c.setOptions(f), t.g = 0;
            });
          }), 3) : _(t, t1(e, function t() {
            var e,
              n,
              i,
              o,
              u,
              s,
              f = this;
            return x(t, function (t) {
              switch (t.g) {
                case 1:
                  if (!f.o) return t.return();
                  if (!f.u) {
                    t.g = 2;
                    break;
                  }
                  n = (e = a(f.u)).next();
                case 3:
                  if (n.done) {
                    t.g = 5;
                    break;
                  }
                  return _(t, (n.value)(), 4);
                case 4:
                  n = e.next(), t.g = 3;
                  break;
                case 5:
                  f.u = void 0;
                case 2:
                  if (f.A) {
                    for (i = new f.h.GraphOptionChangeRequestList(), u = (o = a(f.A)).next(); !u.done; u = o.next()) s = u.value, i.push_back(s);
                    f.i.changeOptions(i), i.delete(), f.A = void 0;
                  }
                  f.o = !1, t.g = 0;
              }
            });
          }), 0);
        });
      });
    }, s.overrideFile = function (t, e) {
      this.i ? this.i.overrideFile(t, e) : this.B[t] = e;
    }, s.clearOverriddenFiles = function () {
      this.B = {}, this.i && this.i.clearOverriddenFiles();
    }, s.send = function (t, e) {
      return t1(this, function n() {
        var r,
          i,
          o,
          u,
          s,
          f,
          c,
          l,
          h,
          g = this;
        return x(n, function (n) {
          switch (n.g) {
            case 1:
              if (!g.g.inputs) return n.return();
              return r = 1e3 * (null == e ? performance.now() : e), _(n, g.C, 2);
            case 2:
              return _(n, g.initialize(), 3);
            case 3:
              for (i = new g.h.PacketDataList(), u = (o = a(Object.keys(t))).next(); !u.done; u = o.next()) if (s = u.value, f = g.g.inputs[s]) {
                a: {
                  var $ = g,
                    p = t[s];
                  switch (f.type) {
                    case "video":
                      var v = $.m[f.stream];
                      if (v || (v = new t5($.h, $.D), $.m[f.stream] = v), 0 === ($ = v).l && ($.l = $.h.createTexture()), "undefined" != typeof HTMLVideoElement && p instanceof HTMLVideoElement) {
                        var d = p.videoWidth;
                        v = p.videoHeight;
                      } else "undefined" != typeof HTMLImageElement && p instanceof HTMLImageElement ? (d = p.naturalWidth, v = p.naturalHeight) : (d = p.width, v = p.height);
                      v = {
                        glName: $.l,
                        width: d,
                        height: v
                      }, (d = $.g).canvas.width = v.width, d.canvas.height = v.height, d.activeTexture(d.TEXTURE0), $.h.bindTexture2d($.l), d.texImage2D(d.TEXTURE_2D, 0, d.RGBA, d.RGBA, d.UNSIGNED_BYTE, p), $.h.bindTexture2d(0), $ = v;
                      break a;
                    case "detections":
                      for ((v = $.m[f.stream]) || (v = new tC($.h), $.m[f.stream] = v), ($ = v).data || ($.data = new $.g.DetectionListData()), $.data.reset(p.length), v = 0; v < p.length; ++v) {
                        d = p[v];
                        var y = $.data,
                          m = y.setBoundingBox,
                          b = v,
                          w = d.T,
                          x = new t8();
                        tv(x, 1, w.Z), tv(x, 2, w.$), tv(x, 3, w.height), tv(x, 4, w.width), tv(x, 5, w.rotation), tv(x, 6, w.X);
                        var A = w = new tt();
                        tr(A, 1, tg(x, 1)), tr(A, 2, tg(x, 2)), tr(A, 3, tg(x, 3)), tr(A, 4, tg(x, 4)), tr(A, 5, tg(x, 5));
                        var k = tg(x, 6);
                        if (null != k && null != k) {
                          J(A.g, 48);
                          var j = A.g,
                            F = k;
                          k = 0 > F;
                          var R = (F = Math.abs(F)) >>> 0;
                          for (F = Math.floor((F - R) / 4294967296), F >>>= 0, k && (F = ~F >>> 0, 4294967295 < (R = (~R >>> 0) + 1) && (R = 0, 4294967295 < ++F && (F = 0))), M = R, G = F, k = M, R = G; 0 < R || 127 < k;) j.push(127 & k | 128), k = (k >>> 7 | R << 25) >>> 0, R >>>= 7;
                          j.push(k);
                        }
                        if (tb(x, A), w = tn(w), m.call(y, b, w), d.O) for (y = 0; y < d.O.length; ++y) A = !!(x = d.O[y]).visibility, b = (m = $.data).addNormalizedLandmark, w = v, x = Object.assign(Object.assign({}, x), {
                          visibility: A ? x.visibility : 0
                        }), tv(A = new t6(), 1, x.x), tv(A, 2, x.y), tv(A, 3, x.z), x.visibility && tv(A, 4, x.visibility), tr(j = x = new tt(), 1, tg(A, 1)), tr(j, 2, tg(A, 2)), tr(j, 3, tg(A, 3)), tr(j, 4, tg(A, 4)), tr(j, 5, tg(A, 5)), tb(A, j), x = tn(x), b.call(m, w, x);
                        if (d.M) for (y = 0; y < d.M.length; ++y) {
                          if (b = (m = $.data).addClassification, w = v, x = d.M[y], tv(A = new t2(), 2, x.Y), x.index && tv(A, 1, x.index), x.label && tv(A, 3, x.label), x.displayName && tv(A, 4, x.displayName), j = x = new tt(), null != (R = tg(A, 1)) && null != R) {
                            if (J(j.g, 8), k = j.g, 0 <= R) J(k, R);else {
                              for (F = 0; 9 > F; F++) k.push(127 & R | 128), R >>= 7;
                              k.push(1);
                            }
                          }
                          tr(j, 2, tg(A, 2)), null != (k = tg(A, 3)) && (k = O(k), J(j.g, 26), J(j.g, k.length), te(j, j.g.end()), te(j, k)), null != (k = tg(A, 4)) && (k = O(k), J(j.g, 34), J(j.g, k.length), te(j, j.g.end()), te(j, k)), tb(A, j), x = tn(x), b.call(m, w, x);
                        }
                      }
                      $ = $.data;
                      break a;
                    default:
                      $ = {};
                  }
                }
                switch (c = $, l = f.stream, f.type) {
                  case "video":
                    i.pushTexture2d(Object.assign(Object.assign({}, c), {
                      stream: l,
                      timestamp: r
                    }));
                    break;
                  case "detections":
                    (h = c).stream = l, h.timestamp = r, i.pushDetectionList(h);
                    break;
                  default:
                    throw Error("Unknown input config type: '" + f.type + "'");
                }
              }
              return g.i.send(i), _(n, g.C, 4);
            case 4:
              i.delete(), n.g = 0;
          }
        });
      });
    }, s.onResults = function (t, e) {
      this.listeners[e || "$"] = t;
    }, j("Solution", tP), j("OptionType", {
      BOOL: 0,
      NUMBER: 1,
      aa: 2,
      0: "BOOL",
      1: "NUMBER",
      2: "STRING"
    }), (s = tI.prototype).close = function () {
      return this.g.close(), Promise.resolve();
    }, s.onResults = function (t) {
      this.g.onResults(t);
    }, s.initialize = function () {
      return t1(this, function t() {
        var e = this;
        return x(t, function (t) {
          return _(t, e.g.initialize(), 0);
        });
      });
    }, s.reset = function () {
      this.g.reset();
    }, s.send = function (t) {
      return t1(this, function e() {
        var n = this;
        return x(e, function (e) {
          return _(e, n.g.send(t), 0);
        });
      });
    }, s.setOptions = function (t) {
      this.g.setOptions(t);
    }, j("SelfieSegmentation", tI), j("VERSION", "0.1.1632777926");
  }).call(undefined);

  var Logger$1 = window.log;
  /**
   * This class is used to apply a video effect to the video stream.
   * It's compatible with Ant Media Server JavaScript SDK v2.5.2+
   *
   */
  var _virtualBackgroundImage = /*#__PURE__*/new WeakMap();
  var _noEffect = /*#__PURE__*/new WeakSet();
  class VideoEffect {
    /**
     * 
     * @param {WebRTCAdaptor} webRTCAdaptor 
     */
    constructor(webRTCAdaptor) {
      /**
       * This method is used to disable the virtual background and blur effects.
       */
      _classPrivateMethodInitSpec(this, _noEffect);
      _classPrivateFieldInitSpec(this, _virtualBackgroundImage, {
        writable: true,
        value: null
      });
      this.webRTCAdaptor = webRTCAdaptor;
      this.selfieSegmentation = null;
      this.effectCanvas = null;
      this.ctx = null;
      this.rawLocalVideo = document.createElement('video');
      this.deepAR = null;
      this.backgroundBlurRange = 3;
      this.edgeBlurRange = 4;
      this.effectName = VideoEffect.NO_EFFECT;
      this.startTime = 0;
      this.statTimerId = -1;
      this.renderedFrameCount = 0;
      this.lastRenderedFrameCount = 0;
      this.effectCanvasFPS = 0;
      this.videoCallbackPeriodMs = 0;
      this.initializeSelfieSegmentation();
      this.isInitialized = true;
    }

    /**
     * This method is used to initialize the video effect.
     * @param {MediaStream} stream - Original stream to be manipulated.
     * @returns {Promise<void>}
     */
    init(stream) {
      var _this = this;
      return _asyncToGenerator(function* () {
        yield _this.setRawLocalVideo(stream);
        var trackSettings = stream.getVideoTracks()[0].getSettings();
        _this.effectCanvasFPS = trackSettings.frameRate;
        _this.videoCallbackPeriodMs = 1000 / _this.effectCanvasFPS;
        _this.effectCanvas = _this.createEffectCanvas(trackSettings.width, trackSettings.height);
        _this.ctx = _this.effectCanvas.getContext("2d");
        if (_this.canvasStream) {
          _this.canvasStream.getTracks().forEach(track => track.stop());
          _this.canvasStream = null;
        }
        _this.canvasStream = _this.effectCanvas.captureStream(_this.effectCanvasFPS);
        return new Promise((resolve, reject) => {
          resolve(_this.canvasStream);
        });
      })();
    }

    /**
     * This method is used to set raw local video.
     * @param {MediaStream} stream
     * @returns {Promise<void>}
     */
    setRawLocalVideo(stream) {
      this.rawLocalVideo.srcObject = stream;
      this.rawLocalVideo.muted = true;
      this.rawLocalVideo.autoplay = true;
      return this.rawLocalVideo.play();
    }

    /**
     * This method is used to create the canvas element which is used to apply the video effect.
     * @param {number} height
     * @param {number} width 
     */
    createEffectCanvas(width, height) {
      var effectCanvas = document.createElement('canvas');
      effectCanvas.id = "effectCanvas";
      effectCanvas.width = width;
      effectCanvas.height = height;
      return effectCanvas;
    }

    /**
     * This method is used to initialize the selfie segmentation.
     */
    initializeSelfieSegmentation() {
      this.selfieSegmentation = new SelfieSegmentation({
        locateFile: file => {
          return VideoEffect.LOCATE_FILE_URL + "/" + file;
        }
      });
      this.selfieSegmentation.setOptions({
        selfieMode: false,
        // true: selfie mode, false: portrait mode
        modelSelection: 1 // 0: General Model, 1: Landscape Model - We use Landscape Model for better performance
      });

      this.selfieSegmentation.onResults(results => {
        this.onResults(results);
      });
    }
    /**
     * @param {HTMLElement} imageElement
     */
    set virtualBackgroundImage(imageElement) {
      _classPrivateFieldSet(this, _virtualBackgroundImage, imageElement);
    }
    startFpsCalculation() {
      this.statTimerId = setInterval(() => {
        var currentTime = new Date().getTime();
        var deltaTime = (currentTime - this.startTime) / 1000;
        this.startTime = currentTime;
        var fps = (this.renderedFrameCount - this.lastRenderedFrameCount) / deltaTime;
        this.renderedFrameCount = this.lastRenderedFrameCount;
        Logger$1.warn("Fps: " + fps + "fps");
      }, 1000);
    }
    stopFpsCalculation() {
      if (this.statTimerId !== -1) {
        clearInterval(this.statTimerId);
        this.statTimerId = -1;
      }
    }
    processFrame() {
      var _this2 = this;
      return _asyncToGenerator(function* () {
        yield _this2.selfieSegmentation.send({
          image: _this2.rawLocalVideo
        });

        //call if the effect name is not NO_EFFECT
        if (_this2.effectName !== VideoEffect.NO_EFFECT) {
          setTimeout(() => {
            _this2.processFrame();
          }, _this2.videoCallbackPeriodMs);
        }
      })();
    }

    /**
     * Enable effect
     * @param {string} effectName
     * @param {string} deepARApiKey
     * @param {*} deepARModel
     */
    enableEffect(effectName, deepARApiKey, deepARModel) {
      var _this3 = this;
      return _asyncToGenerator(function* () {
        if (!_this3.isInitialized) {
          Logger$1.error("VideoEffect is not initialized!");
          return;
        }
        switch (effectName) {
          case VideoEffect.DEEPAR:
          case VideoEffect.VIRTUAL_BACKGROUND:
          case VideoEffect.BLUR_BACKGROUND:
          case VideoEffect.NO_EFFECT:
            break;
          default:
            Logger$1.warn("Unknown effect name please use the constants VideoEffect.VIRTUAL_BACKGROUND,VideoEffect.BLUR_BACKGROUND or VideoEffect.NO_EFFECT ");
            return;
        }
        var currentEffectName = _this3.effectName;
        _this3.effectName = effectName;
        if (currentEffectName === VideoEffect.DEEPAR && effectName !== VideoEffect.DEEPAR) {
          _this3.deepAR.shutdown();
          _this3.deepAR = null;
        }
        if (effectName === VideoEffect.VIRTUAL_BACKGROUND || effectName === VideoEffect.BLUR_BACKGROUND) {
          //check old effect name. If it's no effect, start the process
          if (currentEffectName === VideoEffect.NO_EFFECT || currentEffectName === VideoEffect.DEEPAR) {
            if (VideoEffect.DEBUG) {
              _this3.startFpsCalculation();
            }
            //We cannot use the localStream of the webrtc adaptor because it's gets stopped when updateVideoTrack is called
            //get the video stream with current constraints and stop it when effects are disabled

            //audio:true makes the trick to play the video in the background as well otherwise it stops playing
            return navigator.mediaDevices.getUserMedia({
              video: _this3.webRTCAdaptor.mediaConstraints.video,
              audio: true
            }).then(localStream => {
              return _this3.init(localStream).then(processedStream => {
                return _this3.webRTCAdaptor.updateVideoTrack(processedStream, _this3.webRTCAdaptor.publishStreamId, null, true).then(() => {
                  setTimeout(() => {
                    _this3.processFrame();
                  }, _this3.videoCallbackPeriodMs);
                });
              }).catch(err => {
                //log and throw again to let the catch in the chain it
                Logger$1.error(err);
                throw err;
              });
            });
          } else {
            return new Promise((resolve, reject) => {
              resolve();
            });
          }
        } else if (effectName === VideoEffect.DEEPAR) {
          if (deepARApiKey === undefined || deepARApiKey === null || deepARApiKey === "" || deepARModel === undefined || deepARModel === null || deepARModel === "") {
            Logger$1.error("DeepAR API key or DeepAR Model is not set!");
            return;
          }
          if (currentEffectName === VideoEffect.DEEPAR) {
            _this3.deepAR.switchEffect(0, 'slot', VideoEffect.DEEP_AR_EFFECTS_URL + deepARModel + VideoEffect.DEEP_AR_EXTENSION);
            return;
          } else if (currentEffectName === VideoEffect.BLUR_BACKGROUND || currentEffectName === VideoEffect.VIRTUAL_BACKGROUND) {
            //Stop timer
            _this3.stopFpsCalculation();
            yield _classPrivateMethodGet(_this3, _noEffect, _noEffect2).call(_this3);
          }
          var canvas = _this3.createEffectCanvas(500, 500);
          var deepAR = new DeepAR({
            licenseKey: deepARApiKey,
            canvas: canvas,
            deeparWasmPath: VideoEffect.DEEP_AR_FOLDER_ROOT_URL + '/wasm/deepar.wasm',
            callbacks: {
              onInitialize: function onInitialize() {
                deepAR.startVideo(true);
              }
            }
          });
          _this3.deepAR = deepAR;
          _this3.deepAR.callbacks.onVideoStarted = () => {
            _this3.canvasStream = canvas.captureStream(30);
            _this3.webRTCAdaptor.updateVideoTrack(_this3.canvasStream, _this3.webRTCAdaptor.publishStreamId, null, true);
            _this3.deepAR.switchEffect(0, 'slot', VideoEffect.DEEP_AR_EFFECTS_URL + deepARModel + VideoEffect.DEEP_AR_EXTENSION);
          };
          _this3.deepAR.downloadFaceTrackingModel(VideoEffect.DEEP_AR_FOLDER_ROOT_URL + "/models/face/models-68-extreme.bin");
          _this3.deepAR.setVideoElement(_this3.rawLocalVideo, true);
        } else {
          if (currentEffectName === VideoEffect.DEEPAR) {
            var localStream = yield navigator.mediaDevices.getUserMedia({
              video: _this3.webRTCAdaptor.mediaConstraints.video,
              audio: true
            });
            yield _this3.setRawLocalVideo(localStream);
          }
          return new Promise((resolve, reject) => {
            //Stop timer
            _this3.stopFpsCalculation();
            _classPrivateMethodGet(_this3, _noEffect, _noEffect2).call(_this3);
            resolve();
          });
        }
      })();
    }
    /**
     * This method is used to draw the segmentation mask.
     * @param {*} segmentation
     */
    drawSegmentationMask(segmentation) {
      this.ctx.drawImage(segmentation, 0, 0, this.effectCanvas.width, this.effectCanvas.height);
    }

    /**
     * This method is called by mediapipe when the segmentation mask is ready.
     * @param {*} results
     */
    onResults(results) {
      this.renderedFrameCount++;
      if (this.effectName == VideoEffect.BLUR_BACKGROUND) {
        this.drawBlurBackground(results.image, results.segmentationMask, this.backgroundBlurRange);
      } else if (this.effectName == VideoEffect.VIRTUAL_BACKGROUND) {
        this.drawVirtualBackground(results.image, results.segmentationMask, _classPrivateFieldGet(this, _virtualBackgroundImage));
      } else {
        this.drawImageDirectly(results.image);
      }
    }

    /**
     * This method is used to draw the raw frame directly to the canvas.
     * @param {*} image
     */
    drawImageDirectly(image) {
      this.ctx.save();
      this.ctx.globalCompositeOperation = "source-over";
      this.ctx.filter = "none";
      this.ctx.drawImage(image, 0, 0, image.width, image.height);
      this.ctx.restore();
    }

    /**
     * This method is used to draw the frame with virtual background effect to the canvas.
     * @param {*} image
     * @param {*} segmentation
     * @param {*} virtualBackgroundImage
     */
    drawVirtualBackground(image, segmentation, virtualBackgroundImage) {
      this.ctx.save();
      this.ctx.filter = "none";
      this.ctx.clearRect(0, 0, this.effectCanvas.width, this.effectCanvas.height);
      this.ctx.drawImage(segmentation, 0, 0, this.effectCanvas.width, this.effectCanvas.height);
      this.ctx.globalCompositeOperation = 'source-out';
      this.ctx.drawImage(virtualBackgroundImage, 0, 0, virtualBackgroundImage.naturalWidth, virtualBackgroundImage.naturalHeight, 0, 0, this.effectCanvas.width, this.effectCanvas.height);
      this.ctx.globalCompositeOperation = 'destination-atop';
      this.ctx.drawImage(image, 0, 0, this.effectCanvas.width, this.effectCanvas.height);
      this.ctx.restore();
    }

    /**
     * This method is used to draw frame with background blur effect to the canvas.
     * @param {*} image
     * @param {*} segmentation
     * @param {*} blurAmount
     */
    drawBlurBackground(image, segmentation, blurAmount) {
      this.ctx.clearRect(0, 0, this.effectCanvas.width, this.effectCanvas.height);
      this.ctx.globalCompositeOperation = "copy";
      this.ctx.filter = "none";
      this.ctx.filter = "blur(" + this.edgeBlurRange + "px)";
      this.drawSegmentationMask(segmentation);
      this.ctx.globalCompositeOperation = "source-in";
      this.ctx.filter = "none";
      this.ctx.drawImage(image, 0, 0, this.effectCanvas.width, this.effectCanvas.height);
      this.ctx.globalCompositeOperation = "destination-over";
      this.ctx.filter = "blur(" + blurAmount + "px)";
      this.ctx.drawImage(image, 0, 0, this.effectCanvas.width, this.effectCanvas.height);
      this.ctx.restore();
    }
  }
  function _noEffect2() {
    this.rawLocalVideo.pause();
    if (this.canvasStream != null) {
      this.canvasStream.getVideoTracks().forEach(track => track.stop());
    }
    return this.webRTCAdaptor.switchVideoCameraCapture(this.webRTCAdaptor.publishStreamId);
  }
  _defineProperty(VideoEffect, "DEEPAR", "deepar");
  _defineProperty(VideoEffect, "VIRTUAL_BACKGROUND", "virtual-background");
  _defineProperty(VideoEffect, "BLUR_BACKGROUND", "blur-background");
  _defineProperty(VideoEffect, "NO_EFFECT", "no-effect");
  _defineProperty(VideoEffect, "deepARModelList", ['flower_face', 'Ping_Pong']);
  /**
   * @type {boolean}
   */
  _defineProperty(VideoEffect, "DEBUG", false);
  /**
   * LOCATE_FILE_URL is optional, it's to give locate url of selfie segmentation
   * If you would like to use CDN,
   * Give "https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation/"
   * or give local file relative path "./js/external/selfie-segmentation" according to your file
   */
  //static LOCATE_FILE_URL = "./js/external/selfie-segmentation";
  _defineProperty(VideoEffect, "LOCATE_FILE_URL", "https://cdn.jsdelivr.net/npm/@mediapipe/selfie_segmentation");
  _defineProperty(VideoEffect, "DEEP_AR_FOLDER_ROOT_URL", "https://cdn.jsdelivr.net/npm/deepar@4.0.3");
  _defineProperty(VideoEffect, "DEEP_AR_EFFECTS_URL", "../js/external/deepar-effects/");
  _defineProperty(VideoEffect, "DEEP_AR_EXTENSION", ".deepar");
  WebRTCAdaptor.register(webrtcAdaptorInstance => {
    var videoEffect = new VideoEffect(webrtcAdaptorInstance);
    Object.defineProperty(webrtcAdaptorInstance, "enableEffect", {
      value: function value(effectName, deepARApiKey, deepARModel) {
        return videoEffect.enableEffect(effectName, deepARApiKey, deepARModel);
      }
    });
    Object.defineProperty(webrtcAdaptorInstance, "setBackgroundImage", {
      value: function value(imageElement) {
        videoEffect.virtualBackgroundImage = imageElement;
      }
    });
  });

  var Logger = window.log;
  var STATIC_VIDEO_HTML = "<video id='video-player' class='video-js vjs-default-skin vjs-big-play-centered' controls playsinline></video>";
  class EmbeddedPlayer {
    constructor(window, containerElement, placeHolderElement) {
      /**
      *  "playOrder": the order which technologies is used in playing. Optional. Default value is "webrtc,hls".
      *	possible values are "hls,webrtc","webrtc","hls","vod","dash"
      *   It will be taken from url parameter "playOrder".
      */
      _defineProperty(this, "playOrder", void 0);
      /**
       * currentPlayType: current play type in playOrder
       */
      _defineProperty(this, "currentPlayType", void 0);
      /**
       * "is360": if true, player will be 360 degree player. Optional. Default value is false.
       * It will be taken from url parameter "is360".
       */
      _defineProperty(this, "is360", false);
      /**
       * "streamId": stream id. Mandatory. If it is not set, it will be taken from url parameter "id".
       * It will be taken from url parameter "id".
       */
      _defineProperty(this, "streamId", void 0);
      /**
       * "playType": play type. Optional.  It's used for vod. Default value is "mp4,webm".
       * It can be "mp4,webm","webm,mp4","mp4","webm","mov" and it's used for vod.
       * It will be taken from url parameter "playType".
       */
      _defineProperty(this, "playType", void 0);
      /**
       * "token": token. It's required when stream security for playback is enabled .
       * It will be taken from url parameter "token".
       */
      _defineProperty(this, "token", void 0);
      /**
       * autoplay: if true, player will be started automatically. Optional. Default value is true.
       * autoplay is false by default for mobile devices because of mobile browser's autoplay policy.
       * It will be taken from url parameter "autoplay".
       */
      _defineProperty(this, "autoPlay", true);
      /**
       * mute: if true, player will be started muted. Optional. Default value is true.
       * default value is true because of browser's autoplay policy.
       * It will be taken from url parameter "mute".
       */
      _defineProperty(this, "mute", true);
      /**
       * targetLatency: target latency in seconds. Optional. Default value is 3.
       * It will be taken from url parameter "targetLatency".
       * It's used for dash(cmaf) playback.
       */
      _defineProperty(this, "targetLatency", 3);
      /**
       * subscriberId: subscriber id. Optional. It will be taken from url parameter "subscriberId".
       */
      _defineProperty(this, "subscriberId", void 0);
      /**
       * subscriberCode: subscriber code. Optional. It will be taken from url parameter "subscriberCode".
       */
      _defineProperty(this, "subscriberCode", void 0);
      /**
       * window: window object
       */
      _defineProperty(this, "window", void 0);
      /**
       * video player container element
       */
      _defineProperty(this, "containerElement", void 0);
      /**
       * player placeholder element
       */
      _defineProperty(this, "placeHolderElement", void 0);
      /**
       * videojs player
       */
      _defineProperty(this, "videojsPlayer", void 0);
      /**
       * dash player
       */
      _defineProperty(this, "dashPlayer", void 0);
      /**
       * Ice servers for webrtc
       */
      _defineProperty(this, "iceServers", void 0);
      /**
       * ice connection state
       */
      _defineProperty(this, "iceConnected", void 0);
      /**
       * flag to check if error callback is called
       */
      _defineProperty(this, "errorCalled", void 0);
      /**
       * scene for 360 degree player
       */
      _defineProperty(this, "aScene", void 0);
      /**
       * player listener
       */
      _defineProperty(this, "playerListener", void 0);
      /**
       * webRTCDataListener
       */
      _defineProperty(this, "webRTCDataListener", void 0);
      /**
       * Field to keep if tryNextMethod is already called
       */
      _defineProperty(this, "tryNextTechTimer", void 0);
      EmbeddedPlayer.DEFAULT_PLAY_ORDER = ["webrtc", "hls"];
      EmbeddedPlayer.DEFAULT_PLAY_TYPE = ["mp4", "webm"];
      EmbeddedPlayer.HLS_EXTENSION = "m3u8";
      EmbeddedPlayer.WEBRTC_EXTENSION = "webrtc";
      EmbeddedPlayer.DASH_EXTENSION = "mpd";

      /**
      * streamsFolder: streams folder. Optional. Default value is "streams"
      */
      EmbeddedPlayer.STREAMS_FOLDER = "streams";
      EmbeddedPlayer.VIDEO_HTML = STATIC_VIDEO_HTML;
      EmbeddedPlayer.VIDEO_PLAYER_ID = "video-player";
      this.dom = window.document;
      this.window = window;
      var localStreamId = getUrlParameter("id", this.window.location.search);
      this.containerElement = containerElement;
      this.placeHolderElement = placeHolderElement;
      this.errorCalled = false;
      this.iceConnected = false;
      this.tryNextTechTimer = -1;
      this.videojsPlayer = null;
      this.iceServers = '[ { "urls": "stun:stun1.l.google.com:19302" } ]';
      if (localStreamId == null) {
        //check name variable for compatibility with older versions

        localStreamId = getUrlParameter("name", this.window.location.search);
        if (localStreamId == null) {
          Logger.warn("Please use id parameter instead of name parameter.");
        }
      }
      if (localStreamId == null) {
        var message = "Stream id is not set.Please add your stream id to the url as a query parameter such as ?id={STREAM_ID} to the url";
        Logger.error(message);
        //TODO: we may need to show this message on directly page
        alert(message);
        throw new Error(message);
      }
      this.streamId = localStreamId;
      var localIs360 = getUrlParameter("is360", this.window.location.search);
      if (localIs360 != null) {
        this.is360 = localIs360.toLocaleLowerCase() == "true";
      }
      var localPlayType = getUrlParameter("playType", this.window.location.search);
      if (localPlayType != null) {
        this.playType = localPlayType.split(',');
      } else {
        this.playType = EmbeddedPlayer.DEFAULT_PLAY_TYPE;
      }
      this.token = getUrlParameter("token", this.window.location.search);
      if (this.token === undefined) {
        this.token = null;
      }
      var localAutoPlay = getUrlParameter("autoplay", this.window.location.search);
      if (localAutoPlay != null) {
        this.autoPlay = localAutoPlay.toLocaleLowerCase() == "true";
      }
      var localMute = getUrlParameter("mute", this.window.location.search);
      if (localMute != null) {
        this.mute = localMute.toLocaleLowerCase() == "true";
      }
      var localTargetLatency = getUrlParameter("targetLatency", this.window.location.search);
      if (localTargetLatency != null) {
        var latencyInNumber = Number(localTargetLatency);
        if (!isNaN(latencyInNumber)) {
          this.targetLatency = latencyInNumber;
        } else {
          Logger.warn("targetLatency parameter is not a number. It will be ignored.");
        }
      }
      this.subscriberId = getUrlParameter("subscriberId", this.window.location.search);
      if (this.subscriberId === undefined) {
        this.subscriberId = null;
      }
      this.subscriberCode = getUrlParameter("subscriberCode", this.window.location.search);
      if (this.subscriberCode == null) {
        this.subscriberCode = null;
      }
      var playOrderParameter = getUrlParameter("playOrder", this.window.location.search);
      if (playOrderParameter != null) {
        this.playOrder = playOrderParameter.split(',');
      } else {
        this.playOrder = EmbeddedPlayer.DEFAULT_PLAY_ORDER;
      }
      this.loadScripts();
      this.setPlayerVisible(false);
    }

    /**
     * load scripts dynamically
     */
    loadScripts() {
      if (this.playOrder.includes("hls") || this.playOrder.includes("vod") || this.playOrder.includes("webrtc")) {
        //it means we're going to use videojs
        //load videojs css
        var videoJsExternalCss = this.dom.createElement("link");
        videoJsExternalCss.setAttribute("rel", "stylesheet");
        videoJsExternalCss.setAttribute("type", "text/css");
        videoJsExternalCss.setAttribute("href", "css/external/video-js.css");
        this.dom.head.appendChild(videoJsExternalCss);

        //include videojs -> js
        var videoJsExternalJs = this.dom.createElement("script");
        videoJsExternalJs.type = "text/javascript";
        videoJsExternalJs.src = "js/external/video.js";
        videoJsExternalJs.async = false;
        this.dom.head.appendChild(videoJsExternalJs);

        // These files should call after videojs file loaded completely
        videoJsExternalJs.onload = () => {
          var videoJsQualityLevel = this.dom.createElement("script");
          videoJsQualityLevel.type = "text/javascript";
          videoJsQualityLevel.src = "js/external/videojs-contrib-quality-levels.min.js";
          this.dom.head.appendChild(videoJsQualityLevel);
          var videoJsQualitySelector = this.dom.createElement("script");
          videoJsQualitySelector.type = "text/javascript";
          videoJsQualitySelector.src = "js/external/videojs-hls-quality-selector.min.js";
          this.dom.head.appendChild(videoJsQualitySelector);
        };
      }
      if (this.playOrder.includes("webrtc")) {
        var webrtcVideoJsExternalCss = this.dom.createElement("link");
        webrtcVideoJsExternalCss.setAttribute("rel", "stylesheet");
        webrtcVideoJsExternalCss.setAttribute("type", "text/css");
        webrtcVideoJsExternalCss.setAttribute("href", "css/videojs-webrtc-plugin.css");
        this.dom.head.appendChild(webrtcVideoJsExternalCss);
        var webrtcVideoJsExternalJs = this.dom.createElement("script");
        webrtcVideoJsExternalJs.type = "text/javascript";
        webrtcVideoJsExternalJs.src = "js/videojs-webrtc-plugin.js";
        webrtcVideoJsExternalJs.async = false;
        this.dom.head.appendChild(webrtcVideoJsExternalJs);
      }
      if (this.playOrder.includes("dash")) {
        var js = this.dom.createElement("script");
        js.type = "text/javascript";
        js.src = "js/external/dash.all.min.js";
        this.dom.head.appendChild(js);
      }
      if (this.is360) {
        var aframeJS = this.dom.createElement("script");
        aframeJS.type = "text/javascript";
        aframeJS.src = "js/external/aframe.min.js";
        this.dom.head.appendChild(aframeJS);
      }
    }

    /**
     * enable 360 player
     */
    enable360Player() {
      this.aScene = this.dom.createElement("a-scene");
      var elementId = this.dom.getElementsByTagName("video")[0].id;
      this.aScene.innerHTML = "<a-videosphere src=\"#" + elementId + "\" rotation=\"0 180 0\" style=\"background-color: antiquewhite\"></a-videosphere>";
      this.dom.body.appendChild(this.aScene);
    }

    /**
     * set player visibility
     * @param {boolean} visible
     */
    setPlayerVisible(visible) {
      this.containerElement.style.display = visible ? "block" : "none";
      this.placeHolderElement.style.display = visible ? "none" : "block";
      if (this.is360) {
        if (visible) {
          this.enable360Player();
        } else if (this.aScene != null) {
          var elements = this.dom.getElementsByTagName("a-scene");
          while (elements.length > 0) {
            this.dom.body.removeChild(elements[0]);
            elements = this.dom.getElementsByTagName("a-scene");
          }
          this.aScene = null;
        }
      }
    }
    handleWebRTCInfoMessages(infos) {
      if (infos["info"] == "ice_connection_state_changed") {
        Logger.debug("ice connection state changed to " + infos["obj"].state);
        if (infos["obj"].state == "completed" || infos["obj"].state == "connected") {
          this.iceConnected = true;
        } else if (infos["obj"].state == "failed" || infos["obj"].state == "disconnected" || infos["obj"].state == "closed") {
          //
          Logger.debug("Ice connection is not connected. tryNextTech to replay");
          this.tryNextTech();
        }
      } else if (infos["info"] == "closed") {
        //this means websocket is closed and it stops the playback - tryNextTech
        Logger.debug("Websocket is closed. tryNextTech to replay");
        this.tryNextTech();
      }
    }

    /**
     * Play the stream via videojs
     * @param {*} streamUrl
     * @param {*} extension
     * @returns
     */
    playWithVideoJS(streamUrl, extension) {
      var type;
      if (extension == "mp4") {
        type = "video/mp4";
      } else if (extension == "webm") {
        type = "video/webm";
      } else if (extension == "mov") {
        type = "video/mp4";
        alert("Browsers do not support to play mov format");
      } else if (extension == "avi") {
        type = "video/mp4";
        alert("Browsers do not support to play avi format");
      } else if (extension == "m3u8") {
        type = "application/x-mpegURL";
      } else if (extension == "mpd") {
        type = "application/dash+xml";
      } else if (extension == "webrtc") {
        type = "video/webrtc";
      } else {
        Logger.warn("Unknown extension: " + extension);
        return;
      }
      var preview = this.streamId;
      if (this.streamId.endsWith("_adaptive")) {
        preview = streamId.substring(0, streamId.indexOf("_adaptive"));
      }

      //same videojs is being use for hls, vod and webrtc streams
      this.videojsPlayer = videojs(EmbeddedPlayer.VIDEO_PLAYER_ID, {
        poster: "previews/" + preview + ".png",
        liveui: extension == "m3u8" ? true : false,
        liveTracker: {
          trackingThreshold: 0
        },
        html5: {
          vhs: {
            limitRenditionByPlayerDimensions: false
          }
        },
        controls: true,
        class: 'video-js vjs-default-skin vjs-big-play-centered',
        muted: this.mute,
        preload: "auto",
        autoplay: this.autoPlay
      });
      this.videojsPlayer.on('error', e => {
        Logger.warn("There is an error in playback: " + e);
        // We need to add this kind of check. If we don't add this kind of checkpoint, it will create an infinite loop
        if (!this.errorCalled) {
          this.errorCalled = true;
          setTimeout(() => {
            this.tryNextTech();
            this.errorCalled = false;
          }, 2500);
        }
      });

      //webrtc specific events
      if (extension == "webrtc") {
        this.videojsPlayer.on('webrtc-info', (event, infos) => {
          //Logger.warn("info callback: " + JSON.stringify(infos));
          this.handleWebRTCInfoMessages(infos);
        });
        this.videojsPlayer.on('webrtc-error', (event, errors) => {
          //some of the possible errors, NotFoundError, SecurityError,PermissionDeniedError
          Logger.warn("error callback: " + JSON.stringify(errors));
          if (errors["error"] == "no_stream_exist" || errors["error"] == "WebSocketNotConnected" || errors["error"] == "not_initialized_yet" || errors["error"] == "data_store_not_available" || errors["error"] == "highResourceUsage" || errors["error"] == "unauthorized_access" || errors["error"] == "user_blocked") {
            //handle high resource usage and not authroized errors && websocket disconnected
            //Even if webrtc adaptor has auto reconnect scenario, we dispose the videojs immediately in tryNextTech
            // so that reconnect scenario is managed here

            this.tryNextTech();
          } else if (errors["error"] == "notSetRemoteDescription") {
            /*
            * If getting codec incompatible or remote description error, it will redirect HLS player.
            */
            Logger.warn("notSetRemoteDescription error. Redirecting to HLS player.");
            this.playIfExists("hls");
          }
        });
        this.videojsPlayer.on("webrtc-data-received", (event, obj) => {
          Logger.warn("webrtc-data-received: " + JSON.stringify(obj));
          if (this.webRTCDataListener != null) {
            this.webRTCDataListener(obj);
          }
        });
      }

      //hls specific calls
      if (extension == "m3u8") {
        videojs.Vhs.xhr.beforeRequest = options => {
          var securityParams = this.getSecurityQueryParams();
          if (!options.uri.includes(securityParams)) {
            if (!options.uri.endsWith("?")) {
              options.uri = options.uri + "?";
            }
            options.uri += securityParams;
          }
          Logger.debug("hls request: " + options.uri);
          return options;
        };
        this.videojsPlayer.ready(() => {
          // If it's already added to player, no need to add again
          if (typeof this.videojsPlayer.hlsQualitySelector === "function") {
            this.videojsPlayer.hlsQualitySelector({
              displayCurrentQuality: true
            });
          }

          // If there is no adaptive option in m3u8 no need to show quality selector
          var qualityLevels = this.videojsPlayer.qualityLevels();
          qualityLevels.on('addqualitylevel', function (event) {
            var qualityLevel = event.qualityLevel;
            if (qualityLevel.height) {
              qualityLevel.enabled = true;
            } else {
              qualityLevels.removeQualityLevel(qualityLevel);
              qualityLevel.enabled = false;
            }
          });
        });
      }

      //videojs is being used to play mp4, webm, m3u8 and webrtc
      //make the videoJS visible when ready is called except for webrtc
      //webrtc fires ready event all cases so we use "play" event to make the player visible

      //this setting is critical to play in mobile
      if (extension == "mp4" || extension == "webm" || extension == "m3u8") {
        this.makeVideoJSVisibleWhenReady();
      }
      this.videojsPlayer.on('ended', () => {
        //reinit to play after it ends
        Logger.warn("stream is ended");
        this.setPlayerVisible(false);
        //for webrtc, this event can be called by two reasons
        //1. ice connection is not established, it means that there is a networking issug
        //2. stream is ended
        if (this.currentPlayType != "vod") {
          //if it's vod, it means that stream is ended and no need to replay

          if (this.iceConnected) {
            //if iceConnected is true, it means that stream is really ended for webrtc

            //initialize to play again if the publishing starts again
            this.playIfExists(this.playOrder[0]);
          } else if (this.currentPlayType == "hls") {
            //if it's hls, it means that stream is ended

            this.setPlayerVisible(false);
            if (this.playOrder[0] = "hls") {
              //do not play again if it's hls because it play last seconds again, let the server clear it
              setTimeout(() => {
                this.playIfExists(this.playOrder[0]);
              }, 10000);
            } else {
              this.playIfExists(this.playOrder[0]);
            }
            //TODO: what if the stream is hls vod then it always re-play
          } else {
            //if iceConnected is false, it means that there is a networking issue for webrtc
            this.tryNextTech();
          }
        }
        if (this.playerListener != null) {
          this.playerListener("ended");
        }
      });

      //webrtc plugin sends play event. On the other hand, webrtc plugin sends ready event for every scenario.
      //so no need to trust ready event for webrt play
      this.videojsPlayer.on("play", () => {
        this.setPlayerVisible(true);
        if (this.playerListener != null) {
          this.playerListener("play");
        }
      });
      this.iceConnected = false;
      this.videojsPlayer.src({
        src: streamUrl,
        type: type,
        withCredentials: true,
        iceServers: this.iceServers,
        reconnect: false //webrtc adaptor has auto reconnect scenario, just disable it, we manage it here
      });

      if (this.autoPlay) {
        this.videojsPlayer.play().catch(e => {
          Logger.warn("Problem in playback. The error is " + e);
        });
      }
    }
    makeVideoJSVisibleWhenReady() {
      this.videojsPlayer.ready(() => {
        this.setPlayerVisible(true);
      });
    }

    /**
     * check if stream exists via http
     * @param {*} streamsfolder
     * @param {*} streamId
     * @param {*} extension
     * @returns
     */
    checkStreamExistsViaHttp(streamsfolder, streamId, extension) {
      var streamPath = "";
      if (!streamId.startsWith(streamsfolder)) {
        streamPath += streamsfolder + "/";
      }
      streamPath += streamId;
      if (extension != null && extension != "") {
        //if there is extension, add it and try if _adaptive exists
        streamPath += "_adaptive" + "." + extension;
      }
      streamPath = this.addSecurityParams(streamPath);
      return fetch(streamPath, {
        method: 'HEAD'
      }).then(response => {
        if (response.status == 200) {
          // adaptive m3u8 & mpd exists,play it
          return new Promise(function (resolve, reject) {
            resolve(streamPath);
          });
        } else {
          //adaptive not exists, try mpd or m3u8 exists.
          streamPath = streamsfolder + "/" + streamId + "." + extension;
          streamPath = this.addSecurityParams(streamPath);
          return fetch(streamPath, {
            method: 'HEAD'
          }).then(response => {
            if (response.status == 200) {
              return new Promise(function (resolve, reject) {
                resolve(streamPath);
              });
            } else {
              Logger.warn("No stream found");
              return new Promise(function (resolve, reject) {
                reject("resource_is_not_available");
              });
            }
          });
        }
      });
    }
    addSecurityParams(streamPath) {
      var securityParams = this.getSecurityQueryParams();
      if (securityParams != null && securityParams != "") {
        streamPath += "?" + securityParams;
      }
      return streamPath;
    }

    /**
     * try next tech if current tech is not working
     */
    tryNextTech() {
      if (this.tryNextTechTimer == -1) {
        this.destroyDashPlayer();
        this.destroyVideoJSPlayer();
        this.setPlayerVisible(false);
        var index = this.playOrder.indexOf(this.currentPlayType);
        if (index == -1 || index == this.playOrder.length - 1) {
          index = 0;
        } else {
          index++;
        }
        this.tryNextTechTimer = setTimeout(() => {
          this.tryNextTechTimer = -1;
          this.playIfExists(this.playOrder[index]);
        }, 3000);
      } else {
        Logger.debug("tryNextTech is already scheduled no need to schedule again");
      }
    }

    /**
     * play stream throgugh dash player
     * @param {string"} streamUrl
     */
    playViaDash(streamUrl) {
      this.destroyDashPlayer();
      this.dashPlayer = dashjs.MediaPlayer().create();
      this.dashPlayer.extend("RequestModifier", () => {
        return {
          modifyRequestHeader: function modifyRequestHeader(xhr, _ref) {
            return xhr;
          },
          modifyRequestURL: url => {
            var modifiedUrl = "";
            var securityParams = this.getSecurityQueryParams();
            if (!url.includes(securityParams)) {
              if (!url.endsWith("?")) {
                url += "?";
              }
              modifiedUrl = url + securityParams;
              Logger.warn(modifiedUrl);
              return modifiedUrl;
            }
            return url;
          },
          modifyRequest(request) {}
        };
      });
      this.dashPlayer.updateSettings({
        streaming: {
          delay: {
            liveDelay: this.targetLatency
          },
          liveCatchup: {
            maxDrift: 0.05,
            playbackRate: 0.5,
            latencyThreshold: 60
          }
        }
      });
      this.dashPlayer.initialize(this.containerElement.firstChild, streamUrl, this.autoPlay);
      this.dashPlayer.setMute(this.mute);
      this.dashLatencyTimer = setInterval(() => {
        Logger.warn("live latency: " + this.dashPlayer.getCurrentLiveLatency());
      }, 2000);
      this.makeDashPlayerVisibleWhenInitialized();
      this.dashPlayer.on(dashjs.MediaPlayer.events.PLAYBACK_PLAYING, event => {
        Logger.warn("playback started");
        this.setPlayerVisible(true);
        if (this.playerListener != null) {
          this.playerListener("play");
        }
      });
      this.dashPlayer.on(dashjs.MediaPlayer.events.PLAYBACK_ENDED, () => {
        Logger.warn("playback ended");
        this.destroyDashPlayer();
        this.setPlayerVisible(false);
        //streaming can be started again so try to play again with preferred tech
        if (this.playOrder[0] = "dash") {
          //do not play again if it's dash because it play last seconds again, let the server clear it
          setTimeout(() => {
            this.playIfExists(this.playOrder[0]);
          }, 10000);
        } else {
          this.playIfExists(this.playOrder[0]);
        }
        if (this.playerListener != null) {
          this.playerListener("ended");
        }
      });
      this.dashPlayer.on(dashjs.MediaPlayer.events.PLAYBACK_ERROR, event => {
        this.tryNextTech();
      });
      this.dashPlayer.on(dashjs.MediaPlayer.events.ERROR, event => {
        this.tryNextTech();
      });
    }
    makeDashPlayerVisibleWhenInitialized() {
      this.dashPlayer.on(dashjs.MediaPlayer.events.STREAM_INITIALIZED, event => {
        Logger.warn("Stream initialized");
        //make the player visible in mobile devices
        this.setPlayerVisible(true);
      });
    }

    /**
     * destroy the dash player
     */
    destroyDashPlayer() {
      if (this.dashPlayer) {
        this.dashPlayer.destroy();
        this.dashPlayer = null;
        clearInterval(this.dashLatencyTimer);
      }
    }

    /**
     * destroy the videojs player
     */
    destroyVideoJSPlayer() {
      if (this.videojsPlayer) {
        this.videojsPlayer.dispose();
        this.videojsPlayer = null;
      }
    }

    /**
     * play the stream with the given tech
     * @param {string} tech
     */
    playIfExists(tech) {
      var _this = this;
      return _asyncToGenerator(function* () {
        _this.currentPlayType = tech;
        _this.destroyVideoJSPlayer();
        _this.destroyDashPlayer();
        _this.setPlayerVisible(false);
        _this.containerElement.innerHTML = EmbeddedPlayer.VIDEO_HTML;
        Logger.warn("Try to play the stream " + _this.streamId + " with " + _this.currentPlayType);
        switch (_this.currentPlayType) {
          case "hls":
            //TODO: Test case for hls
            //1. Play stream with adaptive m3u8 for live and VoD
            //2. Play stream with m3u8 for live and VoD
            //3. if files are not available check nextTech is being called
            return _this.checkStreamExistsViaHttp(EmbeddedPlayer.STREAMS_FOLDER, _this.streamId, EmbeddedPlayer.HLS_EXTENSION).then(streamPath => {
              _this.playWithVideoJS(streamPath, EmbeddedPlayer.HLS_EXTENSION);
              Logger.warn("incoming stream path: " + streamPath);
            }).catch(error => {
              Logger.warn("HLS stream resource not available for stream:" + _this.streamId + " error is " + error + ". Try next play tech");
              _this.tryNextTech();
            });
          case "dash":
            return _this.checkStreamExistsViaHttp(EmbeddedPlayer.STREAMS_FOLDER, _this.streamId + "/" + _this.streamId, EmbeddedPlayer.DASH_EXTENSION).then(streamPath => {
              _this.playViaDash(streamPath);
            }).catch(error => {
              Logger.warn("DASH stream resource not available for stream:" + _this.streamId + " error is " + error + ". Try next play tech");
              _this.tryNextTech();
            });
          case "webrtc":
            var appName = _this.window.location.pathname.substring(0, _this.window.location.pathname.lastIndexOf("/") + 1);
            var path = _this.window.location.hostname + ":" + _this.window.location.port + appName + _this.streamId + ".webrtc";
            var websocketURL = "ws://" + path;
            if (location.protocol.startsWith("https")) {
              websocketURL = "wss://" + path;
            }
            return _this.playWithVideoJS(_this.addSecurityParams(websocketURL), EmbeddedPlayer.WEBRTC_EXTENSION);
          case "vod":
            //TODO: Test case for vod
            //1. Play stream with mp4 for VoD
            //2. Play stream with webm for VoD
            //3. Play stream with playOrder type

            var lastIndexOfDot = _this.streamId.lastIndexOf(".");
            var extension;
            if (lastIndexOfDot != -1) {
              //if there is a dot in the streamId, it means that this is extension, use it. make the extension empty
              _this.playType[0] = "";
              extension = _this.streamId.substring(lastIndexOfDot + 1);
            } else {
              //we need to give extension to playWithVideoJS
              extension = _this.playType[0];
            }
            return _this.checkStreamExistsViaHttp(EmbeddedPlayer.STREAMS_FOLDER, _this.streamId, _this.playType[0]).then(streamPath => {
              //we need to give extension to playWithVideoJS
              _this.playWithVideoJS(streamPath, extension);
            }).catch(error => {
              Logger.warn("VOD stream resource not available for stream:" + _this.streamId + " and play type " + _this.playType[0] + ". Error is " + error);
              if (_this.playType.length > 1) {
                Logger.warn("Try next play type which is " + _this.playType[1] + ".");
                _this.checkStreamExistsViaHttp(EmbeddedPlayer.STREAMS_FOLDER, _this.streamId, _this.playType[1]).then(streamPath => {
                  _this.playWithVideoJS(streamPath, _this.playType[1]);
                }).catch(error => {
                  Logger.warn("VOD stream resource not available for stream:" + _this.streamId + " and play type error is " + error);
                });
              }
            });
        }
      })();
    }

    /**
     *
     * @returns {String} query string for security
     */
    getSecurityQueryParams() {
      var queryString = "";
      if (this.token != null) {
        queryString += "&token=" + this.token;
      }
      if (this.subscriberId != null) {
        queryString += "&subscriberId=" + this.subscriberId;
      }
      if (this.subscriberCode != null) {
        queryString += "&subscriberCode=" + this.subscriberCode;
      }
      return queryString;
    }

    /**
     * play the stream with videojs player or dash player
     */
    play() {
      if (this.streamId.startsWith(EmbeddedPlayer.STREAMS_FOLDER)) {
        //start videojs player because it directly try to play stream from streams folder
        var lastIndexOfDot = this.streamId.lastIndexOf(".");
        var extension = this.streamId.substring(lastIndexOfDot + 1);
        this.playOrder = ["vod"];
        if (extension == EmbeddedPlayer.DASH_EXTENSION) {
          this.playViaDash(this.addSecurityParams(this.streamId), extension);
        } else {
          this.playWithVideoJS(this.addSecurityParams(this.streamId), extension);
        }
      } else {
        this.playIfExists(this.playOrder[0]);
      }
    }

    /**
     * mute or unmute the player
     * @param {boolean} mutestatus true to mute the player
     */
    mutePlayer(mutestatus) {
      this.mute = mutestatus;
      if (this.videojsPlayer) {
        this.videojsPlayer.muted(mutestatus);
      }
      if (this.dashPlayer) {
        this.dashPlayer.setMute(mutestatus);
      }
    }

    /**
     *
     * @returns {boolean} true if player is muted
     */
    isMuted() {
      return this.mute;
    }
    addPlayerListener(playerListener) {
      this.playerListener = playerListener;
    }

    /**
     * WebRTC data listener
     * @param {*} webRTCDataListener
     */
    addWebRTCDataListener(webRTCDataListener) {
      this.webRTCDataListener = webRTCDataListener;
    }

    /**
     *
     * @param {*} data
     */
    sendWebRTCData(data) {
      try {
        if (this.videojsPlayer && this.currentPlayType == "webrtc") {
          this.videojsPlayer.sendDataViaWebRTC(data);
          return true;
        } else {
          Logger.warn("Player is not ready or playType is not WebRTC");
        }
      } catch (error) {
        // Handle the error here
        Logger.error("An error occurred while sending WebRTC data: ", error);
      }
      return false;
    }
  }
  _defineProperty(EmbeddedPlayer, "DEFAULT_PLAY_ORDER", ["webrtc", "hls"]);
  _defineProperty(EmbeddedPlayer, "DEFAULT_PLAY_TYPE", ["mp4", "webm"]);
  _defineProperty(EmbeddedPlayer, "HLS_EXTENSION", "m3u8");
  _defineProperty(EmbeddedPlayer, "WEBRTC_EXTENSION", "webrtc");
  _defineProperty(EmbeddedPlayer, "DASH_EXTENSION", "mpd");
  /**
  * streamsFolder: streams folder. Optional. Default value is "streams"
  */
  _defineProperty(EmbeddedPlayer, "STREAMS_FOLDER", "streams");
  _defineProperty(EmbeddedPlayer, "VIDEO_HTML", STATIC_VIDEO_HTML);
  _defineProperty(EmbeddedPlayer, "VIDEO_PLAYER_ID", "video-player");

  exports.EmbeddedPlayer = EmbeddedPlayer;
  exports.VideoEffect = VideoEffect;
  exports.WebRTCAdaptor = WebRTCAdaptor;
  exports.getUrlParameter = getUrlParameter;

}));
