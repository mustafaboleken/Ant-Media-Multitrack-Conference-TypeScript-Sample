function asyncGeneratorStep(gen, resolve, reject, _next, _throw, key, arg) {
  try {
    var info = gen[key](arg);
    var value = info.value;
  } catch (error) {
    reject(error);
    return;
  }
  if (info.done) {
    resolve(value);
  } else {
    Promise.resolve(value).then(_next, _throw);
  }
}
function _asyncToGenerator(fn) {
  return function () {
    var self = this,
      args = arguments;
    return new Promise(function (resolve, reject) {
      var gen = fn.apply(self, args);
      function _next(value) {
        asyncGeneratorStep(gen, resolve, reject, _next, _throw, "next", value);
      }
      function _throw(err) {
        asyncGeneratorStep(gen, resolve, reject, _next, _throw, "throw", err);
      }
      _next(undefined);
    });
  };
}
function _defineProperty(obj, key, value) {
  key = _toPropertyKey(key);
  if (key in obj) {
    Object.defineProperty(obj, key, {
      value: value,
      enumerable: true,
      configurable: true,
      writable: true
    });
  } else {
    obj[key] = value;
  }
  return obj;
}
function _toPrimitive(input, hint) {
  if (typeof input !== "object" || input === null) return input;
  var prim = input[Symbol.toPrimitive];
  if (prim !== undefined) {
    var res = prim.call(input, hint || "default");
    if (typeof res !== "object") return res;
    throw new TypeError("@@toPrimitive must return a primitive value.");
  }
  return (hint === "string" ? String : Number)(input);
}
function _toPropertyKey(arg) {
  var key = _toPrimitive(arg, "string");
  return typeof key === "symbol" ? key : String(key);
}
function _classPrivateFieldGet(receiver, privateMap) {
  var descriptor = _classExtractFieldDescriptor(receiver, privateMap, "get");
  return _classApplyDescriptorGet(receiver, descriptor);
}
function _classPrivateFieldSet(receiver, privateMap, value) {
  var descriptor = _classExtractFieldDescriptor(receiver, privateMap, "set");
  _classApplyDescriptorSet(receiver, descriptor, value);
  return value;
}
function _classExtractFieldDescriptor(receiver, privateMap, action) {
  if (!privateMap.has(receiver)) {
    throw new TypeError("attempted to " + action + " private field on non-instance");
  }
  return privateMap.get(receiver);
}
function _classApplyDescriptorGet(receiver, descriptor) {
  if (descriptor.get) {
    return descriptor.get.call(receiver);
  }
  return descriptor.value;
}
function _classApplyDescriptorSet(receiver, descriptor, value) {
  if (descriptor.set) {
    descriptor.set.call(receiver, value);
  } else {
    if (!descriptor.writable) {
      throw new TypeError("attempted to set read only private field");
    }
    descriptor.value = value;
  }
}
function _classPrivateMethodGet(receiver, privateSet, fn) {
  if (!privateSet.has(receiver)) {
    throw new TypeError("attempted to get private field on non-instance");
  }
  return fn;
}
function _checkPrivateRedeclaration(obj, privateCollection) {
  if (privateCollection.has(obj)) {
    throw new TypeError("Cannot initialize the same private elements twice on an object");
  }
}
function _classPrivateFieldInitSpec(obj, privateMap, value) {
  _checkPrivateRedeclaration(obj, privateMap);
  privateMap.set(obj, value);
}
function _classPrivateMethodInitSpec(obj, privateSet) {
  _checkPrivateRedeclaration(obj, privateSet);
  privateSet.add(obj);
}

class PeerStats {
  constructor(streamId) {
    this.streamId = streamId;
    this.totalBytesReceivedCount = 0;
    this.totalBytesSent = 0;
    this.videoPacketsLost = 0;
    this.fractionLost = 0;
    this.startTime = 0;
    this.lastFramesEncoded = 0;
    this.totalFramesEncodedCount = 0;
    this.lastBytesReceived = 0;
    this.lastBytesSent = 0;
    this.totalVideoPacketsSent = 0;
    this.totalAudioPacketsSent = 0;
    this.currentTimestamp = 0;
    this.lastTime = 0;
    this.timerId = 0;
    this.firstByteSentCount = 0;
    this.firstBytesReceivedCount = 0;
    this.audioLevel = -1;
    this.qualityLimitationReason = "";
    //res width and res height are video source resolutions
    this.resWidth = 0;
    this.resHeight = 0;
    this.srcFps = 0;
    //frameWidth and frameHeight are the resolutions of the sent video
    this.frameWidth = 0;
    this.frameHeight = 0;
    this.videoRoundTripTime = 0;
    this.videoJitter = 0;
    this.audioRoundTripTime = 0;
    this.audioJitter = 0;
    this.audioPacketsLost = 0;
    this.framesReceived = 0;
    this.framesDropped = 0;
    this.framesDecoded = 0;
    this.audioJitterAverageDelay = 0;
    this.videoJitterAverageDelay = 0;
  }

  //kbits/sec
  get averageOutgoingBitrate() {
    return Math.floor(8 * (this.totalBytesSentCount - this.firstByteSentCount) / (this.currentTimestamp - this.startTime));
  }

  //frames per second
  get currentFPS() {
    return ((this.totalFramesEncodedCount - this.lastFramesEncoded) / (this.currentTimestamp - this.lastTime) * 1000).toFixed(1);
  }

  //kbits/sec
  get averageIncomingBitrate() {
    return Math.floor(8 * (this.totalBytesReceivedCount - this.firstBytesReceivedCount) / (this.currentTimestamp - this.startTime));
  }

  //kbits/sec
  get currentOutgoingBitrate() {
    return Math.floor(8 * (this.totalBytesSentCount - this.lastBytesSent) / (this.currentTimestamp - this.lastTime));
  }

  //kbits/sec
  get currentIncomingBitrate() {
    return Math.floor(8 * (this.totalBytesReceivedCount - this.lastBytesReceived) / (this.currentTimestamp - this.lastTime));
  }
  set currentTime(timestamp) {
    this.lastTime = this.currentTimestamp;
    this.currentTimestamp = timestamp;
    if (this.startTime == 0) {
      this.startTime = timestamp - 1; // do not have zero division error
    }
  }

  set totalBytesReceived(bytesReceived) {
    this.lastBytesReceived = this.totalBytesReceivedCount;
    this.totalBytesReceivedCount = bytesReceived;
    if (this.firstBytesReceivedCount == 0) {
      this.firstBytesReceivedCount = bytesReceived;
    }
  }
  set totalBytesSent(bytesSent) {
    this.lastBytesSent = this.totalBytesSentCount;
    this.totalBytesSentCount = bytesSent;
    if (this.firstByteSentCount == 0) {
      this.firstByteSentCount = bytesSent;
    }
  }
  set totalFramesEncoded(framesEncoded) {
    this.lastFramesEncoded = this.totalFramesEncodedCount;
    this.totalFramesEncodedCount = framesEncoded;
    if (this.lastFramesEncoded == 0) {
      this.lastFramesEncoded = framesEncoded;
    }
  }
}

class WebSocketAdaptor {
  constructor(initialValues) {
    this.debug = false;
    for (var key in initialValues) {
      if (initialValues.hasOwnProperty(key)) {
        this[key] = initialValues[key];
      }
    }
    this.initWebSocketConnection();
  }
  initWebSocketConnection(callbackConnected) {
    this.connecting = true;
    this.connected = false;
    this.pingTimerId = -1;

    /*
    * It's not mandatory if you don't use the new Load Balancer mechanism
    * It uses one of the nodes on Cluster mode
    * Example parameters: "origin" or "edge"
    */
    var url = new URL(this.websocket_url);
    if (!['origin', 'edge'].includes(url.searchParams.get('target'))) {
      url.searchParams.set('target', this.webrtcadaptor.isPlayMode ? 'edge' : 'origin');
      this.websocket_url = url.toString();
    }
    this.wsConn = new WebSocket(this.websocket_url);
    this.wsConn.onopen = () => {
      if (this.debug) {
        console.debug("websocket connected");
      }
      this.pingTimerId = setInterval(() => {
        this.sendPing();
      }, 3000);
      this.connected = true;
      this.connecting = false;
      this.callback("initialized");
      if (typeof callbackConnected != "undefined") {
        callbackConnected();
      }
    };
    this.wsConn.onmessage = event => {
      var obj = JSON.parse(event.data);
      if (obj.command == "start") {
        //this command is received first, when publishing so playmode is false

        if (this.debug) {
          console.debug("received start command");
        }
        this.webrtcadaptor.startPublishing(obj.streamId);
      } else if (obj.command == "takeCandidate") {
        if (this.debug) {
          console.debug("received ice candidate for stream id " + obj.streamId);
          console.debug(obj.candidate);
        }
        this.webrtcadaptor.takeCandidate(obj.streamId, obj.label, obj.candidate);
      } else if (obj.command == "takeConfiguration") {
        if (this.debug) {
          console.debug("received remote description type for stream id: " + obj.streamId + " type: " + obj.type);
        }
        this.webrtcadaptor.takeConfiguration(obj.streamId, obj.sdp, obj.type, obj.idMapping);
      } else if (obj.command == "stop") {
        if (this.debug) {
          console.debug("Stop command received");
        }
        //server sends stop command when the peers are connected to each other in peer-to-peer.
        //It is not being sent in publish,play modes  
        this.webrtcadaptor.closePeerConnection(obj.streamId);
      } else if (obj.command == "error") {
        this.callbackError(obj.definition, obj);
      } else if (obj.command == "notification") {
        this.callback(obj.definition, obj);
      } else if (obj.command == "streamInformation") {
        this.callback(obj.command, obj);
      } else if (obj.command == "roomInformation") {
        this.callback(obj.command, obj);
      } else if (obj.command == "pong") {
        this.callback(obj.command);
      } else if (obj.command == "trackList") {
        this.callback(obj.command, obj);
      } else if (obj.command == "connectWithNewId") {
        this.multiPeerStreamId = obj.streamId;
        this.join(obj.streamId);
      } else if (obj.command == "peerMessageCommand") {
        this.callback(obj.command, obj);
      }
    };
    this.wsConn.onerror = error => {
      this.connecting = false;
      this.connected = false;
      console.info(" error occured: " + JSON.stringify(error));
      this.clearPingTimer();
      this.callbackError("WebSocketNotConnected", error);
    };
    this.wsConn.onclose = event => {
      this.connecting = false;
      this.connected = false;
      if (this.debug) {
        console.debug("connection closed.");
      }
      this.clearPingTimer();
      this.callback("closed", event);
    };
  }
  clearPingTimer() {
    if (this.pingTimerId != -1) {
      if (this.debug) {
        console.debug("Clearing ping message timer");
      }
      clearInterval(this.pingTimerId);
      this.pingTimerId = -1;
    }
  }
  sendPing() {
    var jsCmd = {
      command: "ping"
    };
    this.wsConn.send(JSON.stringify(jsCmd));
  }
  close() {
    this.wsConn.close();
  }
  send(text) {
    if (this.connecting == false && this.connected == false) {
      //try to reconnect
      this.initWebSocketConnection(() => {
        this.send(text);
      });
      return;
    }
    this.wsConn.send(text);
    if (this.debug) {
      console.debug("sent message:" + text);
    }
  }
  isConnected() {
    return this.connected;
  }
  isConnecting() {
    return this.connecting;
  }
}

class SoundMeter {
  constructor(context) {
    this.context = context;
    this.instant = 0.0;
  }
  connectToSource(stream, levelCallback, errorCallback) {
    var _this = this;
    return _asyncToGenerator(function* () {
      yield _this.context.audioWorklet.addModule('./volume-meter-processor.js').catch(err => {
        if (errorCallback !== undefined) {
          errorCallback(err);
        }
        console.error(err);
      });
      try {
        _this.mic = _this.context.createMediaStreamSource(stream);
        _this.volumeMeterNode = new AudioWorkletNode(_this.context, 'volume-meter');
        _this.volumeMeterNode.port.onmessage = _ref => {
          var {
            data
          } = _ref;
          _this.instant = data;
          levelCallback(data.toFixed(2));
        };
        _this.mic.connect(_this.volumeMeterNode).connect(_this.context.destination);
      } catch (e) {
        if (errorCallback !== undefined) {
          errorCallback(null);
        }
        console.error(e);
      }
    })();
  }
  stop() {
    this.mic.disconnect();
  }
}

/**
 * Media management class is responsible to manage audio and video
 * sources and tracks management for the local stream.
 * Also audio and video properties (like bitrate) are managed by this class .
 */
class MediaManager {
  constructor(initialValues) {
    /**
     * the maximum bandwith value that browser can send a stream
     * keep in mind that browser may send video less than this value
     */
    this.bandwidth = 1200; //kbps

    /**
     * This flags enables/disables debug logging
     */
    this.debug = false;

    /**
     * The cam_location below is effective when camera and screen is send at the same time.
     * possible values are top and bottom. It's on right all the time
     */
    this.camera_location = "top";

    /**
     * The cam_margin below is effective when camera and screen is send at the same time.
     * This is the margin value in px from the edges
     */
    this.camera_margin = 15;

    /**
     * this camera_percent is how large the camera view appear on the screen. It's %15 by default.
     */
    this.camera_percent = 15;

    /**
     * initial media constraints provided by the user
     */
    this.mediaConstraints = {
      video: true,
      audio: true
    };

    /**
     * this is the callback function to get video/audio sender from WebRTCAdaptor
     */
    this.getSender = initialValues.getSender;

    /**
     * This is the Stream Id for the publisher.
     */
    this.publishStreamId = null;

    /**
     * this is the object of the local stream to publish
     * it is initiated in initLocalStream method
     */
    this.localStream = null;

    /**
     * publish mode is determined by the user and set by @mediaConstraints.video
     * It may be camera, screen, screen+camera
     */
    this.publishMode = "camera"; //screen, screen+camera

    /**
     * The values of the above fields are provided as user parameters by the constructor.
     * TODO: Also some other hidden parameters may be passed here
     */
    for (var key in initialValues.userParameters) {
      if (initialValues.userParameters.hasOwnProperty(key)) {
        this[key] = initialValues.userParameters[key];
      }
    }

    /**
     * current volume value which is set by the user
     */
    this.currentVolume = null;

    /**
     * Keeps the audio track to be closed in case of audio track change
     */
    this.previousAudioTrack = null;

    /**
     * The screen video track in screen+camera mode
     */
    this.desktopStream = null;

    /**
     * The camera (overlay) video track in screen+camera mode
     */
    this.smallVideoTrack = null;

    /**
     * Audio context to use for meter, mix, gain
     */
    this.audioContext = new AudioContext();

    /**
     * the main audio in single audio case
     * the primary audio in mixed audio case
     *
     * its volume can be controled
     */
    this.primaryAudioTrackGainNode = null;

    /**
     * the secondary audio in mixed audio case
     *
     * its volume can be controled
     */
    this.secondaryAudioTrackGainNode = null;

    /**
     * this is the sound meter object for the local stream
     */
    this.localStreamSoundMeter = null;

    /**
     * this is the level callback for sound meter object
     */
    this.levelCallback = null;

    /**
     * Timer to create black frame to publish when video is muted
     */
    this.blackFrameTimer = null;

    /**
     * Timer to draw camera and desktop to canvas
     */
    this.desktopCameraCanvasDrawerTimer = null;

    /**
     * For audio check when the user is muted itself.
     * Check enableAudioLevelWhenMuted
     */
    this.mutedAudioStream = null;

    /**
     * This flag is the status of audio stream
     * Checking when the audio stream is updated
     */
    this.isMuted = false;

    /**
     * meter refresh period for "are you talking?" check
     */
    this.meterRefresh = null;

    /**
     * For keeping track of whether user turned off the camera
     */
    this.cameraEnabled = true;

    /**
     * html video element that presents local stream
     */
    this.localVideo = this.localVideoElement || document.getElementById(this.localVideoId);

    //A dummy stream created to replace the tracks when camera is turned off.
    this.dummyCanvas = document.createElement("canvas");

    /**
     * The timer id for SoundMeter for the local stream
     */
    this.soundLevelProviderId = -1;

    // It should be compatible with previous version
    if (this.mediaConstraints) {
      if (this.mediaConstraints.video == "camera") {
        this.publishMode = "camera";
      } else if (this.mediaConstraints.video == "screen") {
        this.publishMode = "screen";
      } else if (this.mediaConstraints.video == "screen+camera") {
        this.publishMode = "screen+camera";
      }
    } else {
      //just define default values
      this.mediaConstraints = {
        video: true,
        audio: true
      };
    }

    //Check browser support for screen share function
    this.checkBrowserScreenShareSupported();
  }

  /**
   * Called by the WebRTCAdaptor at the start if it isn't play mode
   */
  initLocalStream() {
    this.checkWebRTCPermissions();
    if (typeof this.mediaConstraints.video != "undefined" && this.mediaConstraints.video != false) {
      return this.openStream(this.mediaConstraints, this.mode);
    } else if (typeof this.mediaConstraints.audio != "undefined" && this.mediaConstraints.audio != false) {
      // get only audio
      var media_audio_constraint = {
        audio: this.mediaConstraints.audio
      };
      return this.navigatorUserMedia(media_audio_constraint, stream => {
        return this.gotStream(stream);
      }, true);
    } else {
      //neither video nor audio is requested
      //just return null stream
      console.log("no media requested, just return an empty stream");
      return new Promise((resolve, reject) => {
        resolve(null);
      });
    }
  }

  /*
  * Called to checks if Websocket and media usage are allowed
  */
  checkWebRTCPermissions() {
    if (!("WebSocket" in window)) {
      console.log("WebSocket not supported.");
      this.callbackError("WebSocketNotSupported");
      return;
    }
    if (typeof navigator.mediaDevices == "undefined") {
      console.log("Cannot open camera and mic because of unsecure context. Please Install SSL(https)");
      this.callbackError("UnsecureContext");
      return;
    }
    if (typeof navigator.mediaDevices == "undefined" || navigator.mediaDevices == undefined || navigator.mediaDevices == null) {
      this.callbackError("getUserMediaIsNotAllowed");
    }
  }

  /*
   * Called to get the available video and audio devices on the system
   */
  getDevices() {
    return navigator.mediaDevices.enumerateDevices().then(devices => {
      var deviceArray = new Array();
      var checkAudio = false;
      var checkVideo = false;
      devices.forEach(device => {
        if (device.kind == "audioinput" || device.kind == "videoinput") {
          deviceArray.push(device);
          if (device.kind == "audioinput") {
            checkAudio = true;
          }
          if (device.kind == "videoinput") {
            checkVideo = true;
          }
        }
      });
      this.callback("available_devices", deviceArray);

      //TODO: is the following part necessary. why?
      if (checkAudio == false && this.localStream == null) {
        console.log("Audio input not found");
        console.log("Retrying to get user media without audio");
        if (this.inputDeviceNotFoundLimit < 2) {
          if (checkVideo != false) {
            this.openStream({
              video: true,
              audio: false
            }, this.mode);
            this.inputDeviceNotFoundLimit++;
          } else {
            console.log("Video input not found");
            alert("There is no video or audio input");
          }
        } else {
          alert("No input device found, publish is not possible");
        }
      }
      return deviceArray;
    }).catch(err => {
      console.error("Cannot get devices -> error name: " + err.name + ": " + err.message);
      throw err;
    });
  }

  /*
   * Called to add a device change listener
   */
  trackDeviceChange() {
    navigator.mediaDevices.ondevicechange = () => {
      this.getDevices();
    };
  }

  /**
   * This function create a canvas which combines screen video and camera video as an overlay
   *
   * @param {*} stream : screen share stream
   * @param {*} streamId
   * @param {*} onEndedCallback : callback when called on screen share stop
   */
  setDesktopwithCameraSource(stream, streamId, onEndedCallback) {
    this.desktopStream = stream;
    return this.navigatorUserMedia({
      video: true,
      audio: false
    }, cameraStream => {
      this.smallVideoTrack = cameraStream.getVideoTracks()[0];

      //create a canvas element
      var canvas = document.createElement("canvas");
      var canvasContext = canvas.getContext("2d");

      //create video element for screen
      //var screenVideo = document.getElementById('sourceVideo');
      var screenVideo = document.createElement('video');
      screenVideo.srcObject = stream;
      screenVideo.play();
      //create video element for camera
      var cameraVideo = document.createElement('video');
      cameraVideo.srcObject = cameraStream;
      cameraVideo.play();
      var canvasStream = canvas.captureStream(15);
      if (onEndedCallback != null) {
        stream.getVideoTracks()[0].onended = function (event) {
          onEndedCallback(event);
        };
      }
      var promise;
      if (this.localStream == null) {
        promise = this.gotStream(canvasStream);
      } else {
        promise = this.updateVideoTrack(canvasStream, streamId, onended, null);
      }
      promise.then(() => {
        //update the canvas
        this.desktopCameraCanvasDrawerTimer = setInterval(() => {
          //draw screen to canvas
          canvas.width = screenVideo.videoWidth;
          canvas.height = screenVideo.videoHeight;
          canvasContext.drawImage(screenVideo, 0, 0, canvas.width, canvas.height);
          var cameraWidth = screenVideo.videoWidth * (this.camera_percent / 100);
          var cameraHeight = cameraVideo.videoHeight / cameraVideo.videoWidth * cameraWidth;
          var positionX = canvas.width - cameraWidth - this.camera_margin;
          var positionY;
          if (this.camera_location == "top") {
            positionY = this.camera_margin;
          } else {
            //if not top, make it bottom
            //draw camera on right bottom corner
            positionY = canvas.height - cameraHeight - this.camera_margin;
          }
          canvasContext.drawImage(cameraVideo, positionX, positionY, cameraWidth, cameraHeight);
        }, 66);
      });
    }, true);
  }

  /**
   * This function does these:
   *    1. Remove the audio track from the stream provided if it is camera. Other case
   *       is screen video + system audio track. In this case audio is kept in stream.
   *    2. Open audio track again if audio constaint isn't false
   *    3. Make audio track Gain Node to be able to volume adjustable
   *  4. If screen is shared and system audio is available then the system audio and
   *     opened audio track are mixed
   *
   * @param {*} mediaConstraints
   * @param {*} audioConstraint
   * @param {*} stream
   * @param {*} streamId
   */
  prepareStreamTracks(mediaConstraints, audioConstraint, stream, streamId) {
    //this trick, getting audio and video separately, make us add or remove tracks on the fly
    var audioTracks = stream.getAudioTracks();
    if (audioTracks.length > 0 && this.publishMode == "camera") {
      audioTracks[0].stop();
      stream.removeTrack(audioTracks[0]);
    }
    //now get only audio to add this stream
    if (audioConstraint != "undefined" && audioConstraint != false) {
      var media_audio_constraint = {
        audio: audioConstraint
      };
      return this.navigatorUserMedia(media_audio_constraint, audioStream => {
        //here audioStream has onr audio track only
        audioStream = this.setGainNodeStream(audioStream);
        // now audio stream has two audio strams.
        // 1. Gain Node : this will be added to local stream to publish
        // 2. Original audio track : keep its reference to stop later

        //add callback if desktop is sharing
        var onended = event => {
          this.callback("screen_share_stopped");
          this.setVideoCameraSource(streamId, mediaConstraints, null, true);
        };
        if (this.publishMode == "screen") {
          return this.updateVideoTrack(stream, streamId, onended, true).then(() => {
            if (audioTracks.length > 0) {
              //system audio share case, then mix it with device audio
              audioStream = this.mixAudioStreams(stream, audioStream);
            }
            return this.updateAudioTrack(audioStream, streamId, null);
          });
        } else if (this.publishMode == "screen+camera") {
          if (audioTracks.length > 0) {
            //system audio share case, then mix it with device audio
            audioStream = this.mixAudioStreams(stream, audioStream);
          }
          return this.updateAudioTrack(audioStream, streamId, null).then(() => {
            return this.setDesktopwithCameraSource(stream, streamId, onended);
          });
        } else {
          if (audioConstraint != false && audioConstraint != undefined) {
            stream.addTrack(audioStream.getAudioTracks()[0]);
          }
          return this.gotStream(stream);
        }
      }, true);
    } else {
      return this.gotStream(stream);
    }
  }

  /**
   * Called to get user media (camera and/or mic)
   *
   * @param {*} mediaConstraints : media constaint
   * @param {*} func : callback on success. The stream which is got, is passed as parameter to this function
   * @param {*} catch_error : error is checked if catch_error is true
   */
  navigatorUserMedia(mediaConstraints, func, catch_error) {
    return navigator.mediaDevices.getUserMedia(mediaConstraints).then(stream => {
      if (typeof func != "undefined" || func != null) {
        func(stream);
      }
      return stream;
    }).catch(error => {
      if (catch_error == true) {
        if (error.name == "NotFoundError") {
          this.getDevices();
        } else {
          this.callbackError(error.name, error.message);
        }
      } else {
        console.warn(error);
      }
      //throw error if there is a promise
      throw error;
    });
  }

  /**
   * Called to get display media (screen share)
   *
   * @param {*} mediaConstraints : media constaint
   * @param {*} func : callback on success. The stream which is got, is passed as parameter to this function
   */
  navigatorDisplayMedia(mediaConstraints, func) {
    return navigator.mediaDevices.getDisplayMedia(mediaConstraints).then(stream => {
      if (typeof func != "undefined") {
        func(stream);
      }
      return stream;
    }).catch(error => {
      if (error.name === "NotAllowedError") {
        console.debug("Permission denied error");
        this.callbackError("ScreenSharePermissionDenied");

        // If error catched then redirect Default Stream Camera
        if (this.localStream == null) {
          var mediaConstraints = {
            video: true,
            audio: true
          };
          this.openStream(mediaConstraints);
        } else {
          this.switchVideoCameraCapture(streamId);
        }
      }
    });
  }

  /**
   * Called to get the media (User Media or Display Media)
   * @param {*} mediaConstraints
   * @param {*} audioConstraint
   * @param {*} streamId
   */
  getMedia(mediaConstraints, audioConstraint, streamId) {
    if (this.desktopCameraCanvasDrawerTimer != null) {
      clearInterval(this.desktopCameraCanvasDrawerTimer);
      this.desktopCameraCanvasDrawerTimer = null;
    }

    // Check Media Constraint video value screen or screen + camera
    if (this.publishMode == "screen+camera" || this.publishMode == "screen") {
      return this.navigatorDisplayMedia(mediaConstraints).then(stream => {
        if (this.smallVideoTrack) this.smallVideoTrack.stop();
        return this.prepareStreamTracks(mediaConstraints, audioConstraint, stream, streamId);
      });
    }
    // If mediaConstraints only user camera
    else {
      return this.navigatorUserMedia(mediaConstraints).then(stream => {
        if (this.smallVideoTrack) this.smallVideoTrack.stop();
        return this.prepareStreamTracks(mediaConstraints, audioConstraint, stream, streamId);
      }).catch(error => {
        if (error.name == "NotFoundError") {
          this.getDevices();
        } else {
          this.callbackError(error.name, error.message);
        }
      });
    }
  }

  /**
   * Open media stream, it may be screen, camera or audio
   */
  openStream(mediaConstraints) {
    this.mediaConstraints = mediaConstraints;
    var audioConstraint = false;
    if (typeof mediaConstraints.audio != "undefined" && mediaConstraints.audio != false) {
      audioConstraint = mediaConstraints.audio;
    }
    if (typeof mediaConstraints.video != "undefined") {
      return this.getMedia(mediaConstraints, audioConstraint);
    } else {
      return new Promise((resolve, reject) => {
        this.callbackError("media_constraint_video_not_defined");
        console.error("MediaConstraint video is not defined");
        reject("media_constraint_video_not_defined");
      });
    }
  }

  /**
   * Closes stream, if you want to stop peer connection, call stop(streamId)
   */
  closeStream() {
    if (this.localStream) {
      this.localStream.getVideoTracks().forEach(function (track) {
        track.onended = null;
        track.stop();
      });
      this.localStream.getAudioTracks().forEach(function (track) {
        track.onended = null;
        track.stop();
      });
    }
    if (this.videoTrack) {
      this.videoTrack.stop();
    }
    if (this.audioTrack) {
      this.audioTrack.stop();
    }
    if (this.smallVideoTrack) {
      this.smallVideoTrack.stop();
    }
    if (this.previousAudioTrack) {
      this.previousAudioTrack.stop();
    }
    if (this.soundLevelProviderId != -1) {
      clearInterval(this.soundLevelProviderId);
      this.soundLevelProviderId = -1;
    }
  }

  /**
   * Checks browser supports screen share feature
   * if exist it calls callback with "browser_screen_share_supported"
   */
  checkBrowserScreenShareSupported() {
    if (typeof navigator.mediaDevices != "undefined" && navigator.mediaDevices.getDisplayMedia || navigator.getDisplayMedia) {
      this.callback("browser_screen_share_supported");
    }
  }
  /**
   * Changes the secondary stream gain in mixed audio mode
   *
   * @param {*} enable
   */
  enableSecondStreamInMixedAudio(enable) {
    if (this.secondaryAudioTrackGainNode != null) {
      if (enable) {
        this.secondaryAudioTrackGainNode.gain.value = 1;
      } else {
        this.secondaryAudioTrackGainNode.gain.value = 0;
      }
    }
  }

  /**
   * Changes local stream when new stream is prepared
   *
   * @param {*} stream
   */
  gotStream(stream) {
    this.localStream = stream;
    if (this.localVideo) {
      this.localVideo.srcObject = stream;
    }
    this.getDevices();
    this.trackDeviceChange();
    return new Promise((resolve, reject) => {
      resolve();
    });
  }

  /**
   * Changes local video and sets localStream as source
   *
   * @param {*} videoEl
   */
  changeLocalVideo(videoEl) {
    this.localVideo = videoEl;
    if (this.localStream) {
      this.localVideo.srcObject = this.localStream;
    }
  }

  /**
   * These methods are initialized when the user is muted himself in a publish scenario
   * It will keep track if the user is trying to speak without sending any data to server
   * Please don't forget to disable this function with disableAudioLevelWhenMuted if you use it.
   */
  enableAudioLevelWhenMuted() {
    navigator.mediaDevices.getUserMedia({
      video: false,
      audio: true
    }).then(stream => {
      this.mutedAudioStream = stream;
      var soundMeter = new SoundMeter(this.audioContext);
      soundMeter.connectToSource(this.mutedAudioStream, value => {
        if (value > 0.1) {
          this.callback("speaking_but_muted");
        }
      }, e => {
        if (e) {
          alert(e);
          return;
        }
        this.meterRefresh = setInterval(() => {
          if (soundMeter.instant.toFixed(2) > 0.1) {
            this.callback("speaking_but_muted");
          }
        }, 200);
      });
    }).catch(function (err) {
      console.log("Can't get the soundlevel on mute");
    });
  }
  disableAudioLevelWhenMuted() {
    if (this.meterRefresh != null) {
      clearInterval(this.meterRefresh);
    }
    if (this.mutedAudioStream != null) {
      this.mutedAudioStream.getTracks().forEach(function (track) {
        track.stop();
      });
    }
  }

  /**
   * This method mixed the first stream audio to the second stream audio and
   * @param {*} stream  : Primary stream that contain video and audio (system audio)
   * @param {*} secondStream :stream has device audio
   * @returns mixed stream.
   */
  mixAudioStreams(stream, secondStream) {
    //console.debug("audio stream track count: " + audioStream.getAudioTracks().length);
    var composedStream = new MediaStream();
    //added the video stream from the screen
    stream.getVideoTracks().forEach(function (videoTrack) {
      composedStream.addTrack(videoTrack);
    });
    this.audioContext = new AudioContext();
    var audioDestionation = this.audioContext.createMediaStreamDestination();
    if (stream.getAudioTracks().length > 0) {
      this.primaryAudioTrackGainNode = this.audioContext.createGain();

      //Adjust the gain for screen sound
      this.primaryAudioTrackGainNode.gain.value = 1;
      var audioSource = this.audioContext.createMediaStreamSource(stream);
      audioSource.connect(this.primaryAudioTrackGainNode).connect(audioDestionation);
    } else {
      console.debug("Origin stream does not have audio track");
    }
    if (secondStream.getAudioTracks().length > 0) {
      this.secondaryAudioTrackGainNode = this.audioContext.createGain();

      //Adjust the gain for second sound
      this.secondaryAudioTrackGainNode.gain.value = 1;
      var audioSource2 = this.audioContext.createMediaStreamSource(secondStream);
      audioSource2.connect(this.secondaryAudioTrackGainNode).connect(audioDestionation);
    } else {
      console.debug("Second stream does not have audio track");
    }
    audioDestionation.stream.getAudioTracks().forEach(function (track) {
      composedStream.addTrack(track);
      console.log("audio destination add track");
    });
    return composedStream;
  }

  /**
   * This method creates a Gain Node stream to make the audio track adjustable
   *
   * @param {*} stream
   * @returns
   */
  setGainNodeStream(stream) {
    if (this.mediaConstraints.audio != false && typeof this.mediaConstraints.audio != "undefined") {
      // Get the videoTracks from the stream.
      var videoTracks = stream.getVideoTracks();

      // Get the audioTracks from the stream.
      var audioTracks = stream.getAudioTracks();

      /**
       * Create a new audio context and build a stream source,
       * stream destination and a gain node. Pass the stream into
       * the mediaStreamSource so we can use it in the Web Audio API.
       */
      this.audioContext = new AudioContext();
      var mediaStreamSource = this.audioContext.createMediaStreamSource(stream);
      var mediaStreamDestination = this.audioContext.createMediaStreamDestination();
      this.primaryAudioTrackGainNode = this.audioContext.createGain();

      /**
       * Connect the stream to the gainNode so that all audio
       * passes through the gain and can be controlled by it.
       * Then pass the stream from the gain to the mediaStreamDestination
       * which can pass it back to the RTC client.
       */
      mediaStreamSource.connect(this.primaryAudioTrackGainNode);
      this.primaryAudioTrackGainNode.connect(mediaStreamDestination);
      if (this.currentVolume == null) {
        this.primaryAudioTrackGainNode.gain.value = 1;
      } else {
        this.primaryAudioTrackGainNode.gain.value = this.currentVolume;
      }

      /**
       * The mediaStreamDestination.stream outputs a MediaStream object
       * containing a single AudioMediaStreamTrack. Add the video track
       * to the new stream to rejoin the video with the controlled audio.
       */
      var controlledStream = mediaStreamDestination.stream;
      for (var videoTrack of videoTracks) {
        controlledStream.addTrack(videoTrack);
      }
      for (var audioTrack of audioTracks) {
        controlledStream.addTrack(audioTrack);
      }
      if (this.previousAudioTrack !== null) {
        this.previousAudioTrack.stop();
      }
      this.previousAudioTrack = controlledStream.getAudioTracks()[1];

      /**
       * Use the stream that went through the gainNode. This
       * is the same stream but with altered input volume levels.
       */
      return controlledStream;
    }
    return stream;
  }

  /**
   * Called by User
   * to switch the Screen Share mode
   *
   * @param {*} streamId
   */
  switchDesktopCapture(streamId) {
    this.publishMode = "screen";
    var audioConstraint = false;
    if (typeof this.mediaConstraints.audio != "undefined" && this.mediaConstraints.audio != false) {
      audioConstraint = this.mediaConstraints.audio;
    }
    if (typeof this.mediaConstraints.video != "undefined" && this.mediaConstraints.video != false) {
      this.mediaConstraints.video = true;
    }
    //TODO: I don't think we need to get audio again. We just need to switch the video stream
    return this.getMedia(this.mediaConstraints, audioConstraint, streamId);
  }

  /**
   * Called by User
   * to switch the Screen Share with Camera mode
   *
   * @param {*} streamId
   */
  switchDesktopCaptureWithCamera(streamId) {
    if (typeof this.mediaConstraints.video != "undefined" && this.mediaConstraints.video != false) {
      this.mediaConstraints.video = true;
    }
    this.publishMode = "screen+camera";
    var audioConstraint = false;
    if (typeof this.mediaConstraints.audio != "undefined" && this.mediaConstraints.audio != false) {
      audioConstraint = this.mediaConstraints.audio;
    }
    //TODO: I don't think we need to get audio again. We just need to switch the video stream
    return this.getMedia(this.mediaConstraints, audioConstraint, streamId);
  }

  /**
   * This method updates the local stream. It removes existant audio track from the local stream
   * and add the audio track in `stream` parameter to the local stream
   */
  updateLocalAudioStream(stream, onEndedCallback) {
    var newAudioTrack = stream.getAudioTracks()[0];
    if (this.localStream != null && this.localStream.getAudioTracks()[0] != null) {
      var audioTrack = this.localStream.getAudioTracks()[0];
      this.localStream.removeTrack(audioTrack);
      audioTrack.stop();
      this.localStream.addTrack(newAudioTrack);
    } else if (this.localStream != null) {
      this.localStream.addTrack(newAudioTrack);
    } else {
      this.localStream = stream;
    }
    if (this.localVideo != null) {
      //it can be null
      this.localVideo.srcObject = this.localStream;
    }
    if (onEndedCallback != null) {
      stream.getAudioTracks()[0].onended = function (event) {
        onEndedCallback(event);
      };
    }
    if (this.isMuted) {
      this.muteLocalMic();
    } else {
      this.unmuteLocalMic();
    }
    if (this.localStreamSoundMeter != null) {
      this.connectSoundMeterToLocalStream();
    }
  }

  /**
   * This method updates the local stream. It removes existant video track from the local stream
   * and add the video track in `stream` parameter to the local stream
   */
  updateLocalVideoStream(stream, onEndedCallback, stopDesktop) {
    if (stopDesktop && this.desktopStream != null) {
      this.desktopStream.getVideoTracks()[0].stop();
    }
    var newVideoTrack = stream.getVideoTracks()[0];
    if (this.localStream != null && this.localStream.getVideoTracks()[0] != null) {
      var videoTrack = this.localStream.getVideoTracks()[0];
      this.localStream.removeTrack(videoTrack);
      videoTrack.stop();
      this.localStream.addTrack(newVideoTrack);
    } else if (this.localStream != null) {
      this.localStream.addTrack(newVideoTrack);
    } else {
      this.localStream = stream;
    }
    if (this.localVideo) {
      this.localVideo.srcObject = this.localStream;
    }
    if (onEndedCallback != null) {
      stream.getVideoTracks()[0].onended = function (event) {
        onEndedCallback(event);
      };
    }
  }

  /**
   * Called by User
   * to change video source
   *
   * @param {*} streamId
   * @param {*} deviceId
   */
  switchAudioInputSource(streamId, deviceId) {
    //stop the track because in some android devices need to close the current camera stream
    var audioTrack = this.localStream.getAudioTracks()[0];
    if (audioTrack) {
      audioTrack.stop();
    } else {
      console.warn("There is no audio track in local stream");
    }
    if (typeof deviceId != "undefined") {
      //Update the media constraints
      if (this.mediaConstraints.audio !== true) this.mediaConstraints.audio.deviceId = deviceId;else this.mediaConstraints.audio = {
        "deviceId": deviceId
      };

      //to change only audio track set video false otherwise issue #3826 occurs on Android
      var tempMediaConstraints = {
        "video": false,
        "audio": {
          "deviceId": deviceId
        }
      };
      return this.setAudioInputSource(streamId, tempMediaConstraints, null, deviceId);
    } else {
      return new Promise((resolve, reject) => {
        reject("There is no device id for audio input source");
      });
    }
  }

  /**
   * This method sets Audio Input Source and called when you change audio device
   * It calls updateAudioTrack function to update local audio stream.
   */
  setAudioInputSource(streamId, mediaConstraints, onEndedCallback) {
    return this.navigatorUserMedia(mediaConstraints, stream => {
      stream = this.setGainNodeStream(stream);
      return this.updateAudioTrack(stream, streamId, mediaConstraints, onEndedCallback);
    }, true);
  }

  /**
   * Called by User
   * to change video camera capture
   *
   * @param {*} streamId Id of the stream to be changed.
   * @param {*} deviceId Id of the device which will use as a media device
   * @param {*} onEndedCallback callback for when the switching video state is completed, can be used to understand if it is loading or not
   *
   * This method is used to switch to video capture.
   */
  switchVideoCameraCapture(streamId, deviceId, onEndedCallback) {
    //stop the track because in some android devices need to close the current camera stream
    if (this.localStream && this.localStream.getVideoTracks().length > 0) {
      var videoTrack = this.localStream.getVideoTracks()[0];
      videoTrack.stop();
    } else {
      console.warn("There is no video track in local stream");
    }
    this.publishMode = "camera";
    return navigator.mediaDevices.enumerateDevices().then(devices => {
      for (var i = 0; i < devices.length; i++) {
        if (devices[i].kind == "videoinput") {
          //Adjust video source only if there is a matching device id with the given one.
          //It creates problems if we don't check that since video can be just true to select default cam and it is like that in many cases.
          if (devices[i].deviceId == deviceId) {
            if (this.mediaConstraints.video !== true) this.mediaConstraints.video.deviceId = {
              exact: deviceId
            };else this.mediaConstraints.video = {
              deviceId: {
                exact: deviceId
              }
            };
            break;
          }
        }
      }
      //If no matching device found don't adjust the media constraints let it be true instead of a device ID
      console.debug("Given deviceId = " + deviceId + " - Media constraints video property = " + this.mediaConstraints.video);
      return this.setVideoCameraSource(streamId, this.mediaConstraints, null, true, deviceId);
    });
  }

  /**
   * This method sets Video Input Source and called when you change video device
   * It calls updateVideoTrack function to update local video stream.
   */
  setVideoCameraSource(streamId, mediaConstraints, onEndedCallback, stopDesktop) {
    return this.navigatorUserMedia(mediaConstraints, stream => {
      if (stopDesktop && this.secondaryAudioTrackGainNode && stream.getAudioTracks().length > 0) {
        //This audio track update is necessary for such a case:
        //If you enable screen share with browser audio and then
        //return back to the camera, the audio should be only from mic.
        //If, we don't update audio with the following lines,
        //the mixed (mic+browser) audio would be streamed in the camera mode.
        this.secondaryAudioTrackGainNode = null;
        stream = this.setGainNodeStream(stream);
        this.updateAudioTrack(stream, streamId, mediaConstraints, onEndedCallback);
      }
      if (this.cameraEnabled) {
        return this.updateVideoTrack(stream, streamId, onEndedCallback, stopDesktop);
      } else {
        return this.turnOffLocalCamera();
      }
    }, true);
  }

  /**
   * Called by User
   * to switch between front and back camera on mobile devices
   *
   * @param {*} streamId Id of the stream to be changed.
   * @param {*} facingMode it can be "user" or "environment"
   *
   * This method is used to switch front and back camera.
   */
  switchVideoCameraFacingMode(streamId, facingMode) {
    //stop the track because in some android devices need to close the current camera stream
    if (this.localStream && this.localStream.getVideoTracks().length > 0) {
      var videoTrack = this.localStream.getVideoTracks()[0];
      videoTrack.stop();
    } else {
      console.warn("There is no video track in local stream");
    }

    // When device id set, facing mode is not working
    // so, remove device id
    if (this.mediaConstraints.video !== undefined && this.mediaConstraints.video.deviceId !== undefined) {
      delete this.mediaConstraints.video.deviceId;
    }
    var videoConstraint = {
      'facingMode': facingMode
    };
    this.mediaConstraints.video = Object.assign({}, this.mediaConstraints.video, videoConstraint);
    this.publishMode = "camera";
    console.debug("Media constraints video property = " + this.mediaConstraints.video);
    return this.setVideoCameraSource(streamId, {
      video: this.mediaConstraints.video
    }, null, true);
  }

  /**
   * Updates the audio track in the audio sender
   * getSender method is set on MediaManagercreation by WebRTCAdaptor
   *
   * @param {*} stream
   * @param {*} streamId
   * @param {*} onEndedCallback
   */
  updateAudioTrack(stream, streamId, onEndedCallback) {
    var audioTrackSender = this.getSender(streamId, "audio");
    if (audioTrackSender) {
      return audioTrackSender.replaceTrack(stream.getAudioTracks()[0]).then(result => {
        this.updateLocalAudioStream(stream, onEndedCallback);
      }).catch(function (error) {
        console.log(error.name);
      });
    } else {
      this.updateLocalAudioStream(stream, onEndedCallback);
      return new Promise((resolve, reject) => {
        resolve();
      });
    }
  }

  /**
   * Updates the video track in the video sender
   * getSender method is set on MediaManagercreation by WebRTCAdaptor
   *
   * @param {*} stream
   * @param {*} streamId
   * @param {*} onEndedCallback
   */
  updateVideoTrack(stream, streamId, onEndedCallback, stopDesktop) {
    var videoTrackSender = this.getSender(streamId, "video");
    if (videoTrackSender) {
      return videoTrackSender.replaceTrack(stream.getVideoTracks()[0]).then(result => {
        this.updateLocalVideoStream(stream, onEndedCallback, stopDesktop);
      }).catch(error => {
        console.log(error.name);
      });
    } else {
      this.updateLocalVideoStream(stream, onEndedCallback, stopDesktop);
      return new Promise((resolve, reject) => {
        resolve();
      });
    }
  }

  /**
   * If you mute turn off the camera still some data should be sent
   * Tihs method create a black frame to reduce data transfer
   */
  initializeDummyFrame() {
    this.dummyCanvas.getContext('2d').fillRect(0, 0, 320, 240);
    this.replacementStream = this.dummyCanvas.captureStream();
  }

  /**
   * Called by User
   * turns of the camera stream and starts streaming black dummy frame
   */
  turnOffLocalCamera(streamId) {
    //Initialize the first dummy frame for switching.
    this.initializeDummyFrame();

    //We need to send black frames within a time interval, because when the user turn off the camera,
    //player can't connect to the sender since there is no data flowing. Sending a black frame in each 3 seconds resolves it.
    if (this.blackFrameTimer == null) {
      this.blackFrameTimer = setInterval(() => {
        this.initializeDummyFrame();
      }, 3000);
    }
    if (this.localStream != null) {
      var choosenId;
      if (streamId != null || typeof streamId != "undefined") {
        choosenId = streamId;
      } else {
        choosenId = this.publishStreamId;
      }
      this.cameraEnabled = false;
      return this.updateVideoTrack(this.replacementStream, choosenId, null, true);
    } else {
      return new Promise((resolve, reject) => {
        this.callbackError("NoActiveConnection");
        reject("NoActiveStream");
      });
    }
  }

  /**
   * Called by User
   * turns of the camera stream and starts streaming camera again instead of black dummy frame
   */
  turnOnLocalCamera(streamId) {
    if (this.blackFrameTimer != null) {
      clearInterval(this.blackFrameTimer);
      this.blackFrameTimer = null;
    }
    if (this.localStream == null) {
      return this.navigatorUserMedia(this.mediaConstraints, stream => {
        this.gotStream(stream);
      }, false);
    }
    //This method will get the camera track and replace it with dummy track
    else {
      return this.navigatorUserMedia(this.mediaConstraints, stream => {
        var choosenId;
        if (streamId != null || typeof streamId != "undefined") {
          choosenId = streamId;
        } else {
          choosenId = this.publishStreamId;
        }
        this.cameraEnabled = true;
        this.updateVideoTrack(stream, choosenId, null, true);
      }, false);
    }
  }

  /**
   * Called by User
   * to mute local audio streaming
   */
  muteLocalMic() {
    this.isMuted = true;
    if (this.localStream != null) {
      this.localStream.getAudioTracks().forEach(track => track.enabled = false);
    } else {
      this.callbackError("NoActiveConnection");
    }
  }

  /**
   * Called by User
   * to unmute local audio streaming
   *
   * if there is audio it calls callbackError with "AudioAlreadyActive" parameter
   */
  unmuteLocalMic() {
    this.isMuted = false;
    if (this.localStream != null) {
      this.localStream.getAudioTracks().forEach(track => track.enabled = true);
    } else {
      this.callbackError("NoActiveConnection");
    }
  }

  /**
   * If we have multiple video tracks in coming versions, this method may cause some issues
   */
  getVideoSender(streamId) {
    var videoSender = null;
    if (typeof adapter !== "undefined" && adapter !== null && (adapter.browserDetails.browser === 'chrome' || adapter.browserDetails.browser === 'firefox' || adapter.browserDetails.browser === 'safari' && adapter.browserDetails.version >= 64) && 'RTCRtpSender' in window && 'setParameters' in window.RTCRtpSender.prototype) {
      videoSender = this.getSender(streamId, "video");
    }
    return videoSender;
  }

  /**
   * Called by User
   * to set maximum video bandwidth is in kbps
   */
  changeBandwidth(bandwidth, streamId) {
    var errorDefinition = "";
    var videoSender = this.getVideoSender(streamId);
    if (videoSender != null) {
      var parameters = videoSender.getParameters();
      if (!parameters.encodings) {
        parameters.encodings = [{}];
      }
      if (bandwidth === 'unlimited') {
        delete parameters.encodings[0].maxBitrate;
      } else {
        parameters.encodings[0].maxBitrate = bandwidth * 1000;
      }
      return videoSender.setParameters(parameters);
    } else {
      errorDefinition = "Video sender not found to change bandwidth. Streaming may not be active";
    }
    return Promise.reject(errorDefinition);
  }
  /**
   * Called by user
   * sets the volume level
   *
   * @param {*} volumeLevel : Any number between 0 and 1.
   */
  setVolumeLevel(volumeLevel) {
    this.currentVolume = volumeLevel;
    if (this.primaryAudioTrackGainNode != null) {
      this.primaryAudioTrackGainNode.gain.value = volumeLevel;
    }
    if (this.secondaryAudioTrackGainNode != null) {
      this.secondaryAudioTrackGainNode.gain.value = volumeLevel;
    }
  }

  /**
   * Called by user
   * To create a sound meter for the local stream
   *
   * @param {*} levelCallback : callback to provide the audio level to user
   * @param {*} period : measurement period
   */
  enableAudioLevelForLocalStream(levelCallback, period) {
    this.levelCallback = levelCallback;
    this.localStreamSoundMeter = new SoundMeter(this.audioContext);
    this.localStreamSoundMeter.connectToSource(this.localStream, levelCallback).then(() => {
      this.audioContext.resume().then(r => {});
    });
  }

  /**
   * Connects the local stream to Sound Meter
   * It should be called when local stream changes
   */
  connectSoundMeterToLocalStream() {
    this.localStreamSoundMeter.connectToSource(this.localStream, this.levelCallback, function (e) {
      if (e) {
        alert(e);
      }
      // console.log("Added sound meter for stream: " + streamId + " = " + soundMeter.instant.toFixed(2));
    });
  }

  /**
   * Called by user
   * To change audio/video constraints on the fly
   *
   */
  applyConstraints(newConstraints) {
    var constraints = {};
    if (newConstraints.audio === undefined && newConstraints.video === undefined) {
      //if audio or video field is not defined, assume that it's a video constraint
      constraints.video = newConstraints;
      this.mediaConstraints.video = Object.assign({}, this.mediaConstraints.video, constraints.video);
    } else if (newConstraints.video !== undefined) {
      constraints.video = newConstraints.video;
      this.mediaConstraints.video = Object.assign({}, this.mediaConstraints.video, constraints.video);
    }
    if (newConstraints.audio !== undefined) {
      constraints.audio = newConstraints.audio;
      this.mediaConstraints.audio = Object.assign({}, this.mediaConstraints.audio, constraints.audio);
    }
    var promise = null;
    if (constraints.video !== undefined) {
      if (this.localStream && this.localStream.getVideoTracks().length > 0) {
        var videoTrack = this.localStream.getVideoTracks()[0];
        promise = videoTrack.applyConstraints(this.mediaConstraints.video);
      } else {
        promise = new Promise((resolve, reject) => {
          reject("There is no video track to apply constraints");
        });
      }
    }
    if (constraints.audio !== undefined) {
      //just give the audio constraints not to get video stream
      //we dont call applyContrains for audio because it does not work. I think this is due to gainStream things. This is why we call getUserMedia again

      //use the publishStreamId because we don't have streamId in the parameter anymore 
      promise = this.setAudioInputSource(this.publishStreamId, {
        audio: this.mediaConstraints.audio
      }, null);
    }
    if (this.localStreamSoundMeter != null) {
      this.connectSoundMeterToLocalStream();
    }
    return promise;
  }
}

/**
 * This structure is used to handle large size data channel messages (like image)
 * which should be splitted into chunks while sending and receiving.
 *
 */
class ReceivingMessage {
  constructor(size) {
    this.size = size;
    this.received = 0;
    this.data = new ArrayBuffer(size);
  }
}

/**
 * WebRTCAdaptor Class is interface to the JS SDK of Ant Media Server (AMS). This class manages the signalling,
 * keeps the states of peers.
 *
 * This class is used for peer-to-peer signalling,
 * publisher and player signalling and conference.
 *
 * Also it is responsible for some room management in conference case.
 *
 * There are different use cases in AMS. This class is used for all of them.
 *
 * WebRTC Publish
 * WebRTC Play
 * WebRTC Data Channel Connection
 * WebRTC Conference
 * WebRTC Multitrack Play
 * WebRTC Multitrack Conference
 * WebRTC peer-to-peer session
 *
 */
class WebRTCAdaptor {
  /**
   * Register plugins to the WebRTCAdaptor
   * @param {*} plugin
   */
  static register(pluginInitMethod) {
    WebRTCAdaptor.pluginInitMethods.push(pluginInitMethod);
  }
  constructor(initialValues) {
    /**
     * PeerConnection configuration while initializing the PeerConnection.
     * https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection/RTCPeerConnection#parameters
     *
     * More than one STURN and/or TURN servers can be added.  Here is a typical turn server configuration
     *
     *    {
     * 	  urls: "",
     *	  username: "",
     *    credential: "",
     *	}
     *
     *  Default value is the google stun server
     */
    this.peerconnection_config = {
      'iceServers': [{
        'urls': 'stun:stun1.l.google.com:19302'
      }],
      sdpSemantics: 'unified-plan'
    };

    /**
     * Used while creating SDP (answer or offer)
     * https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection/createOffer#parameters
     */
    this.sdp_constraints = {
      OfferToReceiveAudio: false,
      OfferToReceiveVideo: false
    };

    /**
     * This keeps the PeerConnections for each stream id.
     * It is an array because one @WebRTCAdaptor instance can manage multiple WebRTC connections as in the conference.
     * Its indices are the Stream Ids of each stream
     */
    this.remotePeerConnection = new Array();

    /**
     * This keeps statistics for the each PeerConnection.
     * It is an array because one @WebRTCAdaptor instance can manage multiple WebRTC connections as in the conference.
     * Its indices are the Stream Ids of each stream
     */
    this.remotePeerConnectionStats = new Array();

    /**
     * This keeps the Remote Description (SDP) set status for each PeerConnection.
     * We need to keep this status because sometimes ice candidates from the remote peer
     * may come before the Remote Description (SDP). So we need to store those ice candidates
     * in @iceCandidateList field until we get and set the Remote Description.
     * Otherwise setting ice candidates before Remote description may cause problem.
     */
    this.remoteDescriptionSet = new Array();

    /**
     * This keeps the Ice Candidates which are received before the Remote Description (SDP) received.
     * For details please check @remoteDescriptionSet field.
     */
    this.iceCandidateList = new Array();

    /**
     * This is the name for the room that is desired to join in conference mode.
     */
    this.roomName = null;

    /**
     * This keeps StreamIds for the each playing session.
     * It is an array because one @WebRTCAdaptor instance can manage multiple playing sessions.
     */
    this.playStreamId = new Array();

    /**
     * Audio context to use
     */
    this.audioContext = new AudioContext();

    /**
     * This is the flag indicates if multiple peers will join a peer in the peer to peer mode.
     * This is used only with Embedded SDk
     */
    this.isMultiPeer = false;

    /**
     * This is the stream id that multiple peers can join a peer in the peer to peer mode.
     * This is used only with Embedded SDk
     */
    this.multiPeerStreamId = null;

    /**
     * This is instance of @WebSocketAdaptor and manages to websocket connection.
     * All signalling messages are sent to/recived from
     * the Ant Media Server over this web socket connection
     */
    this.webSocketAdaptor = null;

    /**
     * This flags indicates if this @WebRTCAdaptor instance is used only for playing session(s)
     * You don't need camera/mic access in play mode
     */
    this.isPlayMode = false;

    /**
     * This flags enables/disables debug logging
     */
    this.debug = false;

    /**
     * This is the Stream Id for the publisher. One @WebRCTCAdaptor supports only one publishing
     * session for now (23.02.2022).
     * In conference mode you can join a room with null stream id. In that case
     * Ant Media Server generates a stream id and provides it JoinedTheRoom callback and it is set to this field.
     */
    this.publishStreamId = null;

    /**
     * This is used to keep stream id and track id (which is provided in SDP) mapping
     * in MultiTrack Playback and conference.
     */
    this.idMapping = new Array();

    /**
     * This is used when only data is brodcasted with the same way video and/or audio.
     * The difference is that no video or audio is sent when this field is true
     */
    this.onlyDataChannel = false;

    /**
     * While publishing and playing streams data channel is enabled by default
     */
    this.dataChannelEnabled = true;

    /**
     * This is array of @ReceivingMessage
     * When you receive multiple large size messages @ReceivingMessage simultaneously
     * this map is used to indicate them with its index tokens.
     */
    this.receivingMessages = new Map();

    /**
     * Supported candidate types. Below types are for both sending and receiving candidates.
     * It means if when client receives candidate from STUN server, it sends to the server if candidate's protocol
     * is in the list. Likely, when client receives remote candidate from server, it adds as ice candidate
     * if candidate protocol is in the list below.
     */
    this.candidateTypes = ["udp", "tcp"];

    /**
     * Method to call when there is an event happened
     */
    this.callback = null;

    /**
     * Method to call when there is an error happened
     */
    this.callbackError = null;

    /**
     * Flag to indicate if the stream is published or not after the connection fails
     */
    this.reconnectIfRequiredFlag = true;

    /**
     * PAY ATTENTION: The values of the above fields are provided as this constructor parameter.
     * TODO: Also some other hidden parameters may be passed here
     */
    for (var key in initialValues) {
      if (initialValues.hasOwnProperty(key)) {
        this[key] = initialValues[key];
      }
    }

    /**
     * The html video tag for receiver is got here
     */
    this.remoteVideo = this.remoteVideoElement || document.getElementById(this.remoteVideoId);

    /**
     * Keeps the sound meters for each connection. Its index is stream id
     */
    this.soundMeters = new Array();

    /**
     * Keeps the current audio level for each playing streams in conference mode
     */
    this.soundLevelList = new Array();

    /**
     * This is the event listeners that WebRTC Adaptor calls when there is a new event happened
     */
    this.eventListeners = new Array();

    /**
     * This is the error event listeners that WebRTC Adaptor calls when there is an error happened
     */
    this.errorEventListeners = new Array();

    /**
     * This is token that is being used to publish the stream. It's added here to use in reconnect scenario
     */
    this.publishToken = null;

    /**
     * subscriber id that is being used to publish the stream. It's added here to use in reconnect scenario
     */
    this.publishSubscriberId = null;

    /**
     * subscriber code that is being used to publish the stream. It's added here to use in reconnect scenario
     */
    this.publishSubscriberCode = null;

    /**
     * This is the stream name that is being published. It's added here to use in reconnect scenario
     */
    this.publishStreamName = null;

    /**
     * This is the stream id of the main track that the current publishStreamId is going to be subtrack of it. It's added here to use in reconnect scenario
     */
    this.publishMainTrack = null;

    /**
     * This is the metadata that is being used to publish the stream. It's added here to use in reconnect scenario
     */
    this.publishMetaData = null;

    /**
     * This is the token to play the stream. It's added here to use in reconnect scenario
     */
    this.playToken = null;

    /**
     * This is the room id to play the stream. It's added here to use in reconnect scenario
     * This approach is old conferencing. It's better to use multi track conferencing
     */
    this.playRoomId = null;

    /**
     * These are enabled tracks to play the stream. It's added here to use in reconnect scenario
     */
    this.playEnableTracks = null;

    /**
     * This is the subscriber Id to play the stream. It's added here to use in reconnect scenario
     */
    this.playSubscriberId = null;

    /**
     * This is the subscriber code to play the stream. It's added here to use in reconnect scenario
     */
    this.playSubscriberCode = null;

    /**
     * This is the meta data to play the stream. It's added here to use in reconnect scenario
     */
    this.playMetaData = null;

    /**
     * All media management works for teh local stream are made by @MediaManager class.
     * for details please check @MediaManager
     */
    this.mediaManager = new MediaManager({
      userParameters: initialValues,
      webRTCAdaptor: this,
      callback: (info, obj) => {
        this.notifyEventListeners(info, obj);
      },
      callbackError: (error, message) => {
        this.notifyErrorEventListeners(error, message);
      },
      getSender: (streamId, type) => {
        return this.getSender(streamId, type);
      }
    });

    //Initialize the local stream (if needed) and web socket connection
    this.initialize();
  }

  /**
   * Init plugins
   */
  initPlugins() {
    WebRTCAdaptor.pluginInitMethods.forEach(initMethod => {
      initMethod(this);
    });
  }

  /**
   * Add event listener to be notified. This is generally for plugins
   * @param {*} listener
   */
  addEventListener(listener) {
    this.eventListeners.push(listener);
  }

  /**
   * Add error event listener to be notified. Thisis generally for plugins
   * @param {*} errorListener
   */
  addErrorEventListener(errorListener) {
    this.errorEventListeners.push(errorListener);
  }

  /**
   * Notify event listeners and callback method
   * @param {*} info
   * @param {*} obj
   */
  notifyEventListeners(info, obj) {
    this.eventListeners.forEach(listener => {
      listener(info, obj);
    });
    if (this.callback != null) {
      this.callback(info, obj);
    }
  }

  /**
   * Notify error event listeners and callbackError method
   * @param {*} error
   * @param {*} message
   */
  notifyErrorEventListeners(error, message) {
    this.errorEventListeners.forEach(listener => {
      listener(error, message);
    });
    if (this.callbackError != null) {
      this.callbackError(error, message);
    }
  }

  /**
   * Called by constuctor to
   *    -check local stream unless it is in play mode
   *    -start websocket connection
   */
  initialize() {
    if (!this.isPlayMode && !this.onlyDataChannel && this.mediaManager.localStream == null) {
      //we need local stream because it not a play mode
      this.mediaManager.initLocalStream().then(() => {
        this.initPlugins();
        this.checkWebSocketConnection();
      }).catch(error => {
        console.warn(error);
        throw error;
      });
      //return here because initialized message should be delivered after local stream is initialized
      return;
    }
    this.initPlugins();
    this.checkWebSocketConnection();
  }

  /**
   * Called to start a new WebRTC stream. AMS responds with start message.
   * Parameters:
   *     streamId: unique id for the stream
   *     token: required if any stream security (token control) enabled. Check https://github.com/ant-media/Ant-Media-Server/wiki/Stream-Security-Documentation
   *     subscriberId: required if TOTP enabled. Check https://github.com/ant-media/Ant-Media-Server/wiki/Time-based-One-Time-Password-(TOTP)
   *     subscriberCode: required if TOTP enabled. Check https://github.com/ant-media/Ant-Media-Server/wiki/Time-based-One-Time-Password-(TOTP)
   *   streamName: required if you want to set a name for the stream
   *   mainTrack: required if you want to start the stream as a subtrack for a main streamwhich has id of this parameter.
   *                Check:https://antmedia.io/antmediaserver-webrtc-multitrack-playing-feature/
   *                !!! for multitrack conference set this value with roomName
   *   metaData: a free text information for the stream to AMS. It is provided to Rest methods by the AMS
   */
  publish(streamId, token, subscriberId, subscriberCode, streamName, mainTrack, metaData) {
    //TODO: should refactor the repeated code
    this.publishStreamId = streamId;
    this.mediaManager.publishStreamId = streamId;
    this.publishToken = token;
    this.publishSubscriberId = subscriberId;
    this.publishSubscriberCode = subscriberCode;
    this.publishStreamName = streamName;
    this.publishMainTrack = mainTrack;
    this.publishMetaData = metaData;
    if (this.onlyDataChannel) {
      this.sendPublishCommand(streamId, token, subscriberId, subscriberCode, streamName, mainTrack, metaData, false, false);
    }
    //If it started with playOnly mode and wants to publish now
    else if (this.mediaManager.localStream == null) {
      this.mediaManager.initLocalStream().then(() => {
        var videoEnabled = false;
        var audioEnabled = false;
        if (this.mediaManager.localStream != null) {
          videoEnabled = this.mediaManager.localStream.getVideoTracks().length > 0 ? true : false;
          audioEnabled = this.mediaManager.localStream.getAudioTracks().length > 0 ? true : false;
        }
        this.sendPublishCommand(streamId, token, subscriberId, subscriberCode, streamName, mainTrack, metaData, videoEnabled, audioEnabled);
      }).catch(error => {
        console.warn(error);
        throw error;
      });
    } else {
      var videoEnabled = this.mediaManager.localStream.getVideoTracks().length > 0 ? true : false;
      var audioEnabled = this.mediaManager.localStream.getAudioTracks().length > 0 ? true : false;
      this.sendPublishCommand(streamId, token, subscriberId, subscriberCode, streamName, mainTrack, metaData, videoEnabled, audioEnabled);
    }
    setTimeout(() => {
      //check if it is connected or not
      //this resolves if the server responds with some error message
      if (this.iceConnectionState(this.publishStreamId) != "connected" && this.iceConnectionState(this.publishStreamId) != "completed") {
        //if it is not connected, try to reconnect
        this.reconnectIfRequired();
      }
    }, 5000);
  }
  sendPublishCommand(streamId, token, subscriberId, subscriberCode, streamName, mainTrack, metaData, videoEnabled, audioEnabled) {
    var jsCmd = {
      command: "publish",
      streamId: streamId,
      token: token,
      subscriberId: typeof subscriberId !== undefined && subscriberId != null ? subscriberId : "",
      subscriberCode: typeof subscriberCode !== undefined && subscriberCode != null ? subscriberCode : "",
      streamName: typeof streamName !== undefined && streamName != null ? streamName : "",
      mainTrack: typeof mainTrack !== undefined && mainTrack != null ? mainTrack : "",
      video: videoEnabled,
      audio: audioEnabled,
      metaData: typeof metaData !== undefined && metaData != null ? metaData : ""
    };
    this.webSocketAdaptor.send(JSON.stringify(jsCmd));
  }

  /**
   * Called to join a room. AMS responds with joinedTheRoom message.
   * Parameters:
   *     roomName: unique id of the room
   *     stream: unique id of the stream belogns to this participant
   *     mode:    legacy for older implementation (default value)
   *            mcu for merging streams
   *            amcu: audio only conferences with mixed audio
   */
  joinRoom(roomName, streamId, mode) {
    this.roomName = roomName;
    var jsCmd = {
      command: "joinRoom",
      room: roomName,
      streamId: streamId,
      mode: mode
    };
    this.webSocketAdaptor.send(JSON.stringify(jsCmd));
  }

  /**
   * Called to start a playing session for a stream. AMS responds with start message.
   * Parameters:
   *  - streamId:(string) unique id for the stream that you want to play
   *  - token:(string) required if any stream security (token control) enabled. Check https://github.com/ant-media/Ant-Media-Server/wiki/Stream-Security-Documentation
   *  - roomId:(string) required if this stream is belonging to a room participant
   *  - enableTracks:(array) required if the stream is a main stream of multitrack playing. You can pass the the subtrack id list that you want to play.
   *                    you can also provide a track id that you don't want to play by adding ! before the id.
   *   - subscriberId:(string) required if TOTP enabled. Check https://github.com/ant-media/Ant-Media-Server/wiki/Time-based-One-Time-Password-(TOTP)
   *   - subscriberCode:(string) required if TOTP enabled. Check https://github.com/ant-media/Ant-Media-Server/wiki/Time-based-One-Time-Password-(TOTP)
   *   - metaData:(string, json) a free text information for the stream to AMS. It is provided to Rest methods by the AMS
   */
  play(streamId, token, roomId, enableTracks, subscriberId, subscriberCode, metaData) {
    this.playStreamId.push(streamId);
    this.playToken = token;
    this.playRoomId = roomId;
    this.playEnableTracks = enableTracks;
    this.playSubscriberId = subscriberId;
    this.playSubscriberCode = subscriberCode;
    this.playMetaData = metaData;
    var jsCmd = {
      command: "play",
      streamId: streamId,
      token: typeof token !== undefined && token != null ? token : "",
      room: typeof roomId !== undefined && roomId != null ? roomId : "",
      trackList: typeof enableTracks !== undefined && enableTracks != null ? enableTracks : [],
      subscriberId: typeof subscriberId !== undefined && subscriberId != null ? subscriberId : "",
      subscriberCode: typeof subscriberCode !== undefined && subscriberId != null ? subscriberCode : "",
      viewerInfo: typeof metaData !== undefined && metaData != null ? metaData : ""
    };
    this.webSocketAdaptor.send(JSON.stringify(jsCmd));
    setTimeout(() => {
      //check if it is connected or not
      //this resolves if the server responds with some error message
      if (this.iceConnectionState(streamId) != "connected" && this.iceConnectionState(streamId) != "completed") {
        //if it is not connected, try to reconnect
        this.reconnectIfRequired();
      }
    }, 5000);
  }

  /**
   * Reconnects to the stream if it is not stopped on purpose
   * @param {*} streamId 
   * @returns 
   */
  reconnectIfRequired() {
    if (this.reconnectIfRequiredFlag) {
      //if remotePeerConnection has a peer connection for the stream id, it means that it is not stopped on purpose

      //reconnect publish
      if (this.remotePeerConnection[this.publishStreamId] != null) {
        this.closePeerConnection(streamId);
        console.log("It will try to publish again because it is not stopped on purpose");
        setTimeout(() => {
          this.publish(this.publishStreamId, this.publishToken, this.publishSubscriberId, this.publishSubscriberCode, this.publishStreamName, this.publishMainTrack, this.publishMetaData);
        }, 3000);
      }

      //reconnect play
      for (var streamId in this.playStreamId) {
        if (this.remotePeerConnection[streamId] != null) {
          console.log("It will try to play again because it is not stopped on purpose");
          this.closePeerConnection(streamId);
          setTimeout(() => {
            this.play(streamId, this.playToken, this.playRoomId, this.playEnableTracks, this.playSubscriberId, this.playSubscriberCode, this.playMetaData);
          }, 3000);
        }
      }
    }
  }

  /**
   * Called to stop a publishing/playing session for a stream. AMS responds with publishFinished or playFinished message.
   * Parameters:
   *     streamId: unique id for the stream that you want to stop publishing or playing
   */
  stop(streamId) {
    //stop is called on purpose and it deletes the peer connection from remotePeerConnections
    this.closePeerConnection(streamId);
    var jsCmd = {
      command: "stop",
      streamId: streamId
    };
    this.webSocketAdaptor.send(JSON.stringify(jsCmd));
  }

  /**
   * Called to join a peer-to-peer mode session as peer. AMS responds with joined message.
   * Parameters:
   *     streamId: unique id for the peer-to-peer session
   */
  join(streamId) {
    var jsCmd = {
      command: "join",
      streamId: streamId,
      multiPeer: this.isMultiPeer && this.multiPeerStreamId == null,
      mode: this.isPlayMode ? "play" : "both"
    };
    this.webSocketAdaptor.send(JSON.stringify(jsCmd));
  }

  /**
   * Called by browser when a new track is added to WebRTC connetion. This is used to infor html pages with newStreamAvailable callback.
   * Parameters:
   * 	 event: TODO
   * 	 streamId: unique id for the stream
   */
  onTrack(event, streamId) {
    console.log("onTrack for stream");
    if (this.remoteVideo != null) {
      if (this.remoteVideo.srcObject !== event.streams[0]) {
        this.remoteVideo.srcObject = event.streams[0];
        console.log('Received remote stream');
      }
    } else {
      var dataObj = {
        stream: event.streams[0],
        track: event.track,
        streamId: streamId,
        trackId: this.idMapping[streamId][event.transceiver.mid]
      };
      this.notifyEventListeners("newTrackAvailable", dataObj);

      //deprecated. Listen newTrackAvailable in callback. It's kept for backward compatibility
      this.notifyEventListeners("newStreamAvailable", dataObj);
    }
  }

  /**
   * Called to leave from a conference room. AMS responds with leavedTheRoom message.
   * Parameters:
   *     roomName: unique id for the conference room
   */
  leaveFromRoom(roomName) {
    for (var key in this.remotePeerConnection) {
      this.closePeerConnection(key);
    }
    this.roomName = roomName;
    var jsCmd = {
      command: "leaveFromRoom",
      room: roomName
    };
    console.log("leave request is sent for " + roomName);
    this.webSocketAdaptor.send(JSON.stringify(jsCmd));
  }

  /**
   * Called to leave from a peer-to-peer mode session. AMS responds with leaved message.
   * Parameters:
   *     streamId: unique id for the peer-to-peer session
   */
  leave(streamId) {
    var jsCmd = {
      command: "leave",
      streamId: this.isMultiPeer && this.multiPeerStreamId != null ? this.multiPeerStreamId : streamId
    };
    this.webSocketAdaptor.send(JSON.stringify(jsCmd));
    this.closePeerConnection(streamId);
    this.multiPeerStreamId = null;
  }

  /**
   * Called to get a stream information for a specific stream. AMS responds with streamInformation message.
   * Parameters:
   *     streamId: unique id for the stream that you want to get info about
   */
  getStreamInfo(streamId) {
    var jsCmd = {
      command: "getStreamInfo",
      streamId: streamId
    };
    this.webSocketAdaptor.send(JSON.stringify(jsCmd));
  }

  /**
   * Called to update the meta information for a specific stream.
   * Parameters:
   *     streamId: unique id for the stream that you want to update MetaData
   *   metaData: new free text information for the stream
   */
  upateStreamMetaData(streamId, metaData) {
    var jsCmd = {
      command: "updateStreamMetaData",
      streamId: streamId,
      metaData: metaData
    };
    this.webSocketAdaptor.send(JSON.stringify(jsCmd));
  }

  /**
   * Called to get the room information for a specific room. AMS responds with roomInformation message
   * which includes the ids and names of the streams in that room.
   * If there is no active streams in the room, AMS returns error `no_active_streams_in_room` in error callback
   * Parameters:
   *     roomName: unique id for the room that you want to get info about
   *     streamId: unique id for the stream that is streamed by this @WebRTCAdaptor
   */
  getRoomInfo(roomName, streamId) {
    var jsCmd = {
      command: "getRoomInfo",
      streamId: streamId,
      room: roomName
    };
    this.webSocketAdaptor.send(JSON.stringify(jsCmd));
  }

  /**
   * Called to enable/disable data flow from the AMS for a specific track under a main track.
   * Parameters:
   *     mainTrackId: unique id for the main stream
   *     trackId: unique id for the track that you want to enable/disable data flow for
   *     enabled: true or false
   */
  enableTrack(mainTrackId, trackId, enabled) {
    var jsCmd = {
      command: "enableTrack",
      streamId: mainTrackId,
      trackId: trackId,
      enabled: enabled
    };
    this.webSocketAdaptor.send(JSON.stringify(jsCmd));
  }

  /**
   * Called to get the track ids under a main stream. AMS responds with trackList message.
   * Parameters:
   *     streamId: unique id for the main stream
   *     token: not used
   * TODO: check this function
   */
  getTracks(streamId, token) {
    this.playStreamId.push(streamId);
    var jsCmd = {
      command: "getTrackList",
      streamId: streamId,
      token: token
    };
    this.webSocketAdaptor.send(JSON.stringify(jsCmd));
  }

  /**
   * Called by WebSocketAdaptor when a new ice candidate is received from AMS.
   * Parameters:
   *     event: TODO
   *     streamId: unique id for the stream
   */
  iceCandidateReceived(event, streamId) {
    if (event.candidate) {
      var protocolSupported = false;
      if (event.candidate.candidate == "") {
        //event candidate can be received and its value can be "".
        //don't compare the protocols
        protocolSupported = true;
      } else if (typeof event.candidate.protocol == "undefined") {
        this.candidateTypes.forEach(element => {
          if (event.candidate.candidate.toLowerCase().includes(element)) {
            protocolSupported = true;
          }
        });
      } else {
        protocolSupported = this.candidateTypes.includes(event.candidate.protocol.toLowerCase());
      }
      if (protocolSupported) {
        var jsCmd = {
          command: "takeCandidate",
          streamId: streamId,
          label: event.candidate.sdpMLineIndex,
          id: event.candidate.sdpMid,
          candidate: event.candidate.candidate
        };
        if (this.debug) {
          console.log("sending ice candiate for stream Id " + streamId);
          console.log(JSON.stringify(event.candidate));
        }
        this.webSocketAdaptor.send(JSON.stringify(jsCmd));
      } else {
        console.log("Candidate's protocol(full sdp: " + event.candidate.candidate + ") is not supported. Supported protocols: " + this.candidateTypes);
        if (event.candidate.candidate != "") {
          //
          this.notifyErrorEventListeners("protocol_not_supported", "Support protocols: " + this.candidateTypes.toString() + " candidate: " + event.candidate.candidate);
        }
      }
    } else {
      console.log("No event.candidate in the iceCandidate event");
    }
  }

  /**
   * Called internally to initiate Data Channel.
   * Note that Data Channel should be enabled fromAMS settings.
   *     streamId: unique id for the stream
   *   dataChannel: provided by PeerConnection
   */
  initDataChannel(streamId, dataChannel) {
    dataChannel.onerror = error => {
      console.log("Data Channel Error:", error);
      var obj = {
        streamId: streamId,
        error: error
      };
      console.log("channel status: ", dataChannel.readyState);
      if (dataChannel.readyState != "closed") {
        this.notifyErrorEventListeners("data_channel_error", obj);
      }
    };
    dataChannel.onmessage = event => {
      var obj = {
        streamId: streamId,
        data: event.data
      };
      var data = obj.data;
      if (typeof data === 'string' || data instanceof String) {
        this.notifyEventListeners("data_received", obj);
      } else {
        var length = data.length || data.size || data.byteLength;
        var view = new Int32Array(data, 0, 1);
        var token = view[0];
        var msg = this.receivingMessages[token];
        if (msg == undefined) {
          var view = new Int32Array(data, 0, 2);
          var size = view[1];
          msg = new ReceivingMessage(size);
          this.receivingMessages[token] = msg;
          if (length > 8) {
            console.error("something went wrong in msg receiving");
          }
          return;
        }
        var rawData = data.slice(4, length);
        var dataView = new Uint8Array(msg.data);
        dataView.set(new Uint8Array(rawData), msg.received, length - 4);
        msg.received += length - 4;
        if (msg.size == msg.received) {
          obj.data = msg.data;
          this.notifyEventListeners("data_received", obj);
        }
      }
    };
    dataChannel.onopen = () => {
      this.remotePeerConnection[streamId].dataChannel = dataChannel;
      console.log("Data channel is opened");
      this.notifyEventListeners("data_channel_opened", streamId);
    };
    dataChannel.onclose = () => {
      console.log("Data channel is closed");
      this.notifyEventListeners("data_channel_closed", streamId);
    };
  }

  /**
   * Called internally to initiate PeerConnection.
   *     streamId: unique id for the stream
   *   dataChannelMode: can be "publish" , "play" or "peer" based on this it is decided which way data channel is created
   */
  initPeerConnection(streamId, dataChannelMode) {
    if (this.remotePeerConnection[streamId] == null) {
      var closedStreamId = streamId;
      console.log("stream id in init peer connection: " + streamId + " close stream id: " + closedStreamId);
      this.remotePeerConnection[streamId] = new RTCPeerConnection(this.peerconnection_config);
      this.remoteDescriptionSet[streamId] = false;
      this.iceCandidateList[streamId] = new Array();
      if (!this.playStreamId.includes(streamId)) {
        if (this.mediaManager.localStream != null) {
          //AddStream is deprecated thus updated to the addTrack after version 2.4.2.1
          this.mediaManager.localStream.getTracks().forEach(track => this.remotePeerConnection[streamId].addTrack(track, this.mediaManager.localStream));
        }
      }
      this.remotePeerConnection[streamId].onicecandidate = event => {
        this.iceCandidateReceived(event, closedStreamId);
      };
      this.remotePeerConnection[streamId].ontrack = event => {
        this.onTrack(event, closedStreamId);
      };
      this.remotePeerConnection[streamId].onnegotiationneeded = event => {
        console.log("onnegotiationneeded");
      };
      if (this.dataChannelEnabled) {
        // skip initializing data channel if it is disabled
        if (dataChannelMode == "publish") {
          //open data channel if it's publish mode peer connection
          var dataChannelOptions = {
            ordered: true
          };
          if (this.remotePeerConnection[streamId].createDataChannel) {
            var dataChannel = this.remotePeerConnection[streamId].createDataChannel(streamId, dataChannelOptions);
            this.initDataChannel(streamId, dataChannel);
          } else {
            console.warn("CreateDataChannel is not supported");
          }
        } else if (dataChannelMode == "play") {
          //in play mode, server opens the data channel
          this.remotePeerConnection[streamId].ondatachannel = ev => {
            this.initDataChannel(streamId, ev.channel);
          };
        } else {
          //for peer mode do both for now
          var _dataChannelOptions = {
            ordered: true
          };
          if (this.remotePeerConnection[streamId].createDataChannel) {
            var dataChannelPeer = this.remotePeerConnection[streamId].createDataChannel(streamId, _dataChannelOptions);
            this.initDataChannel(streamId, dataChannelPeer);
            this.remotePeerConnection[streamId].ondatachannel = ev => {
              this.initDataChannel(streamId, ev.channel);
            };
          } else {
            console.warn("CreateDataChannel is not supported");
          }
        }
      }
      this.remotePeerConnection[streamId].oniceconnectionstatechange = event => {
        var obj = {
          state: this.remotePeerConnection[streamId].iceConnectionState,
          streamId: streamId
        };
        if (obj.state == "failed" || obj.state == "disconnected" || obj.state == "closed") {
          this.reconnectIfRequired(obj.streamId);
        }
        this.notifyEventListeners("ice_connection_state_changed", obj);

        //
        if (!this.isPlayMode && !this.playStreamId.includes(streamId)) {
          if (this.remotePeerConnection[streamId].iceConnectionState == "connected") {
            this.mediaManager.changeBandwidth(this.mediaManager.bandwidth, streamId).then(() => {
              console.log("Bandwidth is changed to " + this.mediaManager.bandwidth);
            }).catch(e => console.warn(e));
          }
        }
      };
    }
  }

  /**
   * Called internally to close PeerConnection.
   *     streamId: unique id for the stream
   */
  closePeerConnection(streamId) {
    var peerConnection = this.remotePeerConnection[streamId];
    if (peerConnection != null) {
      this.remotePeerConnection[streamId] = null;
      delete this.remotePeerConnection[streamId];
      if (peerConnection.dataChannel != null) {
        peerConnection.dataChannel.close();
      }
      if (peerConnection.signalingState != "closed") {
        peerConnection.close();
        var playStreamIndex = this.playStreamId.indexOf(streamId);
        if (playStreamIndex != -1) {
          this.playStreamId.splice(playStreamIndex, 1);
        }
      }
    }

    //this is for the stats
    if (this.remotePeerConnectionStats[streamId] != null) {
      clearInterval(this.remotePeerConnectionStats[streamId].timerId);
      delete this.remotePeerConnectionStats[streamId];
    }
    if (this.soundMeters[streamId] != null) {
      delete this.soundMeters[streamId];
    }
  }

  /**
   * Called to get the signalling state for a stream.
   * This information can be used for error handling.
   * Check: https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection/connectionState
   *     streamId: unique id for the stream
   */
  signallingState(streamId) {
    if (this.remotePeerConnection[streamId] != null) {
      return this.remotePeerConnection[streamId].signalingState;
    }
    return null;
  }

  /**
   * Called to get the ice connection state for a stream.
   * This information can be used for error handling.
   * Check: https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection/iceConnectionState
   *     streamId: unique id for the stream
   */
  iceConnectionState(streamId) {
    if (this.remotePeerConnection[streamId] != null) {
      return this.remotePeerConnection[streamId].iceConnectionState;
    }
    return null;
  }

  /**
   * Called by browser when Local Configuration (SDP) is created successfully.
   * It is set as LocalDescription first then sent to AMS.
   *     configuration: created Local Configuration (SDP)
   *     streamId: unique id for the stream
   */
  gotDescription(configuration, streamId) {
    this.remotePeerConnection[streamId].setLocalDescription(configuration).then(responose => {
      console.debug("Set local description successfully for stream Id " + streamId);
      var jsCmd = {
        command: "takeConfiguration",
        streamId: streamId,
        type: configuration.type,
        sdp: configuration.sdp
      };
      if (this.debug) {
        console.debug("local sdp: ");
        console.debug(configuration.sdp);
      }
      this.webSocketAdaptor.send(JSON.stringify(jsCmd));
    }).catch(error => {
      console.error("Cannot set local description. Error is: " + error);
    });
  }

  /**
   * Called by WebSocketAdaptor when Remote Configuration (SDP) is received from AMS.
   * It is set as RemoteDescription first then if @iceCandidateList has candidate that
   * is received bfore this message, it is added as ice candidate.
   *     configuration: received Remote Configuration (SDP)
   *     idOfStream: unique id for the stream
   *     typeOfConfiguration: unique id for the stream
   *     idMapping: stream id and track id (which is provided in SDP) mapping in MultiTrack Playback and conference.
   *                It is recorded to match stream id as new tracks are added with @onTrack
   */
  takeConfiguration(idOfStream, configuration, typeOfConfiguration, idMapping) {
    var streamId = idOfStream;
    var type = typeOfConfiguration;
    var conf = configuration;
    var isTypeOffer = type == "offer";
    var dataChannelMode = "publish";
    if (isTypeOffer) {
      dataChannelMode = "play";
    }
    this.idMapping[streamId] = idMapping;
    this.initPeerConnection(streamId, dataChannelMode);
    this.remotePeerConnection[streamId].setRemoteDescription(new RTCSessionDescription({
      sdp: conf,
      type: type
    })).then(response => {
      if (this.debug) {
        console.debug("set remote description is succesfull with response: " + response + " for stream : " + streamId + " and type: " + type);
        console.debug(conf);
      }
      this.remoteDescriptionSet[streamId] = true;
      var length = this.iceCandidateList[streamId].length;
      console.debug("Ice candidate list size to be added: " + length);
      for (var i = 0; i < length; i++) {
        this.addIceCandidate(streamId, this.iceCandidateList[streamId][i]);
      }
      this.iceCandidateList[streamId] = [];
      if (isTypeOffer) {
        //SDP constraints may be different in play mode
        console.log("try to create answer for stream id: " + streamId);
        this.remotePeerConnection[streamId].createAnswer(this.sdp_constraints).then(configuration => {
          console.log("created answer for stream id: " + streamId);
          //support for stereo
          configuration.sdp = configuration.sdp.replace("useinbandfec=1", "useinbandfec=1; stereo=1");
          this.gotDescription(configuration, streamId);
        }).catch(error => {
          console.error("create answer error :" + error);
        });
      }
    }).catch(error => {
      if (this.debug) {
        console.error("set remote description is failed with error: " + error);
      }
      if (error.toString().indexOf("InvalidAccessError") > -1 || error.toString().indexOf("setRemoteDescription") > -1) {
        /**
         * This error generally occurs in codec incompatibility.
         * AMS for a now supports H.264 codec. This error happens when some browsers try to open it from VP8.
         */
        this.notifyErrorEventListeners("notSetRemoteDescription");
      }
    });
  }

  /**
   * Called by WebSocketAdaptor when new ice candidate is received from AMS.
   * If Remote Description (SDP) is already set, the candidate is added immediately,
   * otherwise stored in @iceCandidateList to add after Remote Description (SDP) set.
   *     idOfTheStream: unique id for the stream
   *     tmpLabel: sdpMLineIndex
   *     tmpCandidate: ice candidate
   */
  takeCandidate(idOfTheStream, tmpLabel, tmpCandidate) {
    var streamId = idOfTheStream;
    var label = tmpLabel;
    var candidateSdp = tmpCandidate;
    var candidate = new RTCIceCandidate({
      sdpMLineIndex: label,
      candidate: candidateSdp
    });
    var dataChannelMode = "peer";
    this.initPeerConnection(streamId, dataChannelMode);
    if (this.remoteDescriptionSet[streamId] == true) {
      this.addIceCandidate(streamId, candidate);
    } else {
      console.debug("Ice candidate is added to list because remote description is not set yet");
      this.iceCandidateList[streamId].push(candidate);
    }
  }
  /**
   * Called internally to add the Ice Candidate to PeerConnection
   *     streamId: unique id for the stream
   *     tmpCandidate: ice candidate
   */
  addIceCandidate(streamId, candidate) {
    var protocolSupported = false;
    if (candidate.candidate == "") {
      //candidate can be received and its value can be "".
      //don't compare the protocols
      protocolSupported = true;
    } else if (typeof candidate.protocol == "undefined") {
      this.candidateTypes.forEach(element => {
        if (candidate.candidate.toLowerCase().includes(element)) {
          protocolSupported = true;
        }
      });
    } else {
      protocolSupported = this.candidateTypes.includes(candidate.protocol.toLowerCase());
    }
    if (protocolSupported) {
      this.remotePeerConnection[streamId].addIceCandidate(candidate).then(response => {
        if (this.debug) {
          console.log("Candidate is added for stream " + streamId);
        }
      }).catch(error => {
        console.error("ice candiate cannot be added for stream id: " + streamId + " error is: " + error);
        console.error(candidate);
      });
    } else {
      if (this.debug) {
        console.log("Candidate's protocol(" + candidate.protocol + ") is not supported." + "Candidate: " + candidate.candidate + " Supported protocols:" + this.candidateTypes);
      }
    }
  }
  /**
   * Called by WebSocketAdaptor when start message is received //TODO: may be changed. this logic shouldn't be in WebSocketAdaptor
   *     idOfStream: unique id for the stream
   */
  startPublishing(idOfStream) {
    var streamId = idOfStream;
    this.initPeerConnection(streamId, "publish");
    this.remotePeerConnection[streamId].createOffer(this.sdp_constraints).then(configuration => {
      this.gotDescription(configuration, streamId);
    }).catch(error => {
      console.error("create offer error for stream id: " + streamId + " error: " + error);
    });
  }
  /**
   * Toggle video track on the server side.
   *
   *   streamId: is the id of the stream
   *   trackId: is the id of the track. streamId is also one of the trackId of the stream. If you are having just a single track on your
   *         stream, you need to give streamId as trackId parameter as well.
   *   enabled: is the enable/disable video track. If it's true, server sends video track. If it's false, server does not send video
   */
  toggleVideo(streamId, trackId, enabled) {
    var jsCmd = {
      command: "toggleVideo",
      streamId: streamId,
      trackId: trackId,
      enabled: enabled
    };
    this.webSocketAdaptor.send(JSON.stringify(jsCmd));
  }

  /**
   * Toggle audio track on the server side.
   *
   *   streamId: is the id of the stream
   *   trackId: is the id of the track. streamId is also one of the trackId of the stream. If you are having just a single track on your
   *            stream, you need to give streamId as trackId parameter as well.
   *   enabled: is the enable/disable video track. If it's true, server sends audio track. If it's false, server does not send audio
   *
   */
  toggleAudio(streamId, trackId, enabled) {
    var jsCmd = {
      command: "toggleAudio",
      streamId: streamId,
      trackId: trackId,
      enabled: enabled
    };
    this.webSocketAdaptor.send(JSON.stringify(jsCmd));
  }

  /**
   * Called to get statistics for a PeerConnection. It can be publisher or player.
   *
   *     streamId: unique id for the stream
   */
  getStats(streamId) {
    console.log("peerstatsgetstats = " + this.remotePeerConnectionStats[streamId]);
    this.remotePeerConnection[streamId].getStats(null).then(stats => {
      var bytesReceived = -1;
      var videoPacketsLost = -1;
      var audioPacketsLost = -1;
      var fractionLost = -1;
      var currentTime = -1;
      var bytesSent = -1;
      var videoPacketsSent = -1;
      var audioPacketsSent = -1;
      var audioLevel = -1;
      var qlr = "";
      var framesEncoded = -1;
      var width = -1;
      var height = -1;
      var fps = -1;
      var frameWidth = -1;
      var frameHeight = -1;
      var videoRoundTripTime = -1;
      var videoJitter = -1;
      var audioRoundTripTime = -1;
      var audioJitter = -1;
      var framesDecoded = -1;
      var framesDropped = -1;
      var framesReceived = -1;
      var audioJitterAverageDelay = -1;
      var videoJitterAverageDelay = -1;
      stats.forEach(value => {
        //console.log(value);

        if (value.type == "inbound-rtp" && typeof value.kind != "undefined") {
          bytesReceived += value.bytesReceived;
          if (value.kind == "audio") {
            audioPacketsLost = value.packetsLost;
          } else if (value.kind == "video") {
            videoPacketsLost = value.packetsLost;
          }
          fractionLost += value.fractionLost;
          currentTime = value.timestamp;
        } else if (value.type == "outbound-rtp") {
          //TODO: SPLIT AUDIO AND VIDEO BITRATES
          if (value.kind == "audio") {
            audioPacketsSent = value.packetsSent;
          } else if (value.kind == "video") {
            videoPacketsSent = value.packetsSent;
          }
          bytesSent += value.bytesSent;
          currentTime = value.timestamp;
          qlr = value.qualityLimitationReason;
          if (value.framesEncoded != null) {
            //audio tracks are undefined here
            framesEncoded += value.framesEncoded;
          }
        } else if (value.type == "track" && typeof value.kind != "undefined" && value.kind == "audio") {
          if (typeof value.audioLevel != "undefined") {
            audioLevel = value.audioLevel;
          }
          if (typeof value.jitterBufferDelay != "undefined" && typeof value.jitterBufferEmittedCount != "undefined") {
            audioJitterAverageDelay = value.jitterBufferDelay / value.jitterBufferEmittedCount;
          }
        } else if (value.type == "track" && typeof value.kind != "undefined" && value.kind == "video") {
          if (typeof value.frameWidth != "undefined") {
            frameWidth = value.frameWidth;
          }
          if (typeof value.frameHeight != "undefined") {
            frameHeight = value.frameHeight;
          }
          if (typeof value.framesDecoded != "undefined") {
            framesDecoded = value.framesDecoded;
          }
          if (typeof value.framesDropped != "undefined") {
            framesDropped = value.framesDropped;
          }
          if (typeof value.framesReceived != "undefined") {
            framesReceived = value.framesReceived;
          }
          if (typeof value.jitterBufferDelay != "undefined" && typeof value.jitterBufferEmittedCount != "undefined") {
            videoJitterAverageDelay = value.jitterBufferDelay / value.jitterBufferEmittedCount;
          }
        } else if (value.type == "remote-inbound-rtp" && typeof value.kind != "undefined") {
          if (typeof value.packetsLost != "undefined") {
            if (value.kind == "video") {
              //this is the packetsLost for publishing
              videoPacketsLost = value.packetsLost;
            } else if (value.kind == "audio") {
              //this is the packetsLost for publishing
              audioPacketsLost = value.packetsLost;
            }
          }
          if (typeof value.roundTripTime != "undefined") {
            if (value.kind == "video") {
              videoRoundTripTime = value.roundTripTime;
            } else if (value.kind == "audio") {
              audioRoundTripTime = value.roundTripTime;
            }
          }
          if (typeof value.jitter != "undefined") {
            if (value.kind == "video") {
              videoJitter = value.jitter;
            } else if (value.kind == "audio") {
              audioJitter = value.jitter;
            }
          }
        } else if (value.type == "media-source") {
          if (value.kind == "video") {
            //returns video source dimensions, not necessarily dimensions being encoded by browser
            width = value.width;
            height = value.height;
            fps = value.framesPerSecond;
          }
        }
      });
      this.remotePeerConnectionStats[streamId].totalBytesReceived = bytesReceived;
      this.remotePeerConnectionStats[streamId].videoPacketsLost = videoPacketsLost;
      this.remotePeerConnectionStats[streamId].audioPacketsLost = audioPacketsLost;
      this.remotePeerConnectionStats[streamId].fractionLost = fractionLost;
      this.remotePeerConnectionStats[streamId].currentTime = currentTime;
      this.remotePeerConnectionStats[streamId].totalBytesSent = bytesSent;
      this.remotePeerConnectionStats[streamId].totalVideoPacketsSent = videoPacketsSent;
      this.remotePeerConnectionStats[streamId].totalAudioPacketsSent = audioPacketsSent;
      this.remotePeerConnectionStats[streamId].audioLevel = audioLevel;
      this.remotePeerConnectionStats[streamId].qualityLimitationReason = qlr;
      this.remotePeerConnectionStats[streamId].totalFramesEncoded = framesEncoded;
      this.remotePeerConnectionStats[streamId].resWidth = width;
      this.remotePeerConnectionStats[streamId].resHeight = height;
      this.remotePeerConnectionStats[streamId].srcFps = fps;
      this.remotePeerConnectionStats[streamId].frameWidth = frameWidth;
      this.remotePeerConnectionStats[streamId].frameHeight = frameHeight;
      this.remotePeerConnectionStats[streamId].videoRoundTripTime = videoRoundTripTime;
      this.remotePeerConnectionStats[streamId].videoJitter = videoJitter;
      this.remotePeerConnectionStats[streamId].audioRoundTripTime = audioRoundTripTime;
      this.remotePeerConnectionStats[streamId].audioJitter = audioJitter;
      this.remotePeerConnectionStats[streamId].framesDecoded = framesDecoded;
      this.remotePeerConnectionStats[streamId].framesDropped = framesDropped;
      this.remotePeerConnectionStats[streamId].framesReceived = framesReceived;
      this.remotePeerConnectionStats[streamId].videoJitterAverageDelay = videoJitterAverageDelay;
      this.remotePeerConnectionStats[streamId].audioJitterAverageDelay = audioJitterAverageDelay;
      this.notifyEventListeners("updated_stats", this.remotePeerConnectionStats[streamId]);
    });
  }

  /**
   * Called to start a periodic timer to get statistics periodically (5 seconds) for a specific stream.
   *
   *     streamId: unique id for the stream
   */
  enableStats(streamId) {
    if (this.remotePeerConnectionStats[streamId] == null) {
      this.remotePeerConnectionStats[streamId] = new PeerStats(streamId);
      this.remotePeerConnectionStats[streamId].timerId = setInterval(() => {
        this.getStats(streamId);
      }, 5000);
    }
  }

  /**
   * Called to stop the periodic timer which is set by @enableStats
   *
   *     streamId: unique id for the stream
   */
  disableStats(streamId) {
    if (this.remotePeerConnectionStats[streamId] != null || typeof this.remotePeerConnectionStats[streamId] != 'undefined') {
      clearInterval(this.remotePeerConnectionStats[streamId].timerId);
    }
  }

  /**
   * Called to check and start Web Socket connection if it is not started
   */
  checkWebSocketConnection() {
    if (this.webSocketAdaptor == null || this.webSocketAdaptor.isConnected() == false && this.webSocketAdaptor.isConnecting() == false) {
      this.webSocketAdaptor = new WebSocketAdaptor({
        websocket_url: this.websocket_url,
        webrtcadaptor: this,
        callback: (info, obj) => {
          if (info == "closed") {
            this.reconnectIfRequired();
          }
          this.notifyEventListeners(info, obj);
        },
        callbackError: (error, message) => {
          this.notifyErrorEventListeners(error, message);
        },
        debug: this.debug
      });
    }
  }

  /**
   * Called to stop Web Socket connection
   * After calling this function, create new WebRTCAdaptor instance, don't use the the same object
   * Because all streams are closed on server side as well when websocket connection is closed.
   */
  closeWebSocket() {
    for (var key in this.remotePeerConnection) {
      this.closePeerConnection(key);
    }
    //free the remote peer connection by initializing again
    this.remotePeerConnection = new Array();
    this.webSocketAdaptor.close();
  }

  /**
   * Called to send a text message to other peer in the peer-to-peer sessionnnection is closed.
   */
  peerMessage(streamId, definition, data) {
    var jsCmd = {
      command: "peerMessageCommand",
      streamId: streamId,
      definition: definition,
      data: data
    };
    this.webSocketAdaptor.send(JSON.stringify(jsCmd));
  }

  /**
   * Called to force AMS to send the video with the specified resolution in case of Adaptive Streaming (ABR) enabled.
   * Normally the resolution is automatically determined by AMS according to the network condition.
   *     streamId: unique id for the stream
   *   resolution: default is auto. You can specify any height value from the ABR list.
   */
  forceStreamQuality(streamId, resolution) {
    var jsCmd = {
      command: "forceStreamQuality",
      streamId: streamId,
      streamHeight: resolution
    };
    this.webSocketAdaptor.send(JSON.stringify(jsCmd));
  }

  /**
   * Called to send data via DataChannel. DataChannel should be enabled on AMS settings.
   *     streamId: unique id for the stream
   *   data: data that you want to send. It may be a text (may in Json format or not) or binary
   */
  sendData(streamId, data) {
    var CHUNK_SIZE = 16000;
    if (this.remotePeerConnection[streamId] !== undefined) {
      var dataChannel = this.remotePeerConnection[streamId].dataChannel;
      var length = data.length || data.size || data.byteLength;
      var sent = 0;
      if (typeof data === 'string' || data instanceof String) {
        dataChannel.send(data);
      } else {
        var token = Math.floor(Math.random() * 999999);
        var header = new Int32Array(2);
        header[0] = token;
        header[1] = length;
        dataChannel.send(header);
        var sent = 0;
        while (sent < length) {
          var size = Math.min(length - sent, CHUNK_SIZE);
          var buffer = new Uint8Array(size + 4);
          var tokenArray = new Int32Array(1);
          tokenArray[0] = token;
          buffer.set(new Uint8Array(tokenArray.buffer, 0, 4), 0);
          var chunk = data.slice(sent, sent + size);
          buffer.set(new Uint8Array(chunk), 4);
          sent += size;
          dataChannel.send(buffer);
        }
      }
    } else {
      console.warn("Send data is called for undefined peer connection with stream id: " + streamId);
    }
  }

  /**
   * Called by user
   * to add SoundMeter to a stream (remote stream)
   * to measure audio level. This sound Meters are added to a map with the key of StreamId.
   * When user called @getSoundLevelList, the instant levels are provided.
   *
   * This list can be used to add a sign to talking participant
   * in conference room. And also to determine the dominant audio to focus that player.
   * @param {*} stream
   * @param {*} streamId
   */
  enableAudioLevel(stream, streamId) {
    var soundMeter = new SoundMeter(this.audioContext);

    // Put variables in global scope to make them available to the
    // browser console.
    soundMeter.connectToSource(stream, null, function (e) {
      if (e) {
        alert(e);
        return;
      }
      console.log("Added sound meter for stream: " + streamId + " = " + soundMeter.instant.toFixed(2));
    });
    this.soundMeters[streamId] = soundMeter;
  }

  /**
   * Called by the user
   * to get the audio levels for the streams for the provided StreamIds
   *
   * @param {*} streamsList
   */
  getSoundLevelList(streamsList) {
    for (var i = 0; i < streamsList.length; i++) {
      this.soundLevelList[streamsList[i]] = this.soundMeters[streamsList[i]].instant.toFixed(2);
    }
    this.notifyEventListeners("gotSoundList", this.soundLevelList);
  }

  /**
   * Called media manaher to get video/audio sender for the local peer connection
   *
   * @param {*} streamId :
   * @param {*} type : "video" or "audio"
   * @returns
   */
  getSender(streamId, type) {
    var sender = null;
    if (this.remotePeerConnection[streamId] != null) {
      sender = this.remotePeerConnection[streamId].getSenders().find(function (s) {
        return s.track.kind == type;
      });
    }
    return sender;
  }

  /**
   * Called by user
   *
   * @param {*} videoTrackId : track id associated with pinned video
   * @param {*} streamId : streamId of the pinned video
   * @param {*} enabled : true | false
   * @returns
   */
  assignVideoTrack(videoTrackId, streamId, enabled) {
    var jsCmd = {
      command: "assignVideoTrackCommand",
      streamId: streamId,
      videoTrackId: videoTrackId,
      enabled: enabled
    };
    this.webSocketAdaptor.send(JSON.stringify(jsCmd));
  }

  /**
   * Called by user
   * video tracks may be less than the participants count
   * so these parameters are used for assigning video tracks to participants.
   * This message is used to make pagination in conference.
   *
   * @param {*} offset : start index for participant list to play
   * @param {*} size : number of the participants to play
   * @returns
   */
  updateVideoTrackAssignments(streamId, offset, size) {
    var jsCmd = {
      streamId: streamId,
      command: "updateVideoTrackAssignmentsCommand",
      offset: offset,
      size: size
    };
    this.webSocketAdaptor.send(JSON.stringify(jsCmd));
  }

  /**
   * Called by user
   * This message is used to set max video track count in a conference.
   *
   * @param {*} maxTrackCount : maximum video track count
   * @returns
   */
  setMaxVideoTrackCount(streamId, maxTrackCount) {
    var jsCmd = {
      streamId: streamId,
      command: "setMaxVideoTrackCountCommand",
      maxTrackCount: maxTrackCount
    };
    this.webSocketAdaptor.send(JSON.stringify(jsCmd));
  }

  /**
   * Called by user
   * This message is used to send audio level in a conference.
   *
   * @param {*} value : audio lavel
   * @returns
   */
  updateAudioLevel(streamId, value) {
    var jsCmd = {
      streamId: streamId,
      eventType: "UPDATE_AUDIO_LEVEL",
      audioLevel: value
    };
    this.sendData(streamId, JSON.stringify(jsCmd));
  }

  /**
   * Called by user
   * This message is used to get debug data from server for debugging purposes in conference.
   *
   * @returns
   */
  getDebugInfo(streamId) {
    var jsCmd = {
      streamId: streamId,
      command: "getDebugInfo"
    };
    this.webSocketAdaptor.send(JSON.stringify(jsCmd));
  }

  /**
   * The following messages are forwarded to MediaManager. They are also kept here because of backward compatibility.
   * You can find the details about them in media_manager.js
   */
  turnOffLocalCamera(streamId) {
    this.mediaManager.turnOffLocalCamera(streamId);
  }
  turnOnLocalCamera(streamId) {
    return this.mediaManager.turnOnLocalCamera(streamId);
  }
  muteLocalMic() {
    this.mediaManager.muteLocalMic();
  }
  unmuteLocalMic() {
    this.mediaManager.unmuteLocalMic();
  }
  switchDesktopCapture(streamId) {
    return this.mediaManager.switchDesktopCapture(streamId);
  }

  /**
   * Switch to Video camera capture again. Updates the video track on the fly as well.
   * @param {string} streamId
   * @param {string} deviceId
   * @returns {Promise}
   */
  switchVideoCameraCapture(streamId, deviceId) {
    return this.mediaManager.switchVideoCameraCapture(streamId, deviceId);
  }

  /**
   * Update video track of the stream. Updates the video track on the fly as well.
   * @param {string} stream
   * @param {string} streamId
   * @param {function} onEndedCallback
   * @param {boolean} stopDesktop
   * @returns {Promise}
   */
  updateVideoTrack(stream, streamId, onEndedCallback, stopDesktop) {
    return this.mediaManager.updateVideoTrack(stream, streamId, onEndedCallback, stopDesktop);
  }

  /**
   * Called by User
   * to switch between front and back camera on mobile devices
   *
   * @param {*} streamId Id of the stream to be changed.
   * @param {*} facingMode it can be ""user" or "environment"
   *
   * This method is used to switch front and back camera.
   */
  switchVideoCameraFacingMode(streamId, facingMode) {
    return this.mediaManager.switchVideoCameraFacingMode(streamId, facingMode);
  }
  switchDesktopCaptureWithCamera(streamId) {
    return this.mediaManager.switchDesktopCaptureWithCamera(streamId);
  }
  switchAudioInputSource(streamId, deviceId) {
    return this.mediaManager.switchAudioInputSource(streamId, deviceId);
  }
  setVolumeLevel(volumeLevel) {
    this.mediaManager.setVolumeLevel(volumeLevel);
  }
  enableAudioLevelForLocalStream(levelCallback, period) {
    this.mediaManager.enableAudioLevelForLocalStream(levelCallback, period);
  }
  applyConstraints(constraints) {
    return this.mediaManager.applyConstraints(constraints);
  }
  changeBandwidth(bandwidth, streamId) {
    this.mediaManager.changeBandwidth(bandwidth, streamId);
  }
  enableAudioLevelWhenMuted() {
    this.mediaManager.enableAudioLevelWhenMuted();
  }
  disableAudioLevelWhenMuted() {
    this.mediaManager.disableAudioLevelWhenMuted();
  }
  getVideoSender(streamId) {
    return this.mediaManager.getVideoSender(streamId);
  }
  openStream(mediaConstraints) {
    return this.mediaManager.openStream(mediaConstraints);
  }
  closeStream() {
    return this.mediaManager.closeStream();
  }
}

/* The Information Callbacks Called by This Class */
//TODO:

/* The Error Callbacks Called by This Class */
//TODO:
_defineProperty(WebRTCAdaptor, "pluginInitMethods", new Array());

export { WebRTCAdaptor as W, _defineProperty as _, _classPrivateMethodInitSpec as a, _classPrivateFieldInitSpec as b, _asyncToGenerator as c, _classPrivateFieldSet as d, _classPrivateFieldGet as e, _classPrivateMethodGet as f };
